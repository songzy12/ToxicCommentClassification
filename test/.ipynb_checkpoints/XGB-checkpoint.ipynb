{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import some popular python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import errno\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "from collections import OrderedDict, Counter\n",
    "from subprocess import check_call\n",
    "from shutil import copyfile\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition, ensemble, metrics, model_selection, naive_bayes, preprocessing, pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tag import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling1D,Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sner import Ner "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We let IPython to show all the variables automatically rather than only the last one as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we mute the `pandas` when we try to use chained assignments. Check this for reference: <https://www.dataquest.io/blog/settingwithcopywarning/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we use pandas to read csv files to get the training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = '../input/train.csv'\n",
    "test_path = '../input/test.csv'\n",
    "\n",
    "## Read the train and test dataset and check the top few lines ##\n",
    "train_df = pd.read_csv(train_path).fillna('NANN')\n",
    "test_df = pd.read_csv(test_path).fillna('NANN')\n",
    "\n",
    "train_df, test_df = read(train_path, test_path)\n",
    "\n",
    "train_df.shape[0]\n",
    "test_df.shape[0]\n",
    "train_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the target labels and select the content of particular labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['toxic',\n",
       " 'severe_toxic',\n",
       " 'obscene',\n",
       " 'threat',\n",
       " 'insult',\n",
       " 'identity_hate',\n",
       " 'split',\n",
       " 'num_words']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      1             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0     Nonsense?  kiss off, geek. what I said is true...\n",
       "20    Why hasn't Alitalia been removed rom the allia...\n",
       "26    \"\\nThe Graceful Slick....\\nIs non other than a...\n",
       "30    \"\\n\\n Stupid? \\n\\nAs soon as I saw the phrase ...\n",
       "32    \"\\nBan one side of an argument by a bullshit n...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(train_df)[2:]\n",
    "len(labels)\n",
    "labels\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_df[labels].head()\n",
    "\n",
    "label0 = labels[0]\n",
    "train_df[label0].head()\n",
    "\n",
    "train_df[train_df['toxic']==1]['comment_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we store the `id` field of test file for submission, then drop it from train and test data frame for simplicity. We also get the target labels as `train_y`, and drop them from train data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "train_y = train_df[labels].values\n",
    "test_id = test_df['id'].values\n",
    "labels\n",
    "train_y\n",
    "\n",
    "cols_to_drop = ['id']\n",
    "train_df = train_df.drop(cols_to_drop + labels, axis=1).head()\n",
    "test_df = test_df.drop(cols_to_drop, axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split finished...\n",
      "split finished...\n"
     ]
    }
   ],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['split'] = df['comment_text'].apply(word_tokenize)\n",
    "    print('split finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_ner(df):\n",
    "    st = StanfordNERTagger('../stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                          '../stanford-ner-2017-06-09/stanford-ner.jar',\n",
    "                          encoding='utf-8')\n",
    "    st = Ner(host='localhost',port=9199)\n",
    "    df['st'] = df['comment_text'].apply(lambda x: [w[1] for w in st.get_entities(x)])\n",
    "    df['n_person'] = df['st'].apply(lambda x: x.count('PERSON'))\n",
    "    df['n_location'] = df['st'].apply(lambda x: x.count('LOCATION'))\n",
    "    print('NER finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_sid(df):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df['sid'] = df['comment_text'].apply(sid.polarity_scores)\n",
    "    for k in ['neu', 'compound', 'pos', 'neg']:\n",
    "        df['sid_'+k] = df['sid'].apply(lambda x: x[k])\n",
    "    print('polarity_scores finished...') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_pos(df): \n",
    "    df['pos_tag'] = df['split'].apply(lambda x: [w[1] for w in nltk.pos_tag(x)])\n",
    "    for pos in ['CC', 'RB', 'IN', 'NN', 'VB', 'VBP', 'JJ', 'PRP', 'TO', 'DT']:\n",
    "        df['n_pos_' + pos] = df['pos_tag'].apply(lambda x: x.count(pos))\n",
    "    print('pos_tag finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_cnt(df):\n",
    "    ## Number of words in the text ##\n",
    "    df[\"num_words\"] = df[\"split\"].apply(len)\n",
    "    \n",
    "    return\n",
    "    ## Number of unique words in the text ##\n",
    "    df[\"num_unique_words\"] = df[\"split\"].apply(lambda x: len(set(x)))\n",
    "    \n",
    "    ## Number of characters in the text ##\n",
    "    df[\"num_chars\"] = df['comment_text'].apply(len)\n",
    "    \n",
    "    ## Number of stopwords in the text ##\n",
    "    #eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    df[\"num_stopwords\"] = df[\"split\"].apply(lambda x: len([w for w in x if w in eng_stopwords]))\n",
    "    \n",
    "    ## Number of punctuations in the text ##\n",
    "    df[\"num_punctuations\"] = df['split'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    df[\"num_words_upper\"] = df[\"split\"].apply(lambda x: len([w for w in x if w.isupper()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    df[\"num_words_title\"] = df[\"split\"].apply(lambda x: len([w for w in x if w.istitle()]))\n",
    "    \n",
    "    ## Average length of the words in the text ##\n",
    "    df[\"mean_word_len\"] = df[\"split\"].apply(lambda x: np.mean([len(w) for w in x]))\n",
    "\n",
    "\n",
    "    anchor_words = ['the', 'a', 'appear', 'little', 'was', 'one', 'two', 'three', 'ten', 'is', \n",
    "                    'are', 'ed', 'however', 'to', 'into', 'about', 'th', 'er', 'ex', 'an', \n",
    "                    'ground', 'any', 'silence', 'wall']\n",
    "\n",
    "    gender_words = ['man', 'woman', 'he', 'she', 'her', 'him', 'male', 'female']\n",
    "\n",
    "    for word in anchor_words + gender_words:\n",
    "        df['n_'+word] = df[\"split\"].apply(lambda x: len([w for w in x if w.lower() == word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 575 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for df in [train_df, test_df]:\n",
    "    feature_cnt(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comment_text', 'split', 'num_words']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_lda(train_df, test_df): \n",
    "    \n",
    "    ### Fit transform the tfidf vectorizer ###\n",
    "    tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    full_tfidf = tfidf_vec.fit_transform(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())\n",
    "    \n",
    "    no_topics = 20 \n",
    "    lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(full_tfidf)\n",
    "    train_lda = pd.DataFrame(lda.transform(train_tfidf))\n",
    "    test_lda = pd.DataFrame(lda.transform(test_tfidf))\n",
    "    \n",
    "    train_lda.columns = ['lda_'+str(i) for i in range(no_topics)]\n",
    "    test_lda.columns = ['lda_'+str(i) for i in range(no_topics)]\n",
    "    train_df = pd.concat([train_df, train_lda], axis=1)\n",
    "    test_df = pd.concat([test_df, test_lda], axis=1)\n",
    "    del full_tfidf, train_tfidf, test_tfidf, train_lda, test_lda\n",
    "\n",
    "    print(\"LDA finished...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "wv = '../input/glove.6B.100d.txt'\n",
    "\n",
    "def loadWordVecs():\n",
    "    embeddings_index = {}\n",
    "    f = open(wv)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def doGlove(x_train,x_test):\n",
    "    embeddings_index = loadWordVecs()\n",
    "    # create sentence vectors using the above function for training and validation set\n",
    "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n",
    "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n",
    "    xtrain_glove = np.array(xtrain_glove)\n",
    "    xtest_glove = np.array(xtest_glove)\n",
    "    return xtrain_glove,xtest_glove,embeddings_index\n",
    "\n",
    "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(train_df['comment_text'], test_df['comment_text'])\n",
    "train_df[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\n",
    "test_df[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())\n",
    "print(\"Glove sentence vector finished...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Neural Networks and Facebook's Fasttext\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "# NN\n",
    "def doAddNN(X_train,X_test,pred_train,pred_test):\n",
    "    for i in range(6):\n",
    "        X_train['nn_'+str(i)] = pred_train[:,i]\n",
    "        X_test['nn_'+str(i)] = pred_test[:,i]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN(nb_words_cnt,max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,\n",
    "                     5,\n",
    "                     padding='valid',\n",
    "                     activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def doNN(X_train,X_test,Y_train):\n",
    "    max_len = 70\n",
    "    nb_words = 10000\n",
    "    \n",
    "    print('Processing text dataset')\n",
    "    texts_1 = []\n",
    "    for text in X_train['comment_text']:\n",
    "        texts_1.append(text)\n",
    "\n",
    "    print('Found %s texts.' % len(texts_1))\n",
    "    test_texts_1 = []\n",
    "    for text in X_test['comment_text']:\n",
    "        test_texts_1.append(text)\n",
    "    print('Found %s texts.' % len(test_texts_1))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=nb_words)\n",
    "    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n",
    "    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "\n",
    "    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n",
    "    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n",
    "    del test_sequences_1\n",
    "    del sequences_1\n",
    "    nb_words_cnt = min(nb_words, len(word_index)) + 1\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], len(labels)])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN(nb_words_cnt,max_len)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_pad)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "train_df,test_df = doNN(train_df,test_df,train_y)\n",
    "print('NN finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NN Glove\n",
    "\n",
    "def doAddNN_glove(X_train,X_test,pred_train,pred_test):\n",
    "    for i in range(6):\n",
    "        X_train['nn_glove_'+str(i)] = pred_train[:,i]\n",
    "        X_test['nn_glove_'+str(i)] = pred_test[:,i]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN_glove():\n",
    "    # create a simple 3 layer sequential neural net\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, input_dim=100, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(len(labels)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def doNN_glove(X_train,X_test,Y_train,xtrain_glove,xtest_glove):\n",
    "    # scale the data before any neural net:\n",
    "    scl = preprocessing.StandardScaler()\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    xtrain_glove = scl.fit_transform(xtrain_glove)\n",
    "    xtest_glove = scl.fit_transform(xtest_glove)\n",
    "    pred_train = np.zeros([xtrain_glove.shape[0], len(labels)])\n",
    "    \n",
    "    for dev_index, val_index in kf.split(xtrain_glove):\n",
    "        dev_X, val_X = xtrain_glove[dev_index], xtrain_glove[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN_glove()\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=10, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_glove)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN_glove(X_train,X_test,pred_train,pred_full_test/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fast Text\n",
    "\n",
    "def doAddFastText(X_train,X_test,pred_train,pred_test):\n",
    "    for i in range(6):\n",
    "        X_train['ff_'+str(i)] = pred_train[:,i]\n",
    "        X_test['ff_'+str(i)] = pred_test[:,i]\n",
    "    return X_train,X_test\n",
    "\n",
    "\n",
    "def initFastText(embedding_dims,input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocessFastText(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df['comment_text']:\n",
    "        doc = preprocessFastText(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def doFastText(X_train,X_test,Y_train):\n",
    "    min_count = 2\n",
    "\n",
    "    docs = create_docs(X_train)\n",
    "    tokenizer = Tokenizer(lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "    maxlen = 300\n",
    "\n",
    "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "    input_dim = np.max(docs) + 1\n",
    "    embedding_dims = 20\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "\n",
    "    docs_test = create_docs(X_test)\n",
    "    docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "    xtrain_pad = docs\n",
    "    xtest_pad = docs_test\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], len(labels)])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initFastText(embedding_dims,input_dim)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=25, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(docs_test)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddFastText(X_train,X_test,pred_train,pred_full_test/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df,test_df = doFastText(train_df,test_df,train_y)\n",
    "print('FastText finished...')\n",
    "train_df,test_df = doNN_glove(train_df,test_df,train_y,glove_vecs_train,glove_vecs_test)\n",
    "print('NN Glove finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['comment_text', 'split']\n",
    "train_X = train_df.drop(cols_to_drop, axis=1)\n",
    "test_X = test_df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNB & SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())\n",
    "\n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())\n",
    "\n",
    "\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "for i in range(6):\n",
    "    train_df[\"nb_cvec_\"+str(i)] = pred_train[:,i]\n",
    "    test_df[\"nb_cvec_\"+str(i)] = pred_full_test[:,i]\n",
    "print(\"Naive Bayesian Count Vector finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "tfidf_vec.fit(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())\n",
    "\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "for i in range(6):\n",
    "    train_df[\"nb_cvec_char_\"+str(i)] = pred_train[:,i]\n",
    "    test_df[\"nb_cvec_char_\"+str(i)] = pred_full_test[:,i]\n",
    "print(\"Naive Bayersian Count Vector Char finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())\n",
    "\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "for i in range(6):\n",
    "    train_df[\"nb_tfidf_char_\"+str(i)] = pred_train[:,i]\n",
    "    test_df[\"nb_tfidf_char_\"+str(i)] = pred_full_test[:,i]\n",
    "print(\"Naive Bayersian TFIDF Vector Char finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = len(labels)\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"cv scores : \", cv_scores)\n",
    "\n",
    "pred_full_test /= 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(pred_full_test)\n",
    "out_df.columns = srs\n",
    "out_df.insert(0, 'id', test_id)\n",
    "out_df.to_csv(\"result.csv\", index=False)\n",
    "\n",
    "code.interact(local=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = '../output/train_df.pkl'\n",
    "train_df.to_pickle(file_name)  \n",
    "train_df = pd.read_pickle(file_name)\n",
    "file_name = '../output/test_df.pkl'\n",
    "test_df.to_pickle(file_name)\n",
    "test_df = pd.read_pickle(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
