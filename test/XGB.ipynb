{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import some popular python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import errno\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "from collections import OrderedDict, Counter\n",
    "from subprocess import check_call\n",
    "from shutil import copyfile\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition, ensemble, metrics, model_selection, naive_bayes, preprocessing, pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tag import StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling1D,Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sner import Ner "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We let IPython to show all the variables automatically rather than only the last one as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we mute the `pandas` when we try to use chained assignments. Check this for reference: <https://www.dataquest.io/blog/settingwithcopywarning/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we use pandas to read csv files to get the training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "226998"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] =="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = '../input/train.csv'\n",
    "test_path = '../input/test.csv'\n",
    "\n",
    "\n",
    "## Read the train and test dataset and check the top few lines ##\n",
    "# TODO:\n",
    "train_df = pd.read_csv(train_path).fillna('NANN').head(5000)\n",
    "test_df = pd.read_csv(test_path).fillna('NANN')\n",
    "\n",
    "train_df.shape[0]\n",
    "test_df.shape[0]\n",
    "train_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the target labels and select the content of particular labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      1             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0     Nonsense?  kiss off, geek. what I said is true...\n",
       "20    Why hasn't Alitalia been removed rom the allia...\n",
       "26    \"\\nThe Graceful Slick....\\nIs non other than a...\n",
       "30    \"\\n\\n Stupid? \\n\\nAs soon as I saw the phrase ...\n",
       "32    \"\\nBan one side of an argument by a bullshit n...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(train_df)[2:]\n",
    "len(labels)\n",
    "labels\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_df[labels].head()\n",
    "\n",
    "label0 = labels[0]\n",
    "train_df[label0].head()\n",
    "\n",
    "train_df[train_df['toxic']==1]['comment_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we store the `id` field of test file for submission, then drop it from train and test data frame for simplicity. We also get the target labels as `train_y`, and drop them from train data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "train_y = train_df[labels].values\n",
    "test_id = test_df['id'].values\n",
    "labels\n",
    "train_y\n",
    "\n",
    "cols_to_drop = ['id']\n",
    "train_df = train_df.drop(cols_to_drop + labels, axis=1)\n",
    "test_df = test_df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split finished...\n",
      "split finished...\n"
     ]
    }
   ],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    df['split'] = df['comment_text'].apply(word_tokenize)\n",
    "    print('split finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_ner(df):\n",
    "    st = StanfordNERTagger('../stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                          '../stanford-ner-2017-06-09/stanford-ner.jar',\n",
    "                          encoding='utf-8')\n",
    "    st = Ner(host='localhost',port=9199)\n",
    "    df['st'] = df['comment_text'].apply(lambda x: [w[1] for w in st.get_entities(x)])\n",
    "    df['n_person'] = df['st'].apply(lambda x: x.count('PERSON'))\n",
    "    df['n_location'] = df['st'].apply(lambda x: x.count('LOCATION'))\n",
    "    print('NER finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_sid(df):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df['sid'] = df['comment_text'].apply(sid.polarity_scores)\n",
    "    for k in ['neu', 'compound', 'pos', 'neg']:\n",
    "        df['sid_'+k] = df['sid'].apply(lambda x: x[k])\n",
    "    print('polarity_scores finished...') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_pos(df): \n",
    "    df['pos_tag'] = df['split'].apply(lambda x: [w[1] for w in nltk.pos_tag(x)])\n",
    "    for pos in ['CC', 'RB', 'IN', 'NN', 'VB', 'VBP', 'JJ', 'PRP', 'TO', 'DT']:\n",
    "        df['n_pos_' + pos] = df['pos_tag'].apply(lambda x: x.count(pos))\n",
    "    print('pos_tag finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_cnt(df):\n",
    "    ## Number of words in the text ##\n",
    "    df[\"num_words\"] = df[\"split\"].apply(len)\n",
    "    \n",
    "    return\n",
    "    ## Number of unique words in the text ##\n",
    "    df[\"num_unique_words\"] = df[\"split\"].apply(lambda x: len(set(x)))\n",
    "    \n",
    "    ## Number of characters in the text ##\n",
    "    df[\"num_chars\"] = df['comment_text'].apply(len)\n",
    "    \n",
    "    ## Number of stopwords in the text ##\n",
    "    #eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    df[\"num_stopwords\"] = df[\"split\"].apply(lambda x: len([w for w in x if w in eng_stopwords]))\n",
    "    \n",
    "    ## Number of punctuations in the text ##\n",
    "    df[\"num_punctuations\"] = df['split'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    df[\"num_words_upper\"] = df[\"split\"].apply(lambda x: len([w for w in x if w.isupper()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    df[\"num_words_title\"] = df[\"split\"].apply(lambda x: len([w for w in x if w.istitle()]))\n",
    "    \n",
    "    ## Average length of the words in the text ##\n",
    "    df[\"mean_word_len\"] = df[\"split\"].apply(lambda x: np.mean([len(w) for w in x]))\n",
    "\n",
    "\n",
    "    anchor_words = ['the', 'a', 'appear', 'little', 'was', 'one', 'two', 'three', 'ten', 'is', \n",
    "                    'are', 'ed', 'however', 'to', 'into', 'about', 'th', 'er', 'ex', 'an', \n",
    "                    'ground', 'any', 'silence', 'wall']\n",
    "\n",
    "    gender_words = ['man', 'woman', 'he', 'she', 'her', 'him', 'male', 'female']\n",
    "\n",
    "    for word in anchor_words + gender_words:\n",
    "        df['n_'+word] = df[\"split\"].apply(lambda x: len([w for w in x if w.lower() == word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for df in [train_df, test_df]:\n",
    "    feature_cnt(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comment_text', 'split', 'num_words']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_lda(train_df, test_df): \n",
    "    \n",
    "    ### Fit transform the tfidf vectorizer ###\n",
    "    tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    full_tfidf = tfidf_vec.fit_transform(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())\n",
    "    \n",
    "    no_topics = 20 \n",
    "    lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(full_tfidf)\n",
    "    train_lda = pd.DataFrame(lda.transform(train_tfidf))\n",
    "    test_lda = pd.DataFrame(lda.transform(test_tfidf))\n",
    "    \n",
    "    train_lda.columns = ['lda_'+str(i) for i in range(no_topics)]\n",
    "    test_lda.columns = ['lda_'+str(i) for i in range(no_topics)]\n",
    "    train_df = pd.concat([train_df, train_lda], axis=1)\n",
    "    test_df = pd.concat([test_df, test_lda], axis=1)\n",
    "    del full_tfidf, train_tfidf, test_tfidf, train_lda, test_lda\n",
    "\n",
    "    print(\"LDA finished...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "\n",
      ", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",
      "\n",
      ". -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\n",
      "\n",
      "of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\n",
      "\n",
      "to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\n",
      "\n",
      "and -0.071953 0.23127 0.023731 -0.50638 0.33923 0.1959 -0.32943 0.18364 -0.18057 0.28963 0.20448 -0.5496 0.27399 0.58327 0.20468 -0.49228 0.19974 -0.070237 -0.88049 0.29485 0.14071 -0.1009 0.99449 0.36973 0.44554 0.28998 -0.1376 -0.56365 -0.029365 -0.4122 -0.25269 0.63181 -0.44767 0.24363 -0.10813 0.25164 0.46967 0.3755 -0.23613 -0.14129 -0.44537 -0.65737 -0.042421 -0.28636 -0.28811 0.063766 0.20281 -0.53542 0.41307 -0.59722 -0.38614 0.19389 -0.17809 1.6618 -0.011819 -2.3737 0.058427 -0.2698 1.2823 0.81925 -0.22322 0.72932 -0.053211 0.43507 0.85011 -0.42935 0.92664 0.39051 1.0585 -0.24561 -0.18265 -0.5328 0.059518 -0.66019 0.18991 0.28836 -0.2434 0.52784 -0.65762 -0.14081 1.0491 0.5134 -0.23816 0.69895 -1.4813 -0.2487 -0.17936 -0.059137 -0.08056 -0.48782 0.014487 -0.6259 -0.32367 0.41862 -1.0807 0.46742 -0.49931 -0.71895 0.86894 0.19539\n",
      "\n",
      "in 0.085703 -0.22201 0.16569 0.13373 0.38239 0.35401 0.01287 0.22461 -0.43817 0.50164 -0.35874 -0.34983 0.055156 0.69648 -0.17958 0.067926 0.39101 0.16039 -0.26635 -0.21138 0.53698 0.49379 0.9366 0.66902 0.21793 -0.46642 0.22383 -0.36204 -0.17656 0.1748 -0.20367 0.13931 0.019832 -0.10413 -0.20244 0.55003 -0.1546 0.98655 -0.26863 -0.2909 -0.32866 -0.34188 -0.16943 -0.42001 -0.046727 -0.16327 0.70824 -0.74911 -0.091559 -0.96178 -0.19747 0.10282 0.55221 1.3816 -0.65636 -3.2502 -0.31556 -1.2055 1.7709 0.4026 -0.79827 1.1597 -0.33042 0.31382 0.77386 0.22595 0.52471 -0.034053 0.32048 0.079948 0.17752 -0.49426 -0.70045 -0.44569 0.17244 0.20278 0.023292 -0.20677 -1.0158 0.18325 0.56752 0.31821 -0.65011 0.68277 -0.86585 -0.059392 -0.29264 -0.55668 -0.34705 -0.32895 0.40215 -0.12746 -0.20228 0.87368 -0.545 0.79205 -0.20695 -0.074273 0.75808 -0.34243\n",
      "\n",
      "a -0.27086 0.044006 -0.02026 -0.17395 0.6444 0.71213 0.3551 0.47138 -0.29637 0.54427 -0.72294 -0.0047612 0.040611 0.043236 0.29729 0.10725 0.40156 -0.53662 0.033382 0.067396 0.64556 -0.085523 0.14103 0.094539 0.74947 -0.194 -0.68739 -0.41741 -0.22807 0.12 -0.48999 0.80945 0.045138 -0.11898 0.20161 0.39276 -0.20121 0.31354 0.75304 0.25907 -0.11566 -0.029319 0.93499 -0.36067 0.5242 0.23706 0.52715 0.22869 -0.51958 -0.79349 -0.20368 -0.50187 0.18748 0.94282 -0.44834 -3.6792 0.044183 -0.26751 2.1997 0.241 -0.033425 0.69553 -0.64472 -0.0072277 0.89575 0.20015 0.46493 0.61933 -0.1066 0.08691 -0.4623 0.18262 -0.15849 0.020791 0.19373 0.063426 -0.31673 -0.48177 -1.3848 0.13669 0.96859 0.049965 -0.2738 -0.035686 -1.0577 -0.24467 0.90366 -0.12442 0.080776 -0.83401 0.57201 0.088945 -0.42532 -0.018253 -0.079995 -0.28581 -0.01089 -0.4923 0.63687 0.23642\n",
      "\n",
      "\" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 -0.074775 -0.3766 -0.057774 0.12159 0.34384 0.41928 -0.23236 -0.31547 0.60939 0.25117 -0.68667 0.70873 1.2162 -0.1824 -0.48442 -0.33445 0.30343 1.086 0.49992 -0.20198 0.27959 0.68352 -0.33566 -0.12405 0.059656 0.33617 0.37501 0.56552 0.44867 0.11284 -0.16196 -0.94346 -0.67961 0.18581 0.060653 0.43776 0.13834 -0.48207 -0.56141 -0.25422 -0.52445 0.097003 -0.48925 0.19077 0.21481 1.4969 -0.86665 -3.2846 0.56854 0.41971 1.2294 0.78522 -0.29369 0.63803 -1.5926 -0.20437 1.5306 0.13548 0.50722 0.18742 0.48552 -0.28995 0.19573 0.0046515 0.092879 -0.42444 0.64987 0.52839 0.077908 0.8263 -1.2208 -0.34955 0.49855 -0.64155 -0.72308 0.26566 -1.3643 -0.46364 -0.52048 -1.0525 0.22895 -0.3456 -0.658 -0.16735 0.35158 0.74337 0.26074 0.061104 -0.39079 -0.84557 -0.035432 0.17036\n",
      "\n",
      "'s 0.58854 -0.2025 0.73479 -0.68338 -0.19675 -0.1802 -0.39177 0.34172 -0.60561 0.63816 -0.26695 0.36486 -0.40379 -0.1134 -0.58718 0.2838 0.8025 -0.35303 0.30083 0.078935 0.44416 -0.45906 0.79294 0.50365 0.32805 0.28027 -0.4933 -0.38482 -0.039284 -0.2483 -0.1988 1.1469 0.13228 0.91691 -0.36739 0.89425 0.5426 0.61738 -0.62205 -0.31132 -0.50933 0.23335 1.0826 -0.044637 -0.12767 0.27628 -0.032617 -0.27397 0.77764 -0.50861 0.038307 -0.33679 0.42344 1.2271 -0.53826 -3.2411 0.42626 0.025189 1.3948 0.65085 0.03325 0.37141 0.4044 0.35558 0.98265 -0.61724 0.53901 0.76219 0.30689 0.33065 0.30956 -0.15161 -0.11313 -0.81281 0.6145 -0.44341 -0.19163 -0.089551 -1.5927 0.37405 0.85857 0.54613 -0.31928 0.52598 -1.4802 -0.97931 -0.2939 -0.14724 0.25803 -0.1817 1.0149 0.77649 0.12598 0.54779 -1.0316 0.064599 -0.37523 -0.94475 0.61802 0.39591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for -0.14401 0.32554 0.14257 -0.099227 0.72536 0.19321 -0.24188 0.20223 -0.89599 0.15215 0.035963 -0.59513 -0.051635 -0.014428 0.35475 -0.31859 0.76984 -0.087369 -0.24762 0.65059 -0.15138 -0.42703 0.18813 0.091562 0.15192 0.11303 -0.15222 -0.62786 -0.23923 0.096009 -0.46147 0.41526 -0.30475 0.1371 0.16758 0.53301 -0.043658 0.85924 -0.41192 -0.21394 -0.51228 -0.31945 0.12662 -0.3151 0.0031429 0.27129 0.17328 -1.3159 -0.42414 -0.69126 0.019017 -0.13375 -0.096057 1.7069 -0.65291 -2.6111 0.26518 -0.61178 2.095 0.38148 -0.55823 0.2036 -0.33704 0.37354 0.6951 -0.001637 0.81885 0.51793 0.27746 -0.37177 -0.43345 -0.42732 -0.54912 -0.30715 0.18101 0.2709 -0.29266 0.30834 -1.4624 -0.18999 0.92277 -0.099217 -0.25165 0.49197 -1.525 0.15326 0.2827 0.12102 -0.36766 -0.61275 -0.18884 0.10907 0.12315 0.090066 -0.65447 -0.17252 2.6336e-05 0.25398 1.1078 -0.073074\n",
      "\n",
      "- -1.2557 0.61036 0.56793 -0.96596 -0.45249 -0.071696 0.57122 -0.31292 -0.43814 0.90622 0.06961 -0.053104 0.25029 0.27841 0.77724 0.26329 0.56874 -1.1171 -0.078268 -0.51317 0.8071 0.99214 0.22753 1.0847 0.88292 0.17221 -0.68686 -0.86467 -0.80003 -0.34738 -0.044074 -0.30444 0.23406 0.28592 0.060548 -0.65477 -0.039738 0.74878 -0.46471 0.063023 -0.16519 -1.2217 -0.089479 -0.8125 0.27615 -0.13841 -0.76667 -0.96974 0.83123 -0.77639 -1.3327 -0.28732 -0.053684 1.1735 -1.1795 -2.7519 0.45359 1.1984 2.8203 0.060114 0.32296 0.19097 0.3459 -0.41503 0.1515 0.38148 1.619 0.9929 -0.82549 -0.098692 0.74449 -0.38602 -1.0004 -1.305 -0.31269 -0.57625 0.14095 -0.80269 -1.4714 -0.48014 1.1993 -0.48561 0.40496 -0.032867 -2.051 0.18284 -0.2723 0.043287 0.066801 -0.62832 -0.05854 0.28253 -0.083276 -0.022234 -0.55914 0.24586 0.36052 -1.5877 0.76984 -0.64998\n",
      "\n",
      "that -0.093337 0.19043 0.68457 -0.41548 -0.22777 -0.11803 -0.095434 0.19613 0.17785 -0.020244 -0.055409 0.33867 0.79396 -0.047126 0.44281 -0.061266 0.20796 0.034094 -0.64751 0.35874 0.13936 -0.6831 0.25596 -0.12911 0.2608 -0.11674 0.024925 -0.60259 -0.41474 -0.51104 0.14936 0.79977 -0.12716 0.40474 -0.21435 0.47031 0.49 0.48886 -0.17772 -0.18861 -0.78391 -0.14158 0.22169 -0.22078 -0.30509 -0.10837 0.57168 -0.7832 -0.16328 -0.76131 0.080873 0.00067217 0.44713 1.3434 -0.20014 -2.868 -0.002647 -0.39858 1.8379 1.2211 -0.16066 0.65853 0.26946 0.27212 0.94735 0.24372 0.8194 0.6774 0.063485 -0.55934 0.45541 -0.64684 -0.034702 -0.45566 0.21847 -0.051689 0.32299 -0.022961 -1.7955 0.31217 0.76227 -0.23191 -1.0133 -0.0064374 -1.8135 -0.75221 0.28362 -0.30815 -0.43853 -0.62654 0.13213 -0.54725 -0.47478 -0.0079727 -0.15112 -0.29326 -0.35118 -0.68175 0.28804 0.54893\n",
      "\n",
      "on -0.21863 -0.42664 0.5196 0.0043103 0.58045 -0.10873 -0.37726 0.4566 -0.60627 -0.075773 0.11306 0.17703 0.1605 0.074514 0.63649 -0.078852 0.75268 -0.24962 -0.51628 -0.33348 0.66754 -0.34183 0.61316 0.31668 0.64846 -0.079312 -0.065219 -0.17718 -0.32439 0.51868 -0.23424 0.34381 0.046851 0.74025 -0.47005 0.53685 -0.35549 0.40737 -0.093421 -0.13439 -0.41969 -0.30041 0.28646 0.37419 -0.46054 -0.307 -0.3858 -0.69317 -0.00092461 -0.61984 0.11978 0.1495 0.17833 1.5313 -0.92445 -3.0428 0.030761 -0.64359 2.3824 0.56219 -0.56021 1.0264 -0.45143 0.14117 0.65944 0.37572 0.098334 0.38304 -0.076882 -0.21781 -0.29892 -0.49458 0.095239 -0.63059 -0.061311 0.17767 -0.14051 0.47182 -0.95891 0.045334 0.808 -0.026867 -0.27483 0.35541 -0.82896 -0.78838 -0.079732 0.22941 -0.45013 -0.3004 -0.52716 0.11358 -0.49906 0.827 -0.56991 0.25143 -0.40266 -0.29146 1.3816 0.18084\n",
      "\n",
      "is -0.54264 0.41476 1.0322 -0.40244 0.46691 0.21816 -0.074864 0.47332 0.080996 -0.22079 -0.12808 -0.1144 0.50891 0.11568 0.028211 -0.3628 0.43823 0.047511 0.20282 0.49857 -0.10068 0.13269 0.16972 0.11653 0.31355 0.25713 0.092783 -0.56826 -0.52975 -0.051456 -0.67326 0.92533 0.2693 0.22734 0.66365 0.26221 0.19719 0.2609 0.18774 -0.3454 -0.42635 0.13975 0.56338 -0.56907 0.12398 -0.12894 0.72484 -0.26105 -0.26314 -0.43605 0.078908 -0.84146 0.51595 1.3997 -0.7646 -3.1453 -0.29202 -0.31247 1.5129 0.52435 0.21456 0.42452 -0.088411 -0.17805 1.1876 0.10579 0.76571 0.21914 0.35824 -0.11636 0.093261 -0.62483 -0.21898 0.21796 0.74056 -0.43735 0.14343 0.14719 -1.1605 -0.050508 0.12677 -0.014395 -0.98676 -0.091297 -1.2054 -0.11974 0.047847 -0.54001 0.52457 -0.70963 -0.32528 -0.1346 -0.41314 0.33435 -0.0072412 0.32253 -0.044219 -1.2969 0.76217 0.46349\n",
      "\n",
      "was 0.13717 -0.54287 0.19419 -0.29953 0.17545 0.084672 0.67752 0.098295 -0.035611 0.21334 0.51663 0.20687 0.44082 -0.33655 0.56025 -0.6879 0.51957 -0.21258 -0.52708 -0.12249 0.33099 0.026448 0.59007 0.0065469 0.45405 -0.33884 -0.28261 -0.24633 0.10847 0.3164 -0.15368 0.73503 0.11858 0.70842 0.075081 0.29738 -0.11395 0.40807 -0.042531 -0.21301 -0.79849 -0.12703 0.752 -0.41746 0.46615 -0.039097 0.65961 -0.32336 0.442 -0.94137 -0.23125 -0.30604 0.79912 1.4581 -0.88199 -3.0041 -0.75243 -0.20503 1.1998 0.94881 0.30649 0.48411 -0.7572 0.65856 0.70107 -0.93141 0.52928 0.23323 0.18857 0.38691 0.011489 -0.31937 0.011858 0.22944 0.17764 0.16868 0.14003 0.58647 -1.5447 -0.064425 -0.00064711 0.13606 -0.32695 0.10043 -1.546 -0.5476 0.21027 -0.67195 -0.1597 -0.68271 -0.22043 -0.87088 -0.16248 0.83086 -0.23045 0.19864 -0.051892 -0.52057 0.25434 -0.23759\n",
      "\n",
      "said -0.13128 -0.452 0.043399 -0.99798 -0.21053 -0.95868 -0.24609 0.48413 0.18178 0.475 -0.22305 0.30064 0.43496 -0.3605 0.20245 -0.52594 -0.34708 0.0075873 -1.0497 0.18673 0.57369 0.43814 0.098659 0.3877 -0.2258 0.41911 0.043602 -0.7352 -0.53583 0.19276 -0.21961 0.42515 -0.19082 0.47187 0.18826 0.13357 0.41839 1.3138 0.35678 -0.32172 -1.2257 -0.26635 0.36716 -0.27586 -0.53246 0.16786 -0.11253 -0.99959 -0.60706 -0.89271 0.65156 -0.88784 0.049233 0.67111 -0.27553 -2.4005 -0.36989 0.29136 1.3498 1.7353 0.27 0.021299 0.14422 0.023784 0.33643 -0.35476 1.0921 1.4845 0.4943 0.15688 0.34679 -0.57221 0.12093 -1.2616 1.0541 0.064335 -0.002732 0.19038 -1.7643 0.055068 1.4737 -0.41782 -0.57342 -0.12129 -1.3169 -0.73883 0.17682 -0.019991 -0.49176 -0.55247 1.0623 -0.62879 0.29098 0.13238 -0.70414 0.67128 -0.085462 -0.30526 -0.045495 0.56509\n",
      "\n",
      "with -0.43608 0.39104 0.51657 -0.13861 0.2029 0.50723 -0.012544 0.22948 -0.6316 0.21199 -0.018043 -0.39364 0.74164 0.30221 0.51792 -0.25191 0.25373 -0.65184 -0.42963 0.0093622 0.023334 -0.39245 0.34948 0.21217 0.7346 -0.21962 -0.028611 -0.34641 -0.20934 -0.27091 -0.17637 0.82396 -0.082339 -0.034869 0.079722 0.34841 0.60887 0.22811 -0.29633 0.18633 0.234 -0.70966 0.16312 -0.20857 0.092369 -0.075435 -0.13905 -0.35121 -0.19972 -0.41687 -0.31485 0.16123 0.038882 1.6654 -0.12401 -3.3419 0.10929 -0.026199 1.244 0.84374 -0.15679 0.79041 -0.042433 0.18884 0.064345 -0.11683 1.0467 0.71813 0.57834 0.27014 -0.50908 -0.083995 -0.1437 -0.76408 0.27418 0.56814 -0.39375 -0.32558 -0.92854 -0.13098 1.3277 0.11851 -0.15551 0.5972 -1.084 -0.058137 0.23886 0.14558 -0.59303 -0.47511 -0.22064 -0.37591 -0.79649 0.013465 -0.44595 -0.34623 -0.75398 -0.3517 0.99456 0.088196\n",
      "\n",
      "he 0.1225 -0.058833 0.23658 -0.28877 -0.028181 0.31524 0.070229 0.16447 -0.027623 0.25214 0.21174 -0.059674 0.36133 0.13607 0.18755 -0.1487 0.31315 0.13368 -0.59703 -0.030161 0.080656 0.26162 -0.055924 -0.35351 0.34722 -0.0055801 -0.57935 -0.88007 0.42931 -0.15695 -0.51256 1.2684 -0.25228 0.35265 -0.46419 0.55648 -0.57556 0.32574 -0.21893 -0.13178 -1.1027 -0.039591 0.89643 -0.9845 -0.47393 -0.12855 0.63506 -0.94888 0.40088 -0.77542 -0.35153 -0.27788 0.68747 1.458 -0.38474 -2.8937 -0.29523 -0.38836 0.94881 1.3891 0.054591 0.70486 -0.65699 0.075648 0.7655 -0.63365 0.86556 0.42441 0.14796 0.4156 0.29354 -0.51295 0.19635 -0.45568 0.0080246 0.14528 -0.15395 0.11406 -1.2167 -0.1111 0.8264 0.21738 -0.63776 -0.074874 -1.713 -0.8827 -0.0073058 -0.37623 -0.50209 -0.58844 -0.24943 -1.0425 0.27678 0.64142 -0.64605 0.43559 -0.37276 -0.0032068 0.18744 0.30702\n",
      "\n",
      "as -0.32721 0.096446 0.34244 -0.44327 0.30535 -0.042016 -0.071235 -0.31036 -0.22557 -0.181 -0.29088 -0.61542 0.29751 0.030491 0.41504 -0.51489 0.68628 -0.020302 -0.18486 0.31605 0.59472 -0.2147 0.29256 0.43262 0.35466 -0.29659 -0.27086 -0.48953 -0.047391 0.24521 -0.15783 0.59742 -0.41664 0.057632 0.1233 0.62326 -0.08844 0.3077 -0.15742 -0.28381 -0.58058 -0.022824 0.26689 -0.22565 0.47548 0.11134 0.37263 -0.14554 -0.16775 -0.79377 -0.30593 -0.10671 0.44199 1.5698 -0.73062 -2.7314 -0.19366 -0.32983 1.2881 0.62126 -0.255 0.8416 -0.23658 0.42594 0.86589 -0.35904 0.78162 0.20396 0.82898 0.0016123 -0.24008 -0.72735 -0.053671 -0.22264 0.31034 -0.21243 -0.14335 0.317 -0.80478 -0.49311 0.88023 -0.24147 -0.3922 0.15997 -1.5854 -0.25824 0.052834 -0.11983 -0.018874 -0.77356 0.049285 -0.25332 -0.3073 0.51295 -0.56802 -0.21239 -0.39741 -0.38165 0.43994 0.24683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "it -0.30664 0.16821 0.98511 -0.33606 -0.2416 0.16186 -0.053496 0.4301 0.57342 -0.071569 0.36101 0.26729 0.27789 -0.072268 0.13838 -0.26714 0.12999 0.22949 -0.18311 0.50163 0.44921 -0.020821 0.42642 -0.068762 0.40337 0.095198 -0.31944 -0.54651 -0.13345 -0.56511 -0.20975 1.1592 -0.194 0.19828 -0.11924 0.41781 0.0068383 -0.20537 -0.53375 -0.52225 -0.38227 -0.0065833 0.14265 -0.42502 -0.3115 0.0027352 0.75093 -0.48218 -0.18595 -0.77104 -0.046406 -0.06914 0.41688 1.3235 -0.81742 -3.3998 -0.11307 -0.34123 2.0775 0.61369 0.14792 0.93753 -0.10138 0.28426 0.97899 -0.32335 0.63697 0.58308 0.2282 -0.31696 0.21061 -0.6506 0.21653 -0.24347 0.55519 -0.34351 -0.095093 -0.14715 -1.2876 0.3931 0.30163 -0.21767 -1.1146 0.51349 -1.341 -0.30381 0.32499 -0.45236 -0.17743 -0.048504 -0.12178 -0.42108 -0.40327 0.038452 -0.36084 0.037738 -0.21885 -0.38775 0.36916 0.54521\n",
      "\n",
      "by -0.20875 -0.1174 0.26478 -0.28339 0.19584 0.7446 -0.03887 0.028499 -0.44252 -0.30426 0.27133 -0.51907 0.52183 -0.76648 0.28043 -0.48344 -0.15626 -0.49705 -0.51024 -0.03652 0.20579 -0.6136 0.46388 0.73497 0.66813 -0.4443 -0.17603 -0.5478 -0.013521 0.16333 0.28148 0.054223 -0.19906 -0.1907 -0.43179 0.14781 0.27555 0.18571 -0.40776 -0.15415 -0.5885 -0.0085281 -0.14178 0.7061 0.54031 -0.43305 0.17497 -0.46208 -0.31372 -0.34039 -0.25128 0.68228 0.33576 1.5862 -0.39427 -2.9938 -0.29773 0.04213 1.9075 -0.072628 -0.092191 0.66133 0.13868 0.78774 0.69307 -0.22185 0.71705 1.1453 1.2153 0.14196 -0.79914 0.16965 -0.34532 -0.51742 -0.15648 0.18757 0.1694 -0.0082713 -1.4511 0.061983 1.1019 0.084411 -0.34148 0.49994 -1.1106 -0.13759 0.15377 -0.061006 -0.53826 -0.78941 -0.12566 -0.57381 -0.73484 0.54774 -0.28455 -0.24348 -0.27511 -0.33267 0.27878 -0.8705\n",
      "\n",
      "at 0.1766 0.093851 0.24351 0.44313 -0.39037 0.12524 -0.19918 0.59855 -0.82035 0.28006 0.54231 0.023079 0.12837 -0.044489 0.3837 -0.75659 0.40254 -0.4462 -0.81599 -0.0091513 0.65219 -0.043656 0.54919 -0.16696 0.73028 -0.20703 -0.069863 -0.31259 0.27226 0.084905 -0.60498 0.42826 0.60134 0.50953 -0.39073 0.44608 -0.36331 0.50858 -0.20308 -0.43503 -0.086827 -0.86581 -1.0151 -0.35725 -0.12993 0.3324 0.3026 0.067277 -0.52948 -0.81223 0.39562 -0.79537 0.24331 1.2506 -1.0169 -3.3391 -0.79691 -0.33877 1.366 0.87513 -0.63701 0.68381 -0.057432 0.12541 -0.8258 -0.56117 0.30807 0.1545 0.61473 0.67403 -0.60833 -0.25911 -0.35619 -0.71189 -0.31207 0.035238 0.22488 -0.33492 -1.1586 -0.17373 0.95937 0.24479 -0.46205 -0.075941 -1.0844 0.093676 0.48546 0.13008 0.23455 -0.27964 -0.24481 -0.016213 0.46302 1.0291 -0.81817 0.17522 0.06797 0.056305 1.2312 0.40695\n",
      "\n",
      "( 0.19247 0.36617 0.52301 -0.79857 -0.2592 0.18267 0.19564 0.83148 -0.67636 -0.84648 1.4429 -0.84978 -0.023986 1.328 0.74061 0.039546 0.61659 -0.075604 -0.59537 0.69163 0.71303 0.016798 0.57518 0.94396 0.38447 -0.095771 0.40682 -0.1736 0.29918 0.023185 -0.76169 1.1022 0.25427 -0.59429 0.19951 0.0047585 0.55357 1.3464 0.46901 0.35119 -0.15608 -0.74119 -0.454 -0.4307 0.49688 -0.97074 -0.45785 -0.19753 -0.25268 -0.22272 0.69589 0.37725 0.70939 0.71797 -1.0762 -2.2584 0.21776 -0.086578 1.0148 0.4768 0.56119 0.80465 -1.0638 0.54387 0.84943 0.051364 -0.21503 -0.25736 0.67899 0.31385 -0.02978 0.38168 -0.68692 -0.18547 0.70286 -0.77652 -0.19728 0.45196 -1.2119 -0.28732 -0.027681 -0.66313 -0.39999 -0.29214 -0.72999 1.3007 -0.090283 0.093152 1.0336 -0.29074 -0.24114 0.21022 0.031178 0.20095 -0.75632 0.343 -0.024949 -1.2276 1.1152 -1.0234\n",
      "\n",
      ") -0.13797 0.27084 0.84036 -0.45668 -0.49429 0.35777 0.077772 0.42481 0.0076481 -0.50942 1.4008 -0.79993 0.053011 1.2054 0.3783 0.0842 0.91317 -0.35173 -0.30452 0.65606 0.50428 0.074796 0.31149 0.81957 0.40216 0.10982 0.39245 -0.47063 0.28423 0.13887 -1.0394 1.0172 -0.20146 -0.2723 0.071315 -0.48863 0.55583 0.85816 -0.04158 0.31446 -0.12432 -0.80119 -0.015955 -0.58919 0.32295 -1.0475 -0.53598 -0.77031 -0.39489 -0.27798 0.10422 -0.28293 0.72327 1.2162 -0.67242 -2.6729 -0.30428 0.039434 0.99252 0.29029 0.14169 0.29153 -0.47012 0.54204 0.71572 -0.29337 -0.034427 -0.059365 1.1216 0.41185 -0.094306 -0.21824 -0.027208 -0.53825 0.037405 -0.8403 0.035407 0.51239 -1.0315 -0.24264 -0.42759 -0.77984 -0.58269 0.026131 -0.94419 0.91568 0.16742 -0.45787 0.76123 -0.043248 -0.28311 0.063622 -0.24463 0.1474 -0.82823 0.57946 -0.1286 -0.96056 0.3906 -0.92805\n",
      "\n",
      "from 0.30731 0.24737 0.68231 -0.52367 0.44053 0.42044 0.0002514 0.15265 -0.61363 0.22631 0.083071 0.070425 0.017683 0.56807 1.0067 -0.46206 0.44524 -0.50984 -0.42985 0.19935 0.22729 0.51662 0.56282 0.41282 0.17742 -0.15694 -0.11505 -0.3805 0.4744 -0.16686 0.23153 0.063698 -0.10716 -0.26848 -0.42665 0.52237 0.095376 0.6402 -0.52221 -0.13856 -0.98307 -0.3532 -0.52161 0.11277 0.31634 0.13297 -0.049571 -0.13785 0.11317 -0.50644 0.38373 0.36698 0.39106 0.98143 -0.5441 -2.464 -0.68383 -0.96243 2.2017 0.56643 -0.04941 1.3093 -0.40073 0.8353 0.1744 0.044926 0.54118 -0.11038 0.382 0.15369 -0.37072 -0.13141 -0.52504 -0.56775 -0.16822 -0.091726 0.081418 0.045884 -1.4401 -0.16349 0.49361 0.2141 -0.7011 0.23067 -1.1803 0.065701 -0.046429 0.080979 -0.16424 -0.72896 -0.21221 0.034235 -0.40642 0.28826 -0.81331 -0.067997 -0.25439 0.13735 1.0103 -0.77614\n",
      "\n",
      "his 0.12883 -0.82209 0.27438 -0.069014 0.17989 0.72605 -0.15112 0.0085541 -0.95122 0.77243 -0.28375 0.28329 0.14825 -0.01223 -0.019267 -0.03446 0.31506 -0.16639 -0.013435 -0.0020459 0.064905 -0.20989 0.12524 0.3523 0.6404 0.05957 -0.80302 -0.81648 0.66134 0.05997 -0.061521 0.84922 -0.028733 0.2767 -1.0068 0.71758 -0.37257 0.43064 -0.49244 0.38683 -0.36828 0.027982 1.5346 -0.60533 -0.34449 -0.17069 0.29288 -0.53581 0.56035 -0.63013 -0.12308 0.093633 0.59336 1.5214 -0.092629 -3.1408 0.13931 -0.5382 1.1736 0.62318 0.43621 1.2856 0.12121 0.46206 0.56142 -0.41439 0.9436 0.38954 -0.053156 0.18622 -0.18785 0.37603 0.13878 -1.2881 0.18534 0.35157 -0.80888 -0.067662 -1.1934 0.20095 0.96297 0.92074 -0.030933 -0.11743 -1.521 -0.77539 -0.091178 -0.12774 -0.63958 -0.68099 -0.16037 -0.21732 0.57088 0.86688 -0.67851 -0.60641 -0.68927 -0.33961 0.42743 0.16575\n",
      "\n",
      "'' 0.16478 0.17071 0.62111 -1.2101 -0.84063 0.21893 0.48123 -0.15044 0.36701 -0.20857 -0.23385 0.019356 -0.045098 0.18001 0.11995 -0.25622 -0.026299 0.28473 -0.91322 0.59811 0.30248 0.27973 0.11444 -0.073628 0.88137 1.0633 -0.22116 -0.7982 -0.137 -0.2935 0.30011 -0.027594 0.13646 0.0495 0.066336 0.62306 0.4118 0.090881 0.20817 -1.1378 -0.15726 -0.27827 -0.16412 0.50816 -0.75323 -0.33559 -0.14433 -0.85375 -0.76168 -0.65671 0.063944 0.046424 0.15268 1.3671 0.057888 -2.7643 0.45043 0.95083 1.9512 0.66661 -0.32556 1.1692 -0.058985 -0.4976 0.82273 -0.43086 1.5947 0.97728 0.57046 -0.012486 0.63458 -0.67646 -0.22447 -0.41826 0.48309 -0.003709 0.040551 0.13703 -0.93292 -0.24249 0.69614 -0.1772 -0.49188 -0.1192 -1.3653 0.082923 -0.26056 -0.43172 -0.28799 -0.56304 0.065289 -0.56735 0.22621 0.47139 -0.48157 0.56438 0.34019 -0.44047 -0.10587 0.79509\n",
      "\n",
      "`` 0.092672 0.20241 0.69394 -0.50775 -0.097297 0.045522 -0.14156 0.30736 -0.35448 -0.20612 -0.21092 -0.0026685 -0.11537 0.052913 0.02908 0.0067036 0.47268 0.44669 -0.35419 0.70959 0.6984 0.42713 -0.40276 -0.37443 0.7434 0.67827 -0.53675 -0.96641 -0.42534 -0.26468 0.25737 0.57259 0.072823 0.45968 0.080224 0.34628 0.69218 0.28035 0.484 -0.36537 -0.3727 0.4901 -0.2787 -0.22158 -0.45082 -0.35663 -0.18381 -0.50743 -0.62373 -1.084 -0.14465 -0.049544 0.1102 0.81705 -0.85176 -2.8731 0.72268 0.31316 1.625 1.1221 -0.13595 1.0558 -0.49829 -0.62308 1.1854 -0.52551 1.1743 0.62574 -0.14725 0.25494 0.27639 -0.57139 -0.29194 -0.5177 0.18006 0.024181 0.16562 0.25703 -1.151 -0.18682 0.99148 -0.34432 -0.53266 0.47968 -1.8672 -0.52944 -0.049901 -0.48973 -0.60601 -0.63287 0.14499 -0.10127 0.25294 0.37802 -0.39809 -0.4138 -0.05448 -0.51348 -0.2775 1.1636\n",
      "\n",
      "an -0.4214 -0.18797 0.46241 -0.17605 0.36212 0.36701 0.27924 0.14634 -0.054227 0.45834 0.065416 -0.33725 0.067505 -0.36316 0.50302 -0.010361 0.72826 -0.17564 -0.33996 0.072864 0.64481 -0.23908 0.38383 0.13858 1.0994 -0.24883 -0.15078 -0.48738 -0.23042 0.064788 -0.70183 0.82654 0.06128 0.18531 -0.30162 -0.022151 0.34302 0.80331 0.17135 0.15462 -0.50759 0.39572 0.054291 -0.53081 0.48252 0.086205 0.59585 -0.22377 -0.3955 -0.73036 -0.10279 -0.39166 1.229 1.2129 -1.0365 -3.4971 0.10923 -1.0084 1.9998 0.7964 0.3881 0.43746 0.085194 0.38549 0.61993 -1.032 0.70119 -0.2246 0.079435 0.09126 -0.21196 -0.55429 -0.053352 -0.80201 0.46798 -0.05005 -0.57422 -0.084822 -1.7227 -0.94286 0.98667 0.31211 -0.37735 0.068674 -0.77838 -0.28486 0.81047 0.46596 -0.11865 -0.93411 0.33722 0.037906 -0.18273 -0.019941 0.20494 -0.47718 -0.49253 -0.56518 0.72558 -0.15913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "be -0.46953 0.38432 0.54833 -0.63401 0.010133 0.11364 0.10612 0.58529 0.032302 -0.12274 0.030265 0.52662 1.0398 -0.082143 0.19118 -0.83784 0.50763 0.44488 -0.72604 0.036893 0.24211 -0.28878 0.33657 0.13656 0.14579 -0.13221 0.098428 -0.45276 -0.13029 0.015762 -0.010161 0.4967 -0.28461 0.29655 0.92979 0.42447 -0.082773 0.30438 -0.39219 -0.30585 -0.43201 -0.27333 0.24388 -0.58081 0.22679 0.027226 0.53473 -0.37527 -0.16119 -1.1235 0.12768 -0.69898 0.41341 1.2291 -0.41248 -2.5173 -0.15354 -0.043107 1.9111 0.80754 -0.14759 0.9609 -0.84267 0.084422 1.2616 -0.10938 0.54846 0.75255 -0.071289 -0.73987 0.094808 -0.97589 0.0078721 -0.23928 0.2882 -0.41516 0.034366 0.1197 -1.2142 -0.11306 0.52847 -0.42273 -0.93378 -0.046645 -2.122 -0.341 0.64229 -0.10097 -0.22875 -1.0776 -0.68044 -0.26372 -0.18331 -0.051632 -0.30836 0.066537 0.20422 -0.68914 0.4511 0.25125\n",
      "\n",
      "has 0.093736 0.56152 0.48364 -0.45987 0.56067 -0.1694 0.018687 0.45529 0.065615 0.25181 -0.14251 0.10532 0.77865 0.1428 -0.08114 -0.069555 0.32433 0.019611 -0.15608 0.22235 0.35559 0.14713 0.19156 0.2803 0.27691 -0.2067 -0.11378 -0.48318 -0.64248 -0.35523 0.21939 1.2533 -0.21164 0.91811 0.31986 0.48367 0.15322 0.56109 -0.60692 -0.028075 -0.92199 -0.25583 0.66362 -0.49082 0.34757 -0.048103 0.57283 -0.62332 0.87508 -0.50079 -0.12316 -0.69096 0.10129 1.516 -0.174 -2.8902 -0.24541 -0.17934 1.1001 1.4198 0.49132 0.30282 0.077149 -0.097834 0.90586 -0.1615 0.55681 0.32817 0.49335 0.044815 0.57458 -0.32663 -0.29745 0.001807 0.24382 -0.51915 -0.14392 0.27921 -1.5964 0.37152 0.81129 -0.13488 -0.36534 -0.022346 -1.5091 -0.38727 0.30063 -0.37562 -0.18582 -0.39748 -0.10719 -0.12265 -0.66462 0.12112 -0.37281 0.60048 -0.42683 -0.81305 0.62397 0.73176\n",
      "\n",
      "are -0.51533 0.83186 0.22457 -0.73865 0.18718 0.26021 -0.42564 0.67121 -0.31084 -0.61275 0.089526 -0.24011 1.1878 0.67609 -0.022885 -0.92533 0.071174 0.38837 -0.42924 0.37144 0.32671 0.43141 0.87495 0.34009 -0.23189 -0.41144 0.49061 -0.32906 -0.49109 -0.18988 0.33408 -0.21245 -0.38386 -0.080547 1.1161 0.23617 0.31333 0.49286 0.1 -0.15131 -0.14176 -0.2802 -0.2388 -0.35486 0.18282 -0.19134 0.60544 0.074573 -0.20731 -0.60965 0.19908 -0.57024 -0.17427 1.4419 -0.25019 -1.8648 0.41671 -0.24607 1.501 0.87415 -0.67135 1.2762 -0.2721 0.17583 1.2242 0.28242 0.62375 0.63951 0.36914 -0.84677 -0.3227 -0.67152 -0.19635 -0.40789 -0.20966 -0.19623 0.041885 0.53967 -1.1105 -0.39515 0.6659 -0.233 -1.082 0.046465 -2.0993 -0.28493 0.080025 -0.12963 -0.30011 -0.46764 -0.81831 -0.048509 -0.32233 -0.32013 -1.1207 -0.056788 -0.73004 -1.2024 1.1304 0.3479\n",
      "\n",
      "have 0.15711 0.65606 0.0021149 -0.65144 -0.28427 -0.20369 -0.077596 0.40798 -0.03447 -0.1639 -0.21597 0.34178 1.196 0.33639 -0.21076 -0.56015 0.1507 0.34912 -0.97128 0.18152 0.74408 0.20029 0.66986 0.16085 -0.0013507 -0.55392 0.19411 -0.48043 -0.29777 -0.50765 0.80164 0.50424 -0.40524 0.53991 0.65686 0.2114 0.18575 0.80697 -0.20066 0.095714 -0.58899 -0.35907 0.27162 -0.51794 0.2347 -0.045999 0.56501 -0.40747 0.63377 -0.92266 -0.023418 -0.2504 -0.19576 1.3863 0.087314 -2.2309 0.15084 -0.18661 1.292 1.3259 -0.3018 1.2554 -0.41594 0.045082 1.2569 0.19148 0.53144 0.54904 0.16331 -0.66509 0.11798 -0.52498 -0.084623 -0.55866 -0.44294 -0.19599 -0.17698 0.3181 -1.4736 0.26293 0.96367 -0.25463 -0.70786 -0.11713 -2.3508 -0.47386 0.56258 -0.079965 -0.69856 -0.48524 -0.33081 -0.19205 -0.49695 -0.32643 -0.97207 0.21092 -0.58082 -0.60615 0.71005 0.41469\n",
      "\n",
      "but -0.057078 0.39874 0.68861 -0.68151 -0.45583 0.2008 0.17974 0.053648 0.43762 -0.026725 0.13383 -0.0078137 0.42207 -0.31801 0.18065 -0.35387 -0.30929 0.04066 -0.48854 0.3791 0.47955 -0.041942 0.40894 0.12419 0.40096 0.19545 -0.37819 -0.77684 -0.20677 -0.4313 -0.10095 0.39866 -0.29612 -0.083111 -0.019026 0.53927 0.0011912 0.30235 -0.36048 -0.48434 -0.47751 -0.33922 0.34788 -0.17484 -0.22613 -0.3291 0.81259 -0.58452 0.14509 -0.71497 0.17107 -0.24833 0.22104 1.5517 0.040869 -2.9103 -0.20812 -0.17625 1.6597 0.86277 -0.32527 0.65641 -0.13142 0.32312 0.90836 -0.29105 0.84975 0.53217 0.15041 -0.27983 -0.029015 -0.63378 0.12237 -0.79144 0.16108 0.017446 -0.35095 -0.16949 -1.0001 -0.036832 0.8114 -0.2271 -0.62133 0.16484 -1.6804 -0.39861 0.063602 0.10644 -0.57955 -0.45573 -0.037633 -0.63445 -0.30094 0.39828 -0.82883 0.33827 -0.23613 -0.19357 -0.030606 0.2397\n",
      "\n",
      "were 0.26874 0.17994 -0.29083 -0.72304 -0.05883 0.37211 0.39979 0.47827 -0.41014 -0.089043 0.68457 0.29088 0.9661 0.43289 0.44254 -1.1529 0.15147 -0.02307 -1.2467 -0.037292 0.94212 0.37771 1.2369 0.12327 -0.33831 -0.98651 0.44322 0.083459 -0.11953 -0.057447 0.6761 -0.59646 -0.3251 0.53957 0.66822 0.082015 0.42181 0.62666 0.038678 0.089652 -0.53395 -0.40426 -0.060807 0.14335 0.53841 -0.12983 0.43699 -0.077531 0.20441 -0.9894 -0.080389 -0.13893 0.046432 1.6775 -0.34565 -1.7503 -0.25442 -0.28207 1.2024 1.0927 -0.55076 1.3852 -0.74759 0.96273 0.69044 -0.41462 0.55676 0.39588 0.053647 -0.35503 -0.3909 -0.48323 -0.048448 -0.37728 -0.51204 0.50097 0.16188 0.91052 -1.6308 -0.31484 0.51824 -0.078027 -0.33929 0.42289 -2.3287 -0.56737 0.17769 -0.34047 -0.75328 -0.37805 -0.45665 -0.60386 -0.41089 0.078006 -1.3394 0.049803 -0.91783 -0.47655 0.79018 -0.28336\n",
      "\n",
      "not -0.19104 0.17601 0.3692 -0.50323 -0.47561 0.15798 -0.11679 0.21052 0.32652 0.12194 0.090944 0.26089 0.76294 0.00069673 -0.050001 -0.44853 0.36239 0.56345 -0.68702 0.33237 0.31285 -0.14207 0.35327 -0.16426 -0.10693 0.077786 -0.17704 -0.92897 0.1468 -0.13585 0.25682 0.66019 -0.35569 0.21838 0.38173 0.54337 0.10197 0.3523 -0.2551 -0.15155 -0.67434 0.16903 0.16413 -0.53843 -0.17457 -0.28539 0.74044 -0.67533 -0.23382 -1.3599 0.30225 -0.14968 0.27043 1.1979 -0.29556 -2.5395 0.0010303 -0.26272 1.8303 0.80008 -0.35691 0.56578 -0.5504 0.070845 1.4275 0.09016 0.7842 0.7849 -0.33538 -0.65751 -0.20112 -1.0297 0.069195 -0.61272 0.11373 -0.19547 -0.21256 0.049763 -1.1619 -0.064512 0.53146 -0.47384 -0.68709 0.13024 -2.0899 -0.41346 0.30364 -0.00057448 -0.18833 -0.54779 -0.32058 -0.36704 -0.1474 -0.19044 -0.47712 0.048228 -0.26215 -0.5968 0.080843 0.27866\n",
      "\n",
      "this -0.57058 0.44183 0.70102 -0.41713 -0.34058 0.02339 -0.071537 0.48177 -0.013121 0.16834 -0.13389 0.040626 0.15827 -0.44342 -0.019403 -0.009661 -0.046284 0.093228 -0.27331 0.2285 0.33089 -0.36474 0.078741 0.3585 0.44757 -0.2299 0.18077 -0.6265 0.053852 -0.29154 -0.4256 0.62903 0.14393 -0.046004 -0.21007 0.48879 -0.057698 0.37431 -0.030075 -0.34494 -0.29702 0.15095 0.28248 -0.16578 0.076131 -0.093016 0.79365 -0.60489 -0.18874 -1.0173 0.31962 -0.16344 0.54177 1.1725 -0.47875 -3.3842 -0.081301 -0.3528 1.8372 0.44516 -0.52666 0.99786 -0.32178 0.033462 1.1783 -0.072905 0.39737 0.26166 0.33111 -0.35629 -0.16558 -0.44382 -0.14183 -0.37976 0.28994 -0.029114 -0.35169 -0.27694 -1.344 0.19555 0.16887 0.040237 -0.80212 0.23366 -1.3837 -0.023132 0.085395 -0.74051 -0.073934 -0.58838 -0.085735 -0.10525 -0.51571 0.15038 -0.16694 -0.16372 -0.22702 -0.66102 0.47197 0.37253\n",
      "\n",
      "who 0.26164 0.4472 -0.096845 -0.74067 0.20805 0.33434 -0.26796 0.022865 -0.37251 0.22637 -0.22139 0.20357 0.34547 0.17839 0.15189 -0.09791 0.95879 0.34033 -1.0375 0.33589 0.29997 0.33378 0.56341 -0.04013 0.27697 -0.56576 -0.37009 -0.74752 0.62616 -0.54981 0.26671 1.3877 -0.17649 0.40599 0.48077 0.18777 -0.25752 1.0871 0.55904 0.12204 -1.4921 0.17677 0.90724 -0.7636 0.1091 -0.024877 -0.14734 -0.55559 0.74541 -0.59346 -0.19779 -0.42991 0.30361 1.2408 -0.19161 -2.0891 -0.2374 -0.33597 0.60638 1.4537 0.46323 0.53161 0.067068 -0.043899 0.7305 -0.20189 1.0433 0.55102 -0.25149 0.45955 -0.043735 -0.70793 -0.70816 -0.38365 -0.28454 -0.24853 -0.45679 -0.21562 -0.87828 -0.55946 1.2574 0.37382 -0.84276 0.015869 -1.8082 -0.87081 -0.37524 0.033488 -0.46289 -0.9618 0.28619 -0.58053 0.46974 -0.089393 -1.1858 0.3699 -0.58993 -0.4503 0.49525 -0.20298\n",
      "\n",
      "they -0.07954 0.30171 0.079516 -0.74662 -0.67879 0.35029 -0.19754 0.4929 0.14162 -0.23789 0.10939 0.23465 0.77763 0.12745 0.10873 -0.68024 0.25696 0.53981 -0.92294 0.088309 0.5524 0.073341 0.63424 -0.094834 -0.068988 -0.11287 -0.1932 -0.61233 0.16718 -0.43107 0.29355 0.42588 -0.22194 0.14787 0.53693 0.12846 0.12732 0.50899 0.2408 -0.3513 -0.52486 -0.37477 -0.084382 -0.39593 -0.14876 -0.030951 0.48431 -0.24678 0.12347 -1.1037 -0.09493 -0.038439 0.1075 1.6517 -0.10342 -2.4332 0.040486 -0.39164 1.5943 0.95891 -0.52496 1.1476 -0.57502 0.044767 1.097 -0.15884 0.59743 0.66712 -0.095168 -0.56173 -0.067523 -0.79894 0.066405 -0.82591 -0.58701 0.18065 -0.28443 -0.091645 -1.0481 -0.051988 0.75936 -0.039712 -0.9376 0.056433 -2.1626 -0.64355 0.42302 -0.10572 -0.72611 -0.1294 -0.38103 -0.7459 -0.0096764 -0.16205 -1.0562 -0.11999 -0.60909 -0.16731 0.65344 0.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "had 0.63256 -0.12718 -0.084182 -0.30718 -0.2526 -0.16172 0.47123 0.46553 -0.051526 0.17231 0.42743 0.57854 0.64548 0.31367 0.32752 -0.47608 0.25888 0.035845 -0.95154 -0.20671 0.65171 0.010712 0.3894 0.069552 0.061198 -0.72152 -0.22334 -0.34747 -0.1434 -0.32482 0.79539 0.84708 -0.046052 0.74384 0.18185 0.18666 0.17123 0.93485 -0.1299 0.39219 -1.016 -0.27859 0.79293 -0.40433 0.41505 0.017283 0.2936 -0.72944 0.98233 -0.96504 -0.19016 -0.010012 0.26765 1.3316 -0.19376 -2.4401 -0.39477 -0.36232 0.88315 1.5974 0.31113 0.72201 -0.55023 0.54995 0.6846 -0.40378 0.52677 0.23314 -0.12764 0.12305 -0.12489 -0.25052 0.11632 -0.41648 -0.19738 0.093786 -0.24532 0.29676 -1.6156 0.38933 0.68466 0.048371 -0.4177 0.10663 -2.0738 -0.81679 0.56651 -0.32536 -0.76685 -0.27123 -0.10798 -0.61763 -0.27098 0.077119 -0.89352 0.17811 -0.50156 -0.30966 0.22378 0.038183\n",
      "\n",
      "i -0.046539 0.61966 0.56647 -0.46584 -1.189 0.44599 0.066035 0.3191 0.14679 -0.22119 0.79239 0.29905 0.16073 0.025324 0.18678 -0.31001 -0.28108 0.60515 -1.0654 0.52476 0.064152 1.0358 -0.40779 -0.38011 0.30801 0.59964 -0.26991 -0.76035 0.94222 -0.46919 -0.18278 0.90652 0.79671 0.24825 0.25713 0.6232 -0.44768 0.65357 0.76902 -0.51229 -0.44333 -0.21867 0.3837 -1.1483 -0.94398 -0.15062 0.30012 -0.57806 0.20175 -1.6591 -0.079195 0.026423 0.22051 0.99714 -0.57539 -2.7266 0.31448 0.70522 1.4381 0.99126 0.13976 1.3474 -1.1753 0.0039503 1.0298 0.064637 0.90887 0.82872 -0.47003 -0.10575 0.5916 -0.4221 0.57331 -0.54114 0.10768 0.39784 -0.048744 0.064596 -0.61437 -0.286 0.5067 -0.49758 -0.8157 0.16408 -1.963 -0.26693 -0.37593 -0.95847 -0.8584 -0.71577 -0.32343 -0.43121 0.41392 0.28374 -0.70931 0.15003 -0.2154 -0.37616 -0.032502 0.8062\n",
      "\n",
      "which 0.03024 0.44606 0.43166 -0.37528 0.29068 0.23032 0.18125 0.40201 0.13518 -0.19562 0.30639 -0.13239 0.67897 0.42234 0.32637 -0.15281 0.37698 -0.23303 -0.33817 0.30588 0.44918 -0.83624 0.59146 0.24958 0.39986 -0.50172 -0.23544 -0.14696 -0.35144 -0.56852 0.08954 0.82612 -0.26586 0.3903 -0.036849 0.48257 0.71664 0.11004 -0.59354 -0.33216 -0.25736 -0.34531 -0.026326 -0.23747 0.00019656 -0.2748 0.38512 -0.39581 0.11404 -0.25174 -0.3247 0.089608 0.24929 1.5127 -0.19762 -2.8509 -0.53833 -0.47111 1.7859 0.78126 -0.12963 0.56077 0.32151 0.35571 0.84547 0.14931 0.11487 0.30625 0.54774 -0.50426 0.33824 -0.62043 -0.012869 0.066666 0.062731 -0.44534 0.15541 0.21801 -1.732 0.42054 0.36319 -0.072258 -0.74811 0.19888 -1.4461 -0.27576 0.26646 -0.57838 0.56151 -0.028701 -0.2466 -0.425 -0.57154 0.31939 -0.22075 0.46528 -0.16606 -0.79923 0.80849 0.37378\n",
      "\n",
      "will -0.26703 0.44911 0.55478 -0.69003 0.046175 -0.43044 -0.29348 1.0149 -0.33757 -0.096388 -0.28176 0.41828 0.58357 -0.078788 -0.23511 -0.74174 0.68242 0.77152 -0.80698 0.13537 0.19157 -0.51766 0.30764 0.68624 0.15603 -0.13725 0.064215 -0.25319 0.33305 -0.61336 -0.60918 0.72316 -0.29642 0.34352 0.36234 0.74162 0.30484 0.41837 -0.35164 -0.41675 -0.32865 -0.56729 -0.11949 -0.70703 -0.20916 0.050621 0.057078 -0.56363 0.032941 -1.1897 0.0467 -0.48235 -0.12548 1.088 0.095233 -2.5264 -0.15569 -0.081787 1.9323 0.75413 -0.21121 0.7779 -0.41152 -0.019988 0.83285 0.45991 0.19578 1.1235 -0.21463 -1.0439 0.64653 -1.0831 -0.47316 -0.76509 0.22128 -0.38609 -0.24266 0.28377 -0.8266 -0.022102 0.9439 -0.40069 -0.65322 -0.053926 -1.6434 -0.57158 0.50725 0.24536 0.3272 -0.53953 -0.12521 0.071234 -0.14235 -0.41365 -0.68288 0.22851 0.56056 -0.87247 0.83828 0.465\n",
      "\n",
      "their 0.17137 -0.33437 0.11471 -0.52008 -0.79816 0.48592 -0.73885 0.28653 -0.6273 0.26786 -0.32081 0.11757 0.61241 0.25964 -0.21165 -0.63307 0.40135 0.26175 -0.16827 -0.0092904 0.35687 -0.31246 0.79867 0.54921 0.17423 -0.30704 -0.23997 -0.64167 0.36679 -0.27327 0.64719 0.32952 -0.46198 0.057414 -0.088337 -0.027794 -0.074753 0.52992 -0.48819 -0.17822 0.045592 -0.39832 0.5287 0.01556 0.23284 -0.011029 -0.15533 -0.065942 0.22879 -1.0154 -0.2172 0.26772 -0.32618 1.6999 0.055699 -2.586 0.75177 -0.76514 1.9965 0.28489 -0.1825 1.3012 -0.074158 0.22211 0.8391 0.12388 0.4363 0.4045 0.25744 -0.63188 -0.34572 -0.097293 0.14742 -1.5755 -0.098063 0.36815 -0.83086 -0.41775 -1.3509 0.11379 0.76722 0.75707 -0.24826 0.4637 -1.8673 -0.46703 -0.01854 0.010588 -0.76759 0.21896 -0.46664 0.32206 0.2232 0.30753 -1.179 -0.85791 -1.0137 -0.11272 1.1948 0.53682\n",
      "\n",
      ": -0.54558 1.0965 1.5106 -0.4727 -0.15547 0.21748 -0.36125 0.12727 -0.51148 -0.35528 0.59787 -0.53174 -0.046401 0.65336 0.10376 0.30513 0.38383 -0.22857 -0.78904 1.0066 0.20339 -0.0020182 0.21937 0.64771 0.40531 0.58813 -0.022486 -0.43952 0.15482 -0.48849 -0.60316 0.36198 -0.45655 -0.51369 -0.28462 0.19641 0.42976 0.46083 -0.22081 -0.34077 -0.21216 -0.36246 -0.1494 -0.66431 -0.13962 -0.25126 -0.4993 -0.29225 -0.36749 -0.42177 0.35742 0.28964 -0.14963 1.3742 -0.80804 -2.7821 0.12325 0.41852 1.0374 0.50126 -0.98822 0.76788 -0.30979 -0.55975 1.0845 -0.18401 0.26683 0.39754 0.22833 0.43425 -0.12324 0.21173 -0.40471 -0.44548 0.27367 -0.1379 0.69137 0.32499 -1.361 -0.9774 0.48218 -0.76722 -0.40092 -0.2129 -0.82015 -0.039415 -0.37735 -0.57144 0.070427 -0.091096 0.016801 0.38154 0.39406 0.38044 -0.58943 -0.021365 0.17364 -0.90374 0.48136 0.030378\n",
      "\n",
      "or 0.31039 0.64859 0.28481 -0.46756 -0.25715 0.71389 -0.064742 0.30187 0.52801 0.19849 0.20011 -0.072665 0.23712 0.72137 0.65764 -0.42129 0.18458 -0.11517 -0.4138 0.54855 0.13857 -0.090728 0.3171 0.26416 0.53625 0.068179 -0.096772 -0.64557 0.12414 -0.28334 0.39239 0.57338 -0.58856 -0.44752 0.22755 0.61385 0.07508 -0.11876 0.070905 0.12847 -0.40008 -0.66973 -0.24658 -0.69349 -0.20067 -0.1633 -0.22338 -0.032656 -0.52169 -1.4363 0.068198 0.27494 0.39587 1.3418 -0.66544 -1.9608 0.27415 -0.13552 2.252 0.7521 -0.28896 0.83412 -0.29035 0.291 0.81412 -0.38679 0.71252 -0.29467 0.5797 -0.31411 -1.0094 -0.32144 -0.033035 -0.46199 -0.051451 0.057455 -0.16413 0.28121 -0.58346 0.11114 0.2778 -0.13717 -0.66281 0.45848 -1.6707 0.36448 0.31026 0.48856 0.88709 -0.20468 -0.62125 -0.021678 0.14065 0.10319 -0.74186 -0.03031 -0.25264 -0.88554 0.91767 -0.57253\n",
      "\n",
      "its 0.20589 -0.3171 0.74431 0.047407 -0.10826 -0.18517 -0.47992 0.27736 -0.47515 0.63156 0.035472 0.073577 0.36199 -0.1923 -0.43884 -0.32988 0.44746 -0.39097 0.46493 0.12546 0.38224 -0.45183 0.29384 0.62979 0.32395 -0.22369 -0.16764 -0.30502 -0.10406 -0.27813 0.32921 0.62754 -0.57658 0.12239 -0.56023 0.44731 0.48498 -0.20299 -0.92786 -0.22441 0.21687 -0.26173 0.50944 0.30419 0.5882 0.40474 0.57113 -0.24145 -0.076213 -0.61215 -0.016607 0.10699 -0.083368 1.4618 -0.58467 -3.2127 0.41215 -0.79032 2.7901 -0.39825 0.11872 0.58777 0.25648 0.42427 0.63226 -0.1714 0.11396 0.61377 0.65834 -0.21013 0.24094 -0.036859 -0.2394 -0.87062 0.2499 -0.55732 -0.494 -0.12043 -1.6417 0.43463 0.29555 0.075822 -0.3107 1.0174 -1.115 -0.31493 0.41304 -0.1616 0.070745 0.21949 0.034177 0.78235 -0.39665 0.34179 -0.73382 -0.45151 -0.81274 -0.67858 1.2779 0.86475\n",
      "\n",
      "one -0.22557 0.49418 0.4861 -0.4332 0.13738 0.50617 0.26058 0.30103 -0.091486 0.10876 0.3058 0.051028 0.22303 0.054236 0.068838 -0.24701 0.32689 -0.082203 -0.28866 0.3734 0.73804 -0.040969 0.040201 0.11384 0.69987 -0.49745 -0.06755 -0.42599 -0.10725 -0.010697 -0.01479 0.55976 0.3064 0.053053 0.058034 0.32756 -0.37233 0.46513 0.14285 -0.085003 -0.45476 0.19773 0.6383 -0.31148 0.10858 0.31557 0.36682 -0.35135 -0.48414 -0.33235 -0.33816 -0.39678 0.1908 1.3513 -0.39044 -2.8795 -0.14276 -0.087754 1.7713 0.99332 -0.14129 0.94389 0.050897 0.47373 0.86387 -0.16162 0.67199 0.52344 0.14438 -0.055194 -0.34669 -0.20742 0.18907 -0.19845 0.34862 0.10121 -0.092119 -0.66258 -1.0582 -0.11803 0.70171 0.077776 -0.50546 0.032243 -1.6176 -0.29302 -0.061748 -0.32473 0.3439 -0.44698 0.085689 0.13295 -0.1807 -0.11854 -0.82985 0.13784 -0.34359 -0.45744 0.49646 0.34906\n",
      "\n",
      "after 0.37711 -0.34471 0.13405 -0.01171 -0.19427 0.41464 0.40608 0.43063 -0.05706 -0.19921 0.43267 -0.016269 0.2171 -0.0026149 0.39424 -0.42803 -0.017495 -0.56658 -0.44558 -0.18529 0.26732 -0.15712 0.21657 0.79714 0.69623 0.20405 -0.49907 -0.45519 0.3821 0.20603 -0.21606 0.10093 -0.50148 -0.11058 -0.43455 -0.26785 -0.20234 0.003832 -0.49108 -0.17642 -0.88971 -0.279 0.86387 -0.017356 0.3121 0.41004 0.23199 -0.60812 0.44763 -0.89579 -0.038491 -0.25772 0.39468 1.6186 -0.54882 -3.0291 -0.77845 -0.32463 1.7658 0.97303 -0.39342 0.54811 0.013164 0.3785 0.24538 0.031079 0.23628 0.28901 0.027047 0.28985 -0.74523 0.011517 -0.39456 -0.57706 -0.63604 0.31022 -0.38317 -0.077663 -1.3539 0.018009 0.85646 0.038259 -0.39437 0.44331 -1.0802 -0.43159 0.14391 0.11854 -0.56459 -0.47966 0.2286 -0.24369 -0.42823 1.0366 -0.83071 0.1246 0.2063 0.54232 0.11425 -0.66927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new -0.043959 0.18936 0.6611 -0.49007 0.32211 -0.34161 -0.06848 0.31364 -0.71142 0.57436 -0.33588 -0.52279 -0.39075 -0.089694 0.46371 -0.3561 0.84576 -0.026188 -0.19328 -0.083846 0.31806 -0.19812 0.30009 0.069189 0.5447 -0.59193 0.54221 -0.62876 -0.53447 0.42334 0.030869 0.97164 -0.56222 0.045752 -0.571 0.080185 -0.081434 -0.6026 0.16466 -0.40281 -0.47701 -0.5195 0.12777 -0.43775 0.26602 0.48752 -0.06022 -0.52622 0.37687 -0.18007 0.030166 -0.094577 0.1633 0.59041 -0.48877 -3.423 0.13113 -0.080386 1.8978 0.18857 -0.573 0.86358 0.0021116 0.3606 0.80475 -0.13954 -0.053935 0.38873 0.30673 -0.31395 0.083238 -0.41737 -1.0998 -0.88005 0.2155 -0.26132 -0.10091 0.079584 -1.2341 -0.65281 0.63363 -0.098491 0.33518 0.26332 -0.96427 -0.01415 0.30849 -0.31418 -0.40793 -0.429 0.085451 -0.20073 0.05505 -0.040922 -0.94015 0.069544 -0.45397 -0.14168 0.92789 0.59058\n",
      "\n",
      "been -0.12135 0.15341 -0.014315 -0.50695 0.30361 0.080512 0.39152 0.2933 0.035886 -0.15228 0.09502 0.3393 1.2554 0.11321 -0.051129 -0.57003 0.31498 -0.186 -0.57884 0.0078366 0.54006 0.4534 0.68016 0.22213 0.075292 -0.47016 0.082904 -0.24712 -0.388 0.21901 0.37985 0.15801 -0.30129 0.73229 0.56992 0.086605 -0.20076 0.36037 -0.23286 -0.066291 -0.81877 -0.03379 0.62641 -0.20622 0.56038 0.11993 0.69504 -0.40375 0.52841 -0.86177 -0.010281 -0.56692 0.3033 1.414 -0.44229 -2.3658 -0.36824 -0.28689 1.3539 1.0304 -0.0017468 0.99368 -0.4248 0.2768 1.0864 -0.085121 0.65162 0.18836 0.16279 -0.12945 0.072747 -0.53828 -0.15001 -0.077713 -0.27551 -0.32725 0.15729 0.48495 -1.7036 0.20834 0.535 -0.23635 -0.63572 0.037145 -2.1431 -0.38923 0.32525 -0.28377 -0.77066 -0.79318 -0.44232 -0.42136 -0.46842 0.13889 -0.51673 0.21648 -0.32296 -0.50936 0.5068 0.080349\n",
      "\n",
      "also -0.33819 0.064568 -0.032558 -0.29448 0.84125 -0.29092 -0.35264 0.35777 0.004152 -0.0067549 -0.11512 -0.38832 0.49764 0.47187 0.046247 -0.059806 0.59317 -0.080286 -0.45926 0.28211 0.33909 -0.25741 0.30599 0.53594 0.1168 -0.30916 -0.16143 -0.1841 -0.26339 -0.035592 -0.13136 1.1538 -0.61616 0.73314 0.46168 0.4241 0.2918 0.73092 -0.17098 -0.03529 -0.6867 -0.24653 0.34776 -0.46747 0.21257 -0.052958 0.1032 -0.52122 0.61087 -0.71005 -0.16765 -0.34415 0.27119 1.1337 -0.33195 -2.3864 -0.52352 -0.25531 0.80993 1.3563 -0.1452 0.32792 0.11149 0.17806 1.0008 -0.37243 0.3127 0.28634 0.47915 -0.23534 0.13146 -0.5478 0.054173 -0.19163 0.16276 -0.067267 -0.0044537 0.55708 -1.2568 -0.063385 0.62438 -0.28284 -0.6458 -0.2832 -1.8987 -0.5706 0.026083 -0.41721 0.29686 -0.18416 -0.19252 -0.59915 -0.17981 0.17649 -0.56043 0.48284 -0.44081 -0.84036 0.78533 0.36017\n",
      "\n",
      "we -0.17791 0.62675 0.4787 -0.55295 -0.84935 -0.070802 -0.34724 0.4628 0.12611 -0.24875 0.46881 0.083636 0.56065 -0.21931 0.015561 -0.55806 -0.20738 0.9123 -1.2034 0.30115 0.46676 0.483 -0.10204 -0.56799 -0.027126 0.40567 -0.14058 -0.55485 0.094588 -0.62213 -0.30343 0.60639 0.049799 0.22204 0.48549 0.17629 -0.090535 0.53705 0.2755 -0.78827 -0.70953 -0.16678 0.11206 -0.48491 -0.66644 0.083952 0.32885 -0.45851 -0.37208 -1.5315 0.12994 -0.2409 -0.17219 1.374 -0.22313 -2.615 0.35201 0.33597 1.6117 0.92947 -0.37535 0.82034 -1.0677 -0.45329 1.2332 0.23749 0.63523 0.82859 -0.1744 -0.5853 0.56339 -0.73094 0.30815 -1.0888 0.46139 0.045386 -0.17827 -0.054054 -0.8831 0.033935 0.63083 -0.19741 -0.99051 0.20022 -1.9266 -0.25884 0.10367 -0.34129 -0.93507 -0.54666 -0.40171 -0.37783 -0.065771 -0.13836 -0.91872 -0.055635 -0.080557 -0.19526 0.20078 1.0953\n",
      "\n",
      "would 0.039741 0.035052 0.32988 -0.5879 -0.25054 -0.40719 -0.13904 0.47393 0.059905 0.089996 -0.21521 0.56533 0.30099 -0.28658 -0.05573 -0.78724 0.48656 0.58321 -0.86199 0.20924 0.4674 -0.36396 0.30526 0.042185 -0.053602 -0.27202 -0.28585 -0.5417 0.29678 -0.40868 0.097381 0.81001 -0.20864 0.18478 0.18757 0.8558 0.44071 0.25218 -0.18148 -0.19156 -0.58345 -0.22581 0.0017304 -0.50861 -0.23616 -0.016889 0.4246 -0.50566 -0.1471 -1.2339 0.012578 -0.17898 0.12862 1.2898 -0.14519 -2.5557 -0.069128 -0.47917 1.8359 1.0345 -0.13604 0.58236 -0.48874 0.11841 1.1859 0.10033 0.2632 1.0095 -0.34785 -0.9308 0.41517 -1.1774 -0.33477 -0.52506 0.047776 -0.2339 -0.22857 0.31005 -1.0655 0.051151 0.72159 -0.52556 -0.45882 -0.16729 -1.8869 -0.48996 0.82211 0.0077917 0.072314 -0.46795 -0.13359 -0.32797 -0.27505 -0.31342 -0.67763 0.015973 0.28487 -0.26137 0.55433 0.2451\n",
      "\n",
      "two -0.20154 0.32739 0.0004758 -0.22452 0.4411 0.33599 0.40657 0.66527 -0.63852 0.044351 0.59747 -0.086113 0.39787 0.88041 0.17005 -0.14275 -0.1792 -0.11622 -0.56175 0.21378 0.95272 -0.27654 0.22567 0.4587 0.83297 -0.90489 -0.24019 -0.41669 -0.31842 0.33599 0.17516 0.24785 -0.36002 -0.11895 0.41239 0.19549 0.071716 0.31329 -0.36803 0.51546 -0.44828 -0.2977 0.51132 -0.24885 0.15397 0.28301 -0.12827 -0.58284 -0.16847 0.029912 -0.65801 -0.41906 -0.14271 1.4463 0.050253 -2.6762 0.064998 -0.18747 1.8076 1.5002 -0.68615 1.1491 0.36129 0.50156 0.33909 0.1437 0.41324 0.75141 0.0079898 -0.017592 -0.58609 -0.095992 0.12828 -0.60489 0.27201 0.38497 -0.28595 -0.50807 -1.1539 -0.065888 1.1285 0.28462 -0.24782 0.44859 -1.4726 -0.12682 0.18053 -0.047814 0.016943 -0.22543 -0.22757 0.29341 -0.86753 -0.015261 -1.2675 0.020593 -0.72345 0.023054 0.54016 -0.080331\n",
      "\n",
      "more -0.38469 0.99338 0.13398 -0.32708 -0.07744 0.17769 -0.071985 0.1616 -0.1377 0.051739 0.15964 0.016507 -0.049616 -0.53964 0.24449 -0.62066 -0.345 -0.015009 0.059399 0.79348 1.2096 -0.094457 0.14585 -0.063804 0.1468 -0.50725 -0.15582 -0.69462 -0.18542 -0.20292 0.011547 0.39695 -0.45813 -0.19921 0.32108 0.54069 -0.0073385 0.12096 -0.77902 0.42854 -0.53546 -0.58143 0.14424 -0.47396 -0.20623 -0.20815 0.54938 -0.51741 0.09016 -0.75701 -0.063903 -0.73684 -0.097376 1.2635 -0.51026 -2.5686 0.4768 -0.54214 2.1144 0.49177 0.21146 1.5787 -0.28596 0.051544 0.4962 -0.26324 0.82709 0.50339 0.90994 -0.28113 0.020357 -0.63306 -0.33049 -0.17052 0.66252 -0.055619 -0.37653 -0.15417 -1.1648 -0.15672 1.2909 0.12161 -0.48802 0.47592 -1.5741 0.17322 0.62257 -0.041003 -0.52627 -0.55419 0.55319 0.4197 -0.41377 -0.55962 -0.81527 -0.27972 -0.85805 -0.32359 0.62028 0.46397\n",
      "\n",
      "' -0.34562 -0.24993 0.58678 -0.89119 -1.0954 -0.45078 -0.074549 -0.44779 -0.38492 -0.49234 -0.45656 -0.13329 0.18579 0.5663 -0.20201 -0.39506 0.21791 1.0369 -0.27008 0.5163 0.90963 0.638 0.068316 0.39001 0.24545 0.23652 -0.45476 -0.62325 0.38835 -0.33045 0.80349 0.3418 -0.4699 0.41377 -0.07501 -0.36963 -0.27099 1.2762 -0.21643 -0.4014 0.26181 -0.044938 0.32548 0.040404 0.62208 -0.1977 -1.1014 -0.2059 0.83509 -0.50192 -0.65494 0.42921 -0.32926 1.4271 -1.4865 -2.3979 0.93493 0.22123 1.3821 0.032328 -0.3801 0.74577 -0.28081 -0.90269 0.88931 0.16672 0.5924 0.2686 -0.76482 -0.21685 -0.090338 0.4549 -0.77978 -1.3978 -0.20202 -0.35109 0.087548 0.082081 -1.1563 -0.47656 1.0378 0.11395 0.11529 -0.72876 -2.192 -0.18326 -0.36075 -0.5495 0.33266 0.71664 -0.30022 -0.068747 -0.014708 0.51838 -0.020845 -0.42555 -0.19324 -1.3106 1.0294 -0.058794\n",
      "\n",
      "first -0.020102 0.037514 0.35363 0.16576 0.094826 0.087456 0.79498 0.20762 -0.58127 -0.077115 0.19662 -0.40312 -0.078157 0.29174 0.16982 0.0039022 0.73646 0.39023 -0.097257 -0.17307 0.63939 -0.28145 0.11622 0.67527 0.63166 -0.78246 -0.052301 -0.27931 0.46076 0.12461 -0.72262 0.73386 0.12163 0.23468 0.037115 0.12662 -0.56842 0.62871 -0.22405 0.11521 -0.57071 0.18036 0.42331 0.041353 0.36782 0.26287 0.47149 -0.83604 0.33438 -0.72676 -0.50733 0.033712 0.3284 0.86322 -0.45689 -3.1522 -0.53969 0.033392 1.5954 0.7581 -0.52425 0.61545 -0.26716 0.4432 0.95084 -0.29942 -0.0040878 0.39697 0.022105 -0.12852 -0.11692 0.1899 -0.19811 -0.4304 -0.075405 0.43592 -0.62717 -0.25134 -1.2143 -0.40294 0.51429 0.1677 -0.31839 0.38681 -1.4578 -0.43424 0.39687 -0.50085 0.48592 -0.40226 0.0056424 0.037021 -0.19116 0.31763 -0.74249 -0.097248 -0.19801 0.062608 0.28386 -0.31632\n",
      "\n",
      "about 0.66039 0.63888 0.86264 0.27455 -0.89222 0.64171 -0.27304 0.29033 -0.057512 -0.24491 0.62227 0.49926 0.227 -0.59077 0.90674 -0.69621 0.19357 -0.2317 -0.24986 0.67362 0.70398 0.053391 0.17195 -0.059168 -0.024024 -0.074141 0.14598 -0.23227 -0.17126 -0.043476 -0.41067 0.26134 0.4208 -0.12309 -0.82775 0.42115 0.05912 0.50765 0.21512 0.36136 -0.90275 -0.69712 0.12395 -0.23245 -0.4932 -0.39441 0.15376 -1.0667 -0.10011 -0.77981 0.58789 -0.1514 0.1825 1.5862 -0.94788 -2.5911 0.27182 -0.57057 1.5857 0.72034 0.64183 1.5991 -0.22208 -0.20578 0.55937 -0.11309 1.0917 0.77839 0.59488 0.36718 -0.0063842 -0.13847 -0.49184 -0.2293 -0.091083 0.14165 0.51691 0.094609 -1.4873 0.3587 0.76758 -0.01885 -0.09132 0.00024154 -1.0555 -0.21219 0.38834 -0.0097613 -0.43296 -0.50509 0.28889 0.50233 -0.3463 -0.61814 -0.66255 -0.043362 -0.44525 -0.55901 1.0752 0.577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "up 0.21469 0.43367 0.33964 -0.65715 0.15546 0.15318 -0.62081 0.27839 -0.3704 0.0029626 0.37131 0.32756 -0.32802 0.10206 0.52715 -0.33415 -0.012657 0.20382 -0.19846 0.10483 0.72682 0.30136 0.73955 0.2264 0.5213 -0.46339 -0.56209 -0.47684 0.056159 -0.46364 -0.18426 0.15954 0.23868 -0.030124 -0.18315 0.27942 0.031251 -0.16198 -0.18941 0.2571 -0.48811 -0.70303 -0.0055224 -0.63184 -0.17694 0.38916 -0.64778 -0.08909 0.17655 -1.2462 -0.21257 -0.20355 0.11958 1.6196 -0.77112 -2.8367 -0.21148 0.11873 2.1393 0.78805 0.41318 0.97607 -0.67157 0.29821 0.12548 0.10129 0.69104 0.61075 0.58256 0.3346 0.042307 -0.45933 -0.24029 -0.73154 -0.3054 0.19878 -0.34562 0.0035721 -0.57002 0.027172 0.68865 0.4502 -0.62077 0.36449 -1.2001 0.15149 0.58623 0.35867 -0.22877 -0.032302 -0.18218 0.18319 -0.34823 -0.36982 -0.61882 -0.38964 0.0028948 0.046601 0.83004 0.40299\n",
      "\n",
      "when 0.073242 0.11134 0.62281 -0.35905 -0.70731 0.43756 0.12819 0.13478 0.34282 -0.31661 0.58363 -0.093659 0.5544 -0.038733 0.62641 -0.51071 -0.038933 0.026205 -0.49006 -0.17801 0.14975 -0.2256 0.37801 0.2171 0.75329 0.19316 -0.55328 -0.7362 0.33339 -0.23334 -0.098592 0.8545 -0.07557 -0.0081974 -0.49419 -0.075416 -0.13138 0.12209 0.034606 -0.4088 -0.69892 -0.49642 0.51698 -0.14214 0.041307 0.2919 0.21277 -0.62993 0.35931 -0.71405 -0.1883 -0.0034622 0.39229 1.4246 -0.16196 -2.8915 -0.53611 0.086307 1.6249 0.77567 -0.25686 1.0327 0.15666 0.47451 0.72194 -0.22697 0.53295 0.26461 0.29597 0.33638 -0.14925 -0.44578 0.18481 -0.52155 -0.22525 0.17517 0.053832 -0.46764 -1.0003 -0.1603 0.6986 -0.12426 -0.69526 0.081973 -1.3761 -0.49588 0.059994 -0.23035 -0.571 -0.61682 -0.041618 -0.63177 -0.31651 0.54411 -0.81656 0.0089473 0.033319 0.53417 -0.1646 -0.27516\n",
      "\n",
      "year 0.44234 0.48431 0.37284 -0.52861 0.21558 -0.4629 0.60307 0.71816 -0.41382 0.15157 -0.02936 -0.012905 0.24569 -0.02638 0.00028137 -0.41139 0.16784 -0.094465 -0.39723 0.25608 0.39144 -0.14293 0.1543 1.1509 0.51744 -0.4226 -0.07987 -0.77108 0.078489 -0.49301 -0.14824 0.16366 0.23525 -0.32219 -0.50613 0.68354 -0.39673 0.48081 -0.59104 -0.19323 -0.28618 -0.16291 0.48853 0.037437 0.039265 0.02585 0.47258 -1.3542 0.026895 -1.2436 -0.13284 -0.9807 0.39823 1.1715 -0.49008 -2.8411 -0.1709 -0.2184 2.0011 0.59575 -0.36505 0.7859 -0.45694 0.070966 0.18904 0.22404 0.060091 0.050271 0.82711 -0.24997 0.46051 -0.11298 -1.0912 0.14487 -0.83451 0.12452 -0.26793 0.02768 -1.4872 -0.1812 0.8862 -0.053061 -0.49827 0.31571 -1.1596 -0.18781 0.15968 -0.57221 -0.047602 -0.32081 -0.070405 0.36944 -0.42656 -0.080083 -0.87588 0.28568 0.099353 0.38253 0.47456 -0.6505\n",
      "\n",
      "there -0.084215 0.69597 0.28383 -0.22497 -0.55923 0.21196 -0.15175 0.31601 0.24803 -0.36385 0.2252 0.34109 0.61438 0.08318 0.74894 -0.38785 -0.27211 0.2656 -0.66332 0.45571 0.34949 0.38635 0.25707 -0.60101 -0.047292 -0.41198 0.38246 -0.52151 0.037757 -0.42492 -0.36352 -0.037377 0.31263 0.19709 0.008142 0.53871 -0.074505 0.31395 0.35129 -0.39005 -0.5474 -0.11395 0.076668 -0.61069 0.15894 -0.33043 0.75967 -0.52289 -0.38957 -0.71164 0.28724 -0.35683 0.050529 1.3392 -0.16064 -2.9236 0.17524 -0.42109 1.5235 0.85181 -0.47563 1.1225 -0.48463 0.36458 0.97809 -0.2227 0.88791 0.068738 0.22557 0.026459 -0.032799 -0.40426 0.14579 -0.4535 0.47756 0.15933 0.24236 0.0091798 -1.3502 -0.078424 0.57803 -0.50061 -0.19374 0.29587 -1.2316 -0.099825 0.31661 -0.46539 -0.54243 -0.13506 -0.12897 -0.29543 -0.28094 0.092313 -0.78078 -0.097087 -0.57313 -0.54 0.58031 0.46698\n",
      "\n",
      "all -0.21823 0.69199 0.70441 -0.59642 -0.21818 0.55387 -0.32052 0.52602 -0.31667 -0.19129 0.76109 0.047439 0.43199 0.12232 0.25664 -0.52453 0.048994 0.81621 -0.53336 0.53093 0.24589 -0.046352 0.38898 -0.41434 0.28169 -0.35422 0.24713 -0.44007 0.023343 -0.38592 0.31762 0.26774 -0.19487 0.024135 -0.056042 0.33799 0.12103 0.32306 -0.67209 -0.028449 -0.79051 -0.29798 0.25696 -0.1822 -0.066176 0.28468 0.019382 -0.51672 -0.065801 -0.74178 -0.043 0.10303 -0.22385 0.96676 -0.38914 -2.1671 0.25583 0.067169 2.0256 0.86387 -0.14699 1.0254 -0.42629 0.19325 0.83025 0.097585 0.79303 0.4349 0.26404 -0.17101 -0.13859 -0.55096 0.020747 -0.39791 0.43081 0.37966 -0.52257 -0.20961 -1.1568 -0.38041 0.81093 -0.050365 -0.27241 0.87153 -1.8965 0.19574 -0.2269 -0.28267 -0.14656 -0.18737 -0.61509 0.009347 -0.3978 0.037638 -1.1974 -0.26052 -0.72752 -0.3797 0.68278 0.63878\n",
      "\n",
      "-- -0.80278 0.825 1.3844 -0.96157 0.22544 -0.43478 0.25295 -0.52365 0.29711 -0.050618 0.16381 -0.45443 0.36259 0.22304 0.37628 -0.53194 -0.029086 0.4642 -0.52863 0.19352 0.86622 0.91781 0.38053 0.78411 0.39842 0.17488 -0.96607 -0.90131 -0.15274 -0.59947 0.70187 -0.18048 -0.39477 -0.23888 0.031858 -0.09333 0.64095 0.79028 -0.74182 -0.28715 0.069476 -0.37105 0.25213 -0.31456 -0.36808 -0.0047086 -0.45588 -0.69985 0.55765 -0.54397 -0.47079 -0.55933 0.27796 1.6203 -1.2591 -2.3143 0.066685 0.091261 1.5506 0.034903 -0.57338 0.82028 0.74557 -1.032 0.24555 0.089944 0.3529 0.25769 -0.168 0.087236 -0.038892 -0.20632 -0.8773 -0.32298 -0.30814 -0.07482 0.50286 -0.52047 -0.81231 -0.30948 1.5353 -0.48891 0.2946 -0.74294 -1.964 -0.081309 -0.074727 0.46389 0.17135 -0.0092708 0.18784 -0.42196 -0.074442 0.037707 -0.73605 0.47394 0.31608 -1.5286 0.84824 -0.15322\n",
      "\n",
      "out -0.18572 0.30092 0.36868 -0.92801 0.12335 0.60301 -0.26166 0.077357 -0.26929 0.12 0.39694 0.5988 0.26234 0.26443 0.50642 -0.1063 -0.014123 0.54449 -0.43429 0.0025775 0.85151 0.65793 0.83554 0.070638 0.30912 0.02096 -0.27513 -0.21501 -0.045399 -0.41673 -0.073663 0.29725 -0.0045201 0.13701 -0.32732 0.54573 -0.35412 -0.025978 -0.40114 0.12341 -0.77087 -0.058442 -0.006908 -0.65201 -0.22883 0.0005548 -0.46156 -0.13735 -0.10912 -0.47232 -0.28364 0.10187 -0.19427 1.3658 -0.59353 -2.9771 -0.012324 0.15727 1.9956 0.89 0.1126 1.0571 -0.33717 0.51669 0.6214 -0.039646 0.88343 0.35997 -0.20945 0.17748 0.043365 -0.51312 -0.21662 -0.9102 0.20956 0.39919 -0.54655 -0.14502 -0.81999 -0.020985 0.94665 0.21199 -0.72997 0.32408 -1.3573 -0.4944 0.43304 0.011635 -0.63794 -0.32431 -0.32448 -0.24412 -0.25455 -0.14941 -0.40866 -0.049368 -0.12796 0.43697 0.44212 0.27969\n",
      "\n",
      "she 0.31436 0.15312 0.18259 -0.09589 0.13195 0.44282 -0.14014 0.84879 0.57054 0.20212 0.35752 0.27902 0.24242 0.59286 0.070911 -0.22358 0.29047 0.25837 -0.52388 0.2074 -0.19031 0.078523 0.3739 0.13412 0.60067 0.70332 -0.5381 -1.5178 0.49623 -0.31389 -0.40691 1.1136 0.47278 0.32026 -0.077384 0.29227 -0.28672 0.13304 0.10368 -0.26592 -1.0749 -0.19741 0.11756 -0.55355 -0.26968 -0.069382 -0.0873 -0.61086 0.79169 -0.45729 -0.21037 -0.44649 1.0217 1.4455 -0.15592 -2.9029 -0.14453 -0.22668 0.85715 1.1399 0.18196 1.0707 -0.35308 0.14415 0.63916 -0.35275 0.72692 0.30609 0.26229 0.43192 0.13709 -0.12224 0.11473 0.21213 0.22665 0.851 -0.10737 -0.49545 -0.88584 -0.54387 0.17201 -0.025369 -0.080527 0.33842 -1.9701 -1.4076 -0.074443 -0.21233 -0.65906 -0.60364 0.012477 -0.7423 0.95703 0.41057 -0.53352 0.65747 -0.56492 -0.042725 0.060967 0.74345\n",
      "\n",
      "other -0.24211 0.50818 0.17431 -0.53048 0.3501 0.073978 -0.22706 -0.030032 0.062771 -0.18962 0.14141 -0.52596 0.22941 0.40335 0.088935 -0.23171 0.20858 -0.035073 -0.39422 0.42781 0.23424 -0.41532 0.46025 0.27109 0.069915 -0.43305 0.09186 -0.61606 -0.23359 0.26862 0.22462 0.23171 -0.64934 0.069331 0.24512 0.59737 0.19781 0.12867 -0.32702 -0.060648 -0.87321 -0.40892 0.058644 -0.36917 -0.16837 0.3064 -0.03892 -0.12069 -0.39722 -0.36063 -0.10863 0.0035632 0.025545 0.9086 -0.11305 -1.7753 0.04064 -0.39332 2.0438 1.0871 -0.53628 1.3211 0.40919 0.57779 0.82953 -0.05687 0.59288 0.40324 0.82516 -0.32087 -0.68111 -0.63115 -0.10051 -0.44106 0.25615 0.031669 -0.079954 -0.27123 -1.1283 0.13381 1.128 0.075743 -0.1783 0.34094 -2.2092 -0.054769 -0.32665 0.1464 0.066519 -0.39811 -0.32919 -0.32736 -0.21051 -0.055697 -1.2548 0.020434 -1.043 -0.44059 1.0769 0.32406\n",
      "\n",
      "people 0.29019 0.80497 0.31187 -0.32706 -0.47237 1.1363 -0.37966 0.11569 0.41912 0.016284 -0.06543 -0.004047 0.77688 -0.24936 0.18817 -0.64008 -0.02027 -0.18528 -1.0651 0.69115 1.0472 0.36171 0.80863 -0.42767 -0.18299 -0.27847 0.43839 -0.023336 0.37359 -0.3588 0.47261 0.73946 0.015123 0.059862 -0.066841 -0.38842 0.22361 0.8057 0.33789 -0.052222 -1.1057 -0.1113 0.15101 -0.66649 -0.044992 -0.068159 0.098957 -0.55942 -0.78109 -0.50947 -0.37327 -0.75674 0.22583 1.2963 -0.48181 -1.818 0.38529 -0.21849 2.2418 0.20094 -0.35208 1.4893 -0.21701 -0.24615 0.55185 0.97224 0.58526 -0.029754 0.57944 -0.64798 -0.12371 -0.53459 -0.20528 -0.53599 0.30895 0.53935 0.11853 0.42749 -1.0872 -0.23409 0.73882 0.15749 -0.56642 0.16235 -1.9117 -0.019262 -0.50293 -0.17584 -0.53881 -0.39397 0.8133 -0.31654 0.25971 -0.48202 -1.4452 0.27419 -0.72652 -0.33603 0.45998 -0.11278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n't 0.15731 0.3953 0.63586 -1.0975 -0.95768 -0.013841 -0.19853 0.25418 0.36731 -0.17486 0.27685 0.31943 0.30078 0.068531 -0.15917 -0.21944 0.064097 0.84745 -0.61989 0.54173 0.27921 0.50383 0.02146 -0.20571 0.077994 0.32229 -0.49183 -1.1411 0.23333 -0.54358 0.092285 0.8686 0.069127 0.19229 0.28374 0.46014 -0.2832 0.45384 0.35209 -0.49173 -0.14771 -0.071767 -0.24355 -0.63089 -0.67797 -0.13164 0.35974 -0.75292 0.038204 -1.7695 0.18893 -0.18872 -0.20268 0.8309 0.07787 -2.6213 0.081941 0.27262 1.6216 0.86166 -0.21582 1.0098 -0.78122 -0.11663 1.0629 0.1583 1.1009 0.70324 -0.60481 -0.45907 0.079862 -0.61794 -0.093896 -0.50363 -0.12217 -0.0017857 -0.032235 -0.1059 -0.69232 0.076485 0.60384 -0.56075 -0.96372 -0.070192 -2.0788 -0.56423 0.17574 -0.024961 -0.45349 -0.39287 -0.080573 -0.37634 0.035083 -0.39662 -0.76165 0.15113 -0.13033 -0.28513 0.19853 0.67464\n",
      "\n",
      "her 0.3339 -0.52136 0.26848 0.17416 0.15808 0.95567 -0.39404 0.75332 -0.12433 0.64539 -0.12848 0.61024 0.14794 0.56136 -0.11478 -0.23334 0.24459 -0.030026 -0.10939 0.41958 -0.1799 -0.46465 0.46621 0.886 0.83695 0.76019 -0.66037 -1.6917 0.75998 -0.044161 0.078069 0.6895 0.65059 0.2584 -0.63663 0.33152 -0.22198 0.12196 -0.11606 0.23557 -0.51854 -0.1345 0.46457 -0.36864 -0.10744 0.14728 -0.36792 -0.13907 0.77072 -0.47685 -0.031725 0.060066 0.90913 1.6532 0.04431 -3.1636 0.3388 -0.43464 1.1445 0.26247 0.45218 1.5068 0.21069 0.48721 0.49486 -0.070552 1.0531 0.51535 0.19231 0.20466 -0.41545 0.63548 0.077074 -0.42075 0.36767 0.93886 -0.59122 -0.70948 -0.87497 -0.39823 0.2408 0.69948 0.39934 0.29582 -1.803 -1.2961 0.072491 0.27994 -0.79536 -0.62887 0.078952 0.013955 1.1702 0.45136 -0.4172 -0.3487 -0.803 -0.31398 0.31908 0.62061\n",
      "\n",
      "percent 0.48716 0.95136 0.52659 0.09659 -0.034293 -0.96531 0.43908 0.32257 0.025145 0.45707 0.24081 0.36001 -0.88953 -0.12714 0.70388 -0.89373 -0.44567 -0.48162 0.20511 1.0481 1.1133 0.37892 -0.14661 1.0495 0.015263 -1.2702 0.73759 -0.42952 -0.17385 -0.61884 0.80026 0.21586 -0.022794 -0.18106 0.077069 1.0271 1.2787 0.33011 -0.64551 0.26239 -0.28411 -0.98898 -0.053325 -0.0060878 -0.58613 0.091222 -0.080187 -0.61195 -0.87497 -2.0574 -0.20668 -0.78855 0.15424 0.91653 -0.5606 -2.0277 -0.3008 -0.56281 2.7298 0.42778 0.32028 -0.94716 -0.52644 0.087261 -0.58775 1.2147 0.22267 -0.28632 1.2393 0.25182 -0.28052 0.57519 -0.59026 0.50946 -0.67621 -0.042694 -0.68245 0.019549 -1.1441 -0.40967 0.45073 0.25435 -0.94843 0.48257 -1.1498 -1.1396 0.75196 0.17418 0.26517 0.23356 0.37159 0.086542 -0.00081085 -0.22241 -2.1276 0.44577 -0.32632 -0.21151 0.72707 -0.83996\n",
      "\n",
      "than 0.10305 1.2472 0.56724 -0.19117 -0.33626 0.43446 0.13137 0.10865 -0.30365 0.27153 0.28143 -0.070029 0.13069 -0.55749 0.65366 -0.73591 -0.28843 0.025475 0.045126 0.81985 1.1737 -0.023528 0.045885 0.081232 0.23694 -0.43013 -0.15377 -0.63078 -0.010964 -0.4923 0.40203 0.21917 0.060061 -0.4783 0.22343 0.61249 -0.1794 0.42189 -0.47547 0.23102 -0.42927 -0.67041 0.13216 -0.10817 -0.23242 -0.13758 0.60007 -1.0089 -0.23983 -0.79421 0.209 -0.6083 -0.05907 1.5629 -0.24259 -2.4183 -0.09008 -0.39131 2.014 0.62688 0.20731 1.137 -0.2814 0.067306 0.40942 -0.080047 0.66897 0.59008 0.80932 -0.32619 -0.10338 -0.44707 -0.20956 0.2248 0.036496 -0.12258 0.021003 -0.099487 -0.88655 -0.097359 0.89843 0.12676 -0.70422 0.29163 -1.4439 0.35709 0.48294 -0.044056 -0.021766 -0.47151 0.50925 0.11205 -0.17628 -0.50906 -0.9412 -0.05368 -0.57045 -0.25835 0.81651 0.089829\n",
      "\n",
      "over -0.29574 0.35345 0.63326 0.19576 -0.030256 0.54244 -0.21091 0.32894 -0.48888 0.18379 0.24242 0.40346 0.11973 0.013143 0.24154 -0.40184 0.22176 -0.27837 -0.4693 -0.054899 0.65148 0.15958 0.59556 0.33167 0.72649 -0.43182 0.17208 -0.011584 -0.26389 -0.22073 -0.28538 0.35863 0.24592 0.22143 -0.76221 0.39352 -0.023915 0.43028 -0.47099 0.25162 -0.59507 -1.0495 0.17973 -0.31621 0.23788 -0.08856 0.34751 -0.5595 0.12997 -0.70101 0.2885 0.18111 -0.23004 2.0682 -0.14925 -2.87 -0.0046722 -0.22819 1.6623 0.65951 0.21892 0.636 0.10332 0.0013176 0.44414 0.20222 0.5249 0.64131 0.27416 0.10695 -0.1203 0.047109 -0.53503 -0.46869 -0.07605 0.0010654 -0.38456 -0.024067 -0.75877 0.52622 1.3285 -0.39051 -0.12174 0.51886 -1.0374 -0.33789 0.074933 0.20036 0.024703 -0.2909 -0.32043 0.020445 -0.99185 0.016802 -0.60819 -0.26601 -0.19549 0.23127 0.94771 -0.09556\n",
      "\n",
      "into -0.17864 0.14917 0.51219 -0.28412 0.62241 0.77864 -0.0061656 0.077297 0.24987 0.090217 -0.10688 0.38486 -0.091904 0.63615 0.24128 -0.22715 0.33536 0.50803 -0.27839 -0.43778 0.57114 0.59485 0.81121 0.19439 0.25663 -0.017158 -0.26403 -0.36531 0.072311 0.13406 -0.15923 0.46475 -0.49586 -0.43665 -0.078724 0.11747 -0.072248 0.17292 -0.24977 0.0006661 -0.66299 -0.79178 -0.49081 -0.20195 0.041452 0.52386 -0.34706 0.55373 0.3661 -0.32576 -0.43202 0.31898 0.21298 1.3474 -0.28075 -3.0516 -0.16904 -0.48603 2.5154 0.70632 0.089212 1.4163 0.07222 0.54876 0.83681 -0.031956 0.34991 -0.089393 -0.25961 0.0021854 0.19414 -0.56113 -0.027709 -0.86059 -0.14497 -0.12171 -0.39096 -0.25363 -0.66044 0.12262 0.78569 0.72031 -0.7325 0.43228 -0.93254 -0.088381 -0.10426 -0.34027 -0.28873 -0.4137 -0.0034137 0.20758 -0.93073 0.31037 -0.14008 -0.44965 0.21297 0.31481 0.52449 0.15522\n",
      "\n",
      "last 0.23745 0.1241 0.6252 -0.46639 0.12238 -0.03638 0.63611 0.59247 -0.51344 0.06342 0.13814 0.15193 0.094313 -0.20548 0.023568 0.091028 -0.089488 -0.29624 -0.52466 -0.028628 0.87168 -0.16307 -0.06663 1.022 0.44789 -0.48008 -0.17092 -0.30576 -0.061999 -0.23902 0.25188 -0.018878 0.084057 -0.045508 -0.38034 0.45911 -0.4122 0.39722 -0.33249 0.031747 -0.54668 -0.093553 0.67786 0.006941 0.20718 0.075996 0.49682 -0.92707 0.3506 -1.0986 -0.074723 -0.62458 0.31857 1.2155 -0.47096 -2.9422 -0.76529 -0.11556 1.9006 1.0152 -0.49471 0.61459 -0.25465 -0.0065357 0.17702 0.14239 0.18148 0.69493 -0.028739 -0.083564 0.009709 -0.18764 -0.72949 -0.26976 -0.47262 0.23738 -0.38301 -0.017983 -1.4596 0.34242 0.83223 -0.1178 -0.28637 0.35349 -1.2153 -0.37696 0.36761 0.16653 -0.22993 -0.25607 0.19118 0.055805 -0.40315 0.43574 -0.82909 0.3665 -0.14267 0.30254 0.42477 -0.2106\n",
      "\n",
      "some -0.14452 0.56023 0.2054 -0.26664 -0.25669 0.52548 -0.45982 0.21998 -0.013937 -0.24225 0.18083 0.071322 0.3837 -0.36641 0.51304 -0.37246 -0.19897 -0.055156 0.020737 0.70062 0.81707 -0.099955 0.16988 -0.25001 -0.269 -0.69777 -0.035842 -0.50822 -0.0059415 -0.32951 0.18912 0.084527 -0.43161 -0.028211 -0.19869 0.47913 0.064726 0.29788 -0.27285 0.12602 -0.96007 -0.45345 0.18871 -0.41984 0.046052 -0.24216 0.4794 -0.22272 -0.055945 -0.47897 0.090283 -0.059132 -0.24845 1.0303 -0.43041 -2.409 0.17499 -0.61541 1.8243 0.7712 0.24063 1.6522 -0.039428 0.51181 0.72804 -0.30182 0.81372 0.32864 0.6735 -0.39717 -0.45192 -0.038743 -0.11955 -0.21259 0.20072 0.29862 -0.030866 0.1832 -1.302 0.1952 1.1924 -0.38824 -0.33065 0.0014674 -1.7257 0.10662 0.31828 0.010223 -0.43216 -0.1543 0.22103 -0.14726 -0.49401 -0.41887 -1.0336 -0.34403 -0.77463 -0.36395 0.91609 0.82095\n",
      "\n",
      "government -0.219 -0.15314 0.028578 0.19314 0.017295 0.2656 -1.2929 0.036583 0.3522 0.52552 -0.013523 0.47173 0.31009 0.29828 -0.23724 0.047662 0.26025 -0.44439 -0.2429 -0.079745 0.39269 -0.16625 0.67831 -0.3608 -0.90875 -0.59866 -0.098991 -0.4032 -0.25513 -0.32609 0.58892 0.85139 -0.31868 -0.036752 -0.37764 0.47531 0.15249 0.035057 -0.67195 -0.24823 -1.0172 0.085827 0.5829 0.11308 0.29078 -0.49196 0.080374 -0.44738 -0.22613 -0.58053 -0.023165 -0.52726 0.65555 1.5924 -0.34971 -2.0056 -0.063539 -0.14119 2.587 0.6543 0.15131 -0.69478 -0.54288 0.23742 0.90566 0.46563 -0.33104 0.62263 0.56909 0.12134 0.43332 -0.66784 -0.69915 -0.97821 0.010897 0.093153 -0.53922 0.92222 -1.8925 0.3983 1.1314 0.15663 -0.34177 -0.099782 -1.0973 -0.026727 0.28341 0.6354 -0.17618 -1.0249 0.70725 -0.74798 -1.042 0.17089 -0.97143 0.20341 0.2177 -0.40402 0.45702 -0.11144\n",
      "\n",
      "time -0.024221 -0.034855 0.3571 -0.02155 -0.54804 0.31822 0.012929 0.35174 -0.4169 -0.29411 0.63723 -0.14859 -0.015264 -0.35811 0.12395 -0.67468 0.28165 -0.006541 -0.41743 0.0056387 0.26789 0.18282 0.077023 0.23808 0.505 -0.24993 -0.029143 -0.50666 0.42215 -0.097988 -0.45997 0.37969 -0.12402 -0.19568 -0.12479 0.24071 -0.51656 0.44866 -0.040484 -0.41001 -0.55576 -0.26892 0.19317 0.089754 -0.27962 -0.1567 0.39276 -0.97373 0.44148 -0.97508 0.0028868 -0.29774 0.33367 1.5798 -0.047833 -2.9699 -0.41627 0.034471 1.6993 0.8347 -0.28926 0.85307 -0.23608 0.12667 0.99202 -0.18601 0.28053 0.085047 0.27694 -0.030098 -0.30514 -0.29668 -0.027471 -0.2722 0.13718 -0.12591 -0.43509 -0.70461 -0.89953 -0.15115 0.50439 -0.23954 -0.5216 0.12224 -1.467 0.21616 -0.071825 -0.022067 -0.15128 -0.55081 -0.28592 0.32269 0.19877 0.14663 -0.81213 -0.30295 0.19865 -0.087568 0.25961 0.050783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$ 0.97469 1.2276 0.45377 0.20713 0.54067 0.19638 0.65192 0.96774 -0.61037 0.1568 0.95285 0.20265 -0.68878 -0.51994 0.69407 -1.042 0.24674 0.096724 0.2747 1.1922 0.65161 -0.24445 -0.036481 0.47561 0.15128 -0.056215 -0.19301 -0.77174 -0.37396 -1.1527 0.56227 0.56411 0.11622 -0.20223 -0.20121 1.2015 -0.14958 0.73682 0.45533 0.29635 0.8632 -1.063 -0.58136 -0.67818 -0.39116 0.12867 -0.64317 -0.99771 0.14453 -1.2594 0.27609 -0.38135 0.58074 0.80809 -1.0686 -2.6764 0.25807 -0.7799 2.1359 0.40925 0.40489 0.020735 -0.41791 0.076502 -0.63365 -1.4099 0.49511 0.96748 0.95423 -0.08271 0.11921 0.26844 -0.97633 0.79273 -0.49569 -0.12125 -0.075251 0.61632 -1.9643 0.62209 0.68695 0.40478 0.60071 -0.659 -0.29618 0.64787 0.74807 -0.015422 0.11191 0.039671 0.0087593 0.54317 0.28244 -1.6857 -0.88419 -0.095231 0.70215 0.22091 1.7439 -0.89909\n",
      "\n",
      "you -0.49886 0.76602 0.89751 -0.78547 -0.6855 0.62609 -0.39655 0.34913 0.33334 -0.45233 0.61223 0.075948 0.22531 0.16365 0.28095 -0.24758 0.0099009 0.71108 -0.75859 0.87423 0.0031041 0.35796 -0.35233 -0.665 0.38447 0.62677 -0.51543 -0.96653 0.61517 -0.75455 -0.012359 1.1188 0.35719 0.0071769 0.20255 0.5011 -0.44046 0.10661 0.79391 -0.80948 -0.015601 -0.22888 -0.34198 -1.0065 -0.8763 0.15165 -0.085339 -0.6465 -0.16733 -1.4499 -0.0065905 0.0048113 -0.012445 1.0474 -0.19381 -2.5991 0.40528 0.43803 1.9332 0.45814 -0.048819 1.4308 -0.78639 -0.20792 1.09 0.24816 1.1487 0.51481 -0.21832 -0.4572 0.13888 -0.26369 0.13647 -0.60539 0.099586 0.23344 0.13647 -0.1846 -0.047734 -0.18392 0.52719 -0.2885 -1.0742 -0.0467 -1.8302 -0.21197 0.0298 -0.30964 -0.43386 -0.36463 -0.32738 -0.0093427 0.47205 -0.51691 -0.59176 -0.32343 0.20052 -0.41179 0.40539 0.78504\n",
      "\n",
      "years 0.42215 0.6307 -0.1291 -0.13564 0.027991 0.040876 0.40969 0.30895 -0.34683 0.18925 0.33076 0.12862 0.15791 0.28442 0.17781 -0.67706 0.32368 -0.10471 -0.5876 0.2144 0.1563 0.58578 0.18894 0.44344 0.49721 -0.41477 -0.053675 -1.0574 0.17635 -0.15017 0.30718 0.19948 -0.15068 -0.24098 -0.61483 0.57655 -0.17564 0.088882 -0.44189 -0.30435 -0.80766 -0.12871 0.46206 -0.097681 -0.014938 -0.076952 0.75957 -1.0992 0.28323 -0.61529 -0.26492 -1.1732 0.16747 1.5833 0.1703 -2.5473 -0.30474 -0.61114 1.5383 0.77721 -0.3246 1.0506 -0.28259 -0.0034328 0.63225 0.19016 0.55973 -0.13783 0.46362 0.20712 0.35268 0.081224 -0.51583 0.08999 -0.55038 -0.33437 -0.21391 -0.2848 -1.3404 -0.075459 0.9061 0.11474 -0.011783 0.67302 -1.3786 0.077233 -0.10002 -0.69415 -0.50691 -0.4123 0.00076566 0.011298 -0.20379 0.19438 -0.97726 0.11681 -0.59319 0.10446 0.18539 -0.47671\n",
      "\n",
      "if -0.26269 0.6424 0.84803 -0.46166 -0.78775 0.12392 -0.19699 0.33939 0.50745 -0.16794 0.2822 0.50221 0.50103 -0.056929 0.055436 -0.18807 -0.24799 0.5064 -0.38288 0.45644 -0.068668 -0.40771 -0.18506 -0.21759 0.37901 0.37379 -0.25466 -1.0707 0.051377 -0.52884 0.22684 1.0321 -0.09049 -0.14477 0.14422 0.38818 -0.028345 0.11011 -0.13661 -0.71739 -0.6748 -0.37524 0.41628 -0.61854 -0.28223 -0.080765 0.2091 -0.60179 -0.4395 -0.99147 0.37749 -0.15041 0.3967 1.0448 -0.18732 -2.5452 0.22148 -0.084821 2.1498 0.7196 -0.21856 0.58986 -0.37576 0.010633 1.3017 0.34235 0.82135 0.69752 -0.25845 -0.14904 0.025952 -0.78131 -0.041111 -0.46884 0.6883 -0.26664 -0.14099 -0.56311 -0.74742 0.052375 0.84862 -0.49856 -1.1366 -0.049778 -1.5672 -0.26742 0.30891 0.25077 -0.16476 -0.75252 -0.46173 -0.37488 -0.26175 0.045545 -0.51906 0.055513 0.35398 0.2759 0.22535 0.23229\n",
      "\n",
      "no -0.013767 0.33247 0.59895 -0.51561 -0.56201 0.12329 -0.13417 0.26011 0.65116 0.024565 0.3199 0.49118 0.30762 0.53333 0.68015 -0.25419 -0.068138 0.2639 -0.35963 0.47576 0.36392 0.23684 -0.24312 -0.52684 0.15305 0.032089 -0.11053 -0.71643 -0.026425 -0.41872 0.18218 0.084099 -0.1888 0.22899 0.30495 0.45337 0.27868 0.054886 -0.046348 0.14313 -0.48341 0.27654 0.53848 -0.66876 0.13568 -0.45659 0.20602 -0.67056 -0.65926 -1.0911 0.24557 -0.14213 0.086415 0.85842 -0.016081 -2.7815 0.40608 -0.094489 1.7776 0.85032 -0.34224 0.39772 -0.83966 0.13606 1.1896 -0.17331 0.71848 0.042783 -0.24022 0.07143 -0.41633 -0.39236 0.2579 -0.64804 0.64845 0.47344 0.036004 -0.39083 -1.3795 -0.084084 0.81972 -0.70089 -0.22854 0.23514 -1.5969 -0.19316 0.4004 -0.050473 0.081279 -0.32571 0.1183 -0.51485 -0.062061 0.041302 -0.49174 -0.057086 -0.60485 -0.67428 0.51495 0.34287\n",
      "\n",
      "world 0.49177 1.1164 1.1424 0.14381 -0.10696 -0.46727 -0.44374 -0.0088024 -0.50406 -0.20549 0.5091 -0.60904 0.2098 -0.44836 -0.70383 0.21516 0.66189 0.3462 -0.89294 -0.48032 0.43069 0.35697 0.84277 0.52344 0.82065 0.00053183 0.24835 -0.20887 0.81657 0.25048 -0.74761 -0.011309 -0.47481 0.06452 0.54517 0.20714 -0.46237 1.0724 -1.0526 -0.15567 -0.79339 -0.028366 0.10138 -0.20909 0.45513 0.4733 0.68859 -0.2384 -0.055178 -0.83022 -0.47127 0.22713 0.042651 1.1273 -0.084776 -3.0378 -0.18389 0.78244 1.6395 0.76146 -0.14258 0.65115 -0.013549 -0.51465 0.66951 -0.34464 -0.14525 0.49258 0.80085 -0.54971 0.39657 -0.48571 -0.43846 0.3318 0.10356 -0.028987 0.10896 -0.45671 -1.115 -0.082366 1.0186 0.030639 -0.37162 1.0742 -1.0642 -0.20298 -0.98434 -0.3204 0.15969 -0.1791 0.21325 0.47155 0.68247 0.13784 -0.10704 -0.18294 -0.40082 -0.50885 0.62556 0.43917\n",
      "\n",
      "can -0.71766 0.80871 0.31868 -0.53589 -0.31998 0.18929 -0.23241 0.44233 0.25649 -0.32093 -0.16951 0.14993 0.55681 0.33992 0.028335 -0.38295 0.43366 0.73026 -0.79863 0.2954 0.017078 -0.39545 0.19967 0.081727 0.18943 0.095973 -0.033138 -0.69888 0.41562 -0.67422 -0.059873 1.23 -0.31391 0.15313 0.82508 0.49345 -0.051686 0.31035 0.25147 -0.36228 -0.14969 -0.39609 -0.73853 -1.0716 -0.21775 0.23018 0.078188 -0.48584 -0.082414 -1.0405 0.44389 -0.2964 0.036505 1.2115 0.50287 -2.2662 -0.073575 0.010925 1.7266 0.63332 -0.23215 1.2603 -0.43585 -0.082693 1.1903 0.34009 0.59365 0.20098 0.1016 -1.1757 -0.1187 -0.59569 0.34427 -0.48687 0.34953 -0.44521 -0.13131 -0.19316 -0.55304 0.075953 0.6893 -0.26706 -1.1425 -0.4221 -1.9038 0.028733 0.79527 -0.05133 -0.028824 -0.35704 -0.72769 0.19074 -0.026097 -0.48544 -0.225 0.025487 0.27732 -1.0023 0.8588 0.28583\n",
      "\n",
      "three -0.068609 0.37234 0.008263 -0.29759 0.50497 0.3553 0.39532 0.80128 -0.92972 0.037341 0.70599 0.10559 0.29726 0.76445 0.21428 -0.11708 -0.16907 -0.0045694 -0.62429 0.34949 0.91536 -0.20154 0.13332 0.54897 0.70493 -0.96237 -0.17229 -0.45547 -0.30551 0.30407 0.14562 -0.0461 -0.19556 -0.28577 0.37451 0.26128 -0.12611 0.2581 -0.40324 0.50019 -0.48305 -0.14809 0.53787 -0.1196 0.37295 0.35746 -0.077718 -0.76393 -0.17623 -0.0016575 -0.72068 -0.53102 -0.064924 1.3585 -0.078425 -2.551 0.12289 -0.076388 1.8927 1.558 -0.71005 1.0358 0.25433 0.42906 0.34181 0.24824 0.33564 0.59503 -0.0028889 0.12437 -0.47407 0.036503 -0.049999 -0.31483 0.16067 0.45478 -0.41619 -0.35707 -1.0044 -0.15895 1.1112 0.31304 -0.38909 0.42518 -1.3821 -0.028304 0.1542 -0.17196 0.16381 0.046067 -0.2083 0.243 -0.65808 -0.10197 -1.3277 0.14797 -0.66992 0.062075 0.46644 -0.17181\n",
      "\n",
      "do -0.17047 0.4037 0.21069 -0.89495 -1.0974 0.26637 -0.66438 0.43423 0.43604 -0.096505 0.17299 -0.043644 0.25197 0.37532 -0.3742 -0.31528 0.15621 1.0254 -0.69791 0.85064 0.29749 0.2582 0.037544 -0.29904 -0.48232 0.24501 -0.42231 -1.213 0.65729 -0.66008 -0.20341 0.49007 0.21975 -0.015368 0.2685 0.75017 0.066525 0.22781 0.36977 -0.43185 -0.17637 0.19282 -0.5611 -0.83103 -0.93903 -0.037231 0.20521 -0.67886 -0.75606 -1.4499 0.024081 0.13546 -0.33894 0.9505 0.023876 -2.3589 0.25861 0.027507 1.7216 0.45003 -0.38897 1.1423 -0.7378 -0.57465 1.2876 0.6925 0.93593 0.57175 -0.23874 -0.48673 0.090969 -0.51457 0.21292 -0.85689 -0.055736 -0.20015 -0.3957 0.061003 -0.45949 -0.057818 0.7602 -0.55281 -0.96297 -0.086508 -2.036 -0.2284 0.061782 0.1475 -0.19664 -0.50653 -0.16682 -0.29811 0.10284 -0.4235 -0.71824 0.045687 -0.21338 -0.67179 0.31645 0.50594\n",
      "\n",
      "; -0.33171 1.137 0.43713 -0.70258 0.124 0.53634 -0.019432 -0.05114 -0.28845 0.18795 0.70995 -0.78661 -0.055123 0.50955 0.62587 -0.23993 0.37385 -0.18557 -1.023 0.92972 0.22683 0.035221 0.56157 1.0049 0.58483 0.64794 -0.13557 -0.64331 0.15485 -0.36426 -0.076322 0.80252 -0.74517 -0.36448 -0.4774 0.31185 0.74625 0.23383 0.12129 -0.033855 0.55299 -1.2807 -0.62178 -0.4747 -0.18132 -0.51971 -0.36847 -0.75721 0.15833 -0.12968 0.3351 -0.15136 0.020061 1.2513 -0.45627 -2.5421 0.072208 -0.41713 0.82535 0.4359 -0.65355 0.51807 0.064443 -0.10398 0.75786 -0.45152 0.31349 0.0024841 0.57638 0.28168 -0.77904 0.044369 -0.080063 -0.47581 0.64605 -0.51456 -0.081823 0.073657 -0.88173 -0.44466 -0.19111 -0.51014 -0.25247 0.33798 -0.98933 0.28301 0.065917 -0.50169 0.26816 -0.38964 -0.10522 -0.55555 0.37979 0.40815 -1.1375 0.42237 -0.12652 -0.77933 0.56038 -0.74618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "president -0.064549 -0.13812 0.50017 0.41434 0.45832 -0.048331 -0.21651 0.34987 -0.83235 -0.62282 -0.40099 -0.31978 0.42928 -0.026035 -0.16171 -0.32513 0.77604 -0.39852 -0.67013 -0.55876 -0.39863 -0.28143 0.68614 0.29229 -0.7146 0.36995 -0.45309 -0.23193 0.76916 -0.031589 1.0603 1.5446 -0.13272 0.33701 -0.97561 0.73617 0.60841 0.558 -0.39274 0.1121 -1.0474 -0.03692 0.99908 -0.044925 -0.41186 0.19605 0.093281 -0.59623 -0.60765 -0.4742 -0.30162 -0.70128 -0.20055 0.99519 0.20088 -2.5636 -0.14627 0.77824 1.4913 0.29724 -0.12084 -0.060082 0.079881 -0.38348 0.41371 -0.41284 0.61702 1.1316 -0.047434 0.2848 0.57694 -0.45501 -0.76359 -1.1659 0.068946 -0.22498 0.18691 0.64142 -1.7505 -0.091248 0.85848 -0.52203 -0.38122 -0.43335 -0.73706 -0.51609 -0.13884 0.65128 0.66747 -2.1334 0.93429 0.24036 -0.43398 0.86494 -0.78319 -0.032875 -0.19761 -0.23146 -0.20256 0.1193\n",
      "\n",
      "only -0.14166 0.62942 0.57058 -0.040109 0.02072 0.45121 0.26174 0.34943 0.1109 -0.36037 0.35116 0.054917 0.35091 -0.16997 0.46228 -0.39724 0.19025 0.31776 -0.2807 0.54674 0.61512 -0.18724 0.19859 0.16014 0.20327 -0.57256 -0.1346 -0.66692 0.16478 -0.27554 -0.056533 0.78158 -0.0020976 -0.01123 0.2442 0.69785 -0.24226 0.3075 -0.41023 0.098434 -0.51658 0.011216 0.44111 -0.2839 0.30369 -0.24598 0.78145 -0.48708 -0.22522 -0.75786 -0.064799 -0.087682 -0.077229 1.3044 -0.17245 -2.7213 -0.35306 -0.33959 1.6545 0.84517 -0.18675 0.57979 -0.31977 0.37098 1.0342 -0.026049 0.47004 0.4676 0.035356 -0.063601 -0.30439 -0.49438 0.46235 0.015311 0.60372 0.27612 -0.4291 -0.29407 -0.96331 -0.14686 0.65568 -0.13284 -0.42279 0.10513 -1.4623 -0.071751 0.56444 -0.052735 0.25655 -0.055966 -0.27185 -0.071298 -0.12923 -0.062932 -0.77035 -0.041691 -0.32776 -0.44814 0.44719 0.2194\n",
      "\n",
      "state 0.0011835 -0.16506 1.2236 -0.35888 -0.00046666 0.35421 -0.39616 0.96808 -0.36594 0.20423 -0.92766 -0.2089 0.12617 0.39375 -0.49755 -0.23417 1.4522 0.21365 -0.76703 -0.12358 0.23549 0.44375 0.60648 0.31059 -1.0244 -0.40315 0.12837 -0.44123 -0.065786 -0.24412 0.41748 0.47772 0.23025 -0.16855 -0.84334 -0.41697 -0.54191 -0.11788 -0.29388 0.27256 -0.50508 -0.57695 -0.020231 0.17436 0.0016214 -0.33531 0.59673 -0.32311 -0.4827 -0.2522 -0.27833 -0.34784 0.98028 0.75049 0.11419 -2.4365 0.33841 -0.38158 2.0807 0.59222 -0.30639 -0.46595 0.32719 -0.11977 1.0265 0.36545 0.55236 0.9872 0.60452 0.78035 0.16037 -0.51707 0.50057 -0.070219 0.28201 0.19479 -0.25634 0.06454 -1.385 -0.10779 0.88646 -0.21093 -0.74791 -0.19361 -0.85338 0.40181 -0.20398 -0.34291 0.31476 -1.0479 0.19723 -0.80509 -1.1856 0.40432 -1.3208 0.27864 0.43764 -0.29483 0.84858 -0.5023\n",
      "\n",
      "million 1.4357 1.6767 0.63211 -0.43359 0.60978 0.0062815 0.54298 0.62905 -0.11585 0.116 0.71535 0.39289 -0.21513 -0.34233 0.059774 -1.106 0.42299 -0.27268 -0.13966 0.92101 0.93425 0.31288 -0.28117 0.9416 0.45229 -0.50224 1.3541 0.36959 -0.52658 -0.82775 0.4895 0.66905 -0.41784 -0.039168 -0.24732 0.4914 0.35699 0.54917 -0.52063 1.007 0.26187 -0.92915 0.13219 -0.21152 0.28515 -0.18898 -0.4714 -1.2304 -0.85009 -0.75386 -0.26656 -0.99438 1.1924 0.79018 -0.91775 -2.2502 -0.22508 -1.0433 2.2649 -0.17922 0.70508 0.288 -0.28394 -0.12459 -0.40745 0.41914 -0.047033 0.27216 0.91368 -0.57498 -0.017806 0.092672 -0.97937 -0.090228 -0.029517 -0.093017 -0.32174 1.1496 -1.3027 1.0699 1.1601 0.1343 -0.33999 0.55588 -1.1302 0.62342 0.28802 -0.47331 -0.090186 -0.15285 0.22417 0.27824 -0.15512 -1.4736 -1.0598 0.12333 -0.11708 0.73181 0.88584 -1.3885\n",
      "\n",
      "could 0.05869 0.40273 0.38634 -0.58888 -0.24626 -0.31042 0.01324 0.075456 0.22054 0.064226 -0.28233 0.69801 0.39535 -0.10333 -0.084886 -0.49865 0.23266 0.59157 -0.75346 0.075669 0.43555 -0.32304 0.51099 0.1227 0.010213 -0.17402 -0.53118 -0.59124 0.21893 -0.39508 0.10967 0.64653 -0.32986 0.20875 0.31209 0.3969 0.23556 0.14959 0.044618 -0.039851 -0.67002 -0.26655 -0.22886 -0.75891 0.023181 0.064913 0.6395 -0.511 0.14996 -1.3028 0.13912 -0.048588 0.011077 1.2961 0.33386 -2.5705 -0.28144 -0.20408 1.6794 1.0534 -0.24247 1.0631 -0.39964 0.15578 1.1181 0.19073 0.44355 0.59406 -0.34413 -0.89059 0.2529 -0.84498 -0.021412 -0.53437 0.16315 -0.357 -0.13122 0.057807 -0.81308 0.47248 0.91395 -0.31517 -0.80876 -0.25029 -1.9391 -0.66841 0.77363 0.21205 -0.17103 -0.44721 -0.25471 -0.21399 -0.27142 -0.45612 -0.40669 0.15552 0.41519 -0.35974 0.43718 0.10121\n",
      "\n",
      "us 0.3309 1.0111 1.5401 -0.0037205 -0.16854 -0.36448 -0.49264 -0.27382 0.042796 -0.10149 0.82923 -0.21641 0.18006 0.22698 0.014333 0.26679 -0.17742 -0.35467 -0.61956 0.81431 1.208 -0.064785 0.11816 0.19243 -0.026561 0.28291 0.18635 -0.63154 1.008 -0.33754 0.031217 0.55929 -0.34031 -0.038397 -0.086859 0.70105 -0.475 0.12132 0.38364 0.31194 -0.77032 -0.84787 0.00012489 0.23546 0.37578 -0.17466 -0.68517 -0.34409 -0.60872 -1.6763 0.1537 -0.067015 0.74586 1.1758 -0.5217 -2.4951 0.31104 -0.42603 2.1117 0.018723 0.40115 0.41757 -0.21425 -0.19927 -0.5357 0.093413 -0.042322 1.0767 0.10693 0.38199 0.34468 -0.43743 -0.77398 -0.42631 0.23625 -0.075068 0.51797 0.43557 -1.4849 0.1453 1.0455 -0.15742 -0.094204 0.5251 -1.1266 0.20811 0.22303 0.23587 0.081314 -0.80265 0.037458 0.25518 -0.36446 -0.81031 -0.49818 0.037837 0.25238 0.030629 0.59237 0.41334\n",
      "\n",
      "most -0.56127 0.66595 0.33998 -0.44736 -0.024115 0.14962 -0.026301 -0.16401 -0.15748 -0.1723 0.10616 -0.40657 0.14299 -0.32548 -0.24691 -0.29022 0.17005 -0.28188 0.12926 0.68151 0.83832 0.05572 0.26773 0.010408 0.39365 -0.78113 -0.038376 -0.45335 -0.090688 -0.036742 0.23847 0.27364 -0.19374 0.0022642 0.45898 0.56988 -0.18467 0.26012 -0.56891 0.026703 -0.50796 0.18566 0.63028 -0.15645 0.23492 0.026142 1.0702 -0.33102 0.084071 -0.49171 -0.17161 -0.34591 0.21128 0.69198 -0.57813 -2.2655 0.1295 -0.23815 1.8614 0.52522 -0.4304 1.4683 0.15087 0.1327 1.0504 -0.2488 0.85428 0.012577 0.75356 -0.31425 -0.30174 -0.45269 -0.067231 0.25082 0.31847 -0.38984 -0.44767 -0.17684 -0.94018 -0.26807 0.97909 0.07253 -0.64095 0.48025 -1.9846 -0.013741 -0.19458 -0.37874 -0.17319 -0.50619 0.2157 -0.05004 -0.29758 0.085809 -1.2525 0.23257 -1.0936 -0.38088 0.6156 0.3717\n",
      "\n",
      "_ -0.079588 1.0425 1.198 -0.62186 -0.040437 0.49017 0.27598 0.056033 -0.067537 -0.20508 0.34237 -0.33197 -0.048758 0.1558 0.24583 -0.25191 0.23346 0.053946 -0.81845 0.38367 0.21813 0.59056 0.11077 0.39728 0.89418 0.28427 -0.68023 -0.68442 -0.13217 -0.80081 0.65754 0.33097 -0.20079 -0.49424 0.30287 -0.046807 0.54981 0.37721 0.018467 -0.15068 -0.029369 -0.35362 0.15501 -0.5046 -0.50217 -0.50376 0.32958 -0.35235 -0.051894 -0.84021 0.18086 -0.26929 0.034581 1.229 -0.28533 -2.226 -0.24293 0.34611 1.6295 0.51413 -0.65765 1.1085 0.52715 -0.55442 0.36363 -0.5123 0.61631 0.11449 0.10605 0.11603 -0.16997 -0.445 -0.81342 -0.058189 -0.28556 -0.40484 0.30436 -0.24347 -0.74751 -0.056678 0.79696 -0.32531 -0.23435 -0.19253 -1.367 -0.066577 0.39506 0.17405 -0.17044 -0.26573 0.04649 -0.13884 0.016257 0.083125 -1.2149 0.28855 0.22301 -0.65562 0.79654 0.37697\n",
      "\n",
      "against -0.0092921 0.18686 0.084314 -0.26811 -0.21608 0.84506 -0.73735 -0.73384 -1.0819 0.39841 0.13304 -0.58118 -0.53203 -0.15233 0.46465 0.2805 0.020769 -0.2033 -0.989 0.38855 0.99298 -0.47496 0.5176 0.85468 0.70004 0.094016 0.13977 -0.67541 -0.23665 0.22737 0.16264 0.3318 0.33722 -0.21665 0.0076034 -0.39518 -0.087066 0.0050744 -0.54576 0.059273 -0.8163 -0.49495 0.29306 -0.64932 0.39675 -0.67127 0.79005 -0.52142 -0.43738 -1.0235 -0.07925 0.85788 0.1008 1.6252 -0.0061692 -2.253 0.090421 -0.72234 1.4996 0.51537 -0.79267 -0.26272 -0.27267 0.32687 0.21483 0.28405 0.22274 0.77563 -0.39605 0.18825 0.077636 -0.79385 -0.78221 -0.5283 -0.041909 -0.12369 -0.31177 0.12547 -1.4027 0.7901 1.4086 -0.13294 -0.74329 0.48184 -1.2293 -0.72056 -0.75255 0.79022 0.20151 -0.37587 -0.65065 -0.68236 -0.4791 1.0763 -0.032219 -0.35781 -0.53345 0.70178 0.56552 -0.43409\n",
      "\n",
      "u.s. 0.32396 0.5981 1.2378 0.21162 0.32223 -0.933 -0.36429 -0.46309 -0.19794 0.19983 0.13876 -0.58763 0.09724 0.28784 -0.19291 0.25512 0.35596 -0.42983 -0.75188 -0.10966 1.104 0.21188 0.74477 0.42071 -0.18735 0.24202 0.46133 -1.1325 0.4886 -0.029384 0.33875 0.53299 -0.25313 0.11577 -0.08838 0.62731 -0.10941 -0.16472 0.3472 0.56492 -0.74448 -0.9998 0.22918 0.85612 0.75357 -0.048493 -0.31846 -0.83283 -0.74052 -1.6537 0.38901 -0.35591 0.60411 0.81544 -0.0021813 -2.2976 0.063694 -0.97151 2.2791 0.43913 -0.11079 0.27595 1.0608 -0.012467 -0.24295 -0.28065 -0.2669 0.99697 0.093197 0.6632 0.025634 -0.65658 -0.72335 -0.16165 0.16611 0.452 0.57443 0.24713 -1.5824 0.059356 1.3486 -0.041878 -0.18273 0.51898 -0.83905 -0.3887 0.43774 0.099288 0.29579 -1.0994 -0.10151 0.18774 -0.65817 -0.23207 -0.74606 0.24291 0.12691 0.10485 0.49419 0.30868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "so -0.39551 0.5466 0.50315 -0.63682 -0.4547 0.30889 -0.04924 0.27191 0.31562 -0.32879 0.25089 0.14508 0.35136 -0.22793 -0.15894 -0.51527 -0.27978 0.3647 -0.39425 0.33299 0.43051 0.183 0.25095 -0.18547 0.34698 0.055137 -0.45979 -0.82963 -0.018523 -0.36772 0.045566 0.71052 -0.022782 -0.080889 0.20685 0.49855 -0.059794 -0.0080048 -0.23823 -0.33759 -0.24201 -0.23788 -0.0011362 -0.40395 -0.44859 -0.32189 0.48405 -0.027999 0.10148 -0.93585 -0.087522 -0.39959 0.36545 1.3726 -0.30713 -2.594 0.22431 -0.041168 1.7765 0.4001 -0.10996 1.4178 -0.26154 0.18617 0.79328 -0.11709 0.87541 0.43911 0.34711 -0.28515 0.076269 -0.63038 0.16408 -0.37053 0.58485 -0.15472 -0.26382 -0.1859 -0.75228 -0.15752 0.78539 -0.018846 -0.8013 0.15561 -1.8624 -0.16969 0.19419 -0.30683 -0.78067 -0.49689 -0.18256 -0.042016 -0.2629 0.058531 -0.44664 -0.099765 -0.4305 -0.23693 -0.014519 0.31981\n",
      "\n",
      "them -0.10131 0.10941 0.24065 -0.66767 -0.18687 0.91068 -0.49355 0.22234 -0.00068759 -0.2633 0.35566 0.25153 0.2762 0.25009 0.58282 -0.63401 0.047375 0.18918 -0.82015 0.68126 0.20599 -0.39743 0.66879 -0.19695 0.0072995 -0.45157 -0.44265 -0.66212 0.39773 -0.27854 0.24966 0.29716 -0.41428 -0.19447 0.11666 0.29235 -0.05677 0.051592 -0.32637 -0.4391 -0.77075 -0.18956 -0.20683 -0.66622 -0.091887 -0.039072 -0.27523 0.17081 -0.5802 -0.6876 -0.079209 0.2897 -0.097517 1.8687 -0.27563 -2.1903 0.35603 -0.37762 1.9264 0.21876 -0.21676 1.5126 -0.21941 0.33431 0.47717 0.022636 1.1532 0.86151 0.0051738 -0.1659 -0.30709 -0.43137 0.084986 -0.67329 0.27454 0.48618 -0.555 -0.21077 -0.72292 -0.046827 1.0762 0.17047 -0.8775 0.4648 -1.5632 -0.092291 0.19847 0.20053 -0.55668 -0.07711 -0.2355 -0.55851 0.15774 -0.61762 -0.93783 -0.75155 -0.5933 0.2568 0.61547 0.19253\n",
      "\n",
      "what -0.1518 0.38409 0.8934 -0.42421 -0.92161 0.037988 -0.32026 0.0034119 0.22101 -0.22045 0.16661 0.21956 0.25325 -0.29267 0.10171 -0.075491 -0.060406 0.28194 -0.58519 0.48271 0.017504 -0.12086 -0.1099 -0.69554 0.156 0.070558 -0.15058 -0.81811 -0.18535 -0.36863 0.03165 0.76616 0.084041 0.0026928 -0.2744 0.21815 -0.035157 0.32569 0.10032 -0.60932 -0.70316 0.18299 0.33134 -0.12416 -0.90542 -0.039157 0.44719 -0.57338 -0.40172 -0.82234 0.5574 0.15101 0.24598 1.0113 -0.46626 -2.7133 0.43273 -0.16314 1.5828 0.55081 -0.24738 1.4184 -0.016867 -0.19368 1.009 -0.059864 0.91853 0.43022 -0.20624 0.076127 0.21595 -0.26834 -0.33342 -0.37151 0.45197 -0.08246 0.32984 -0.57376 -1.3042 0.27121 0.66277 -0.079626 -0.79167 -0.0053662 -1.7916 -0.33298 -0.30698 -0.3398 -0.55618 -0.69471 0.27427 -0.21898 -0.26714 0.0022561 -0.50178 -0.32775 -0.4567 -0.27123 0.22157 0.92112\n",
      "\n",
      "him 0.042409 -0.52195 0.40389 -0.31683 0.015581 1.1447 -0.18197 -0.061639 -0.33253 -0.40223 0.034373 0.41981 0.031487 -0.080156 0.22563 -0.3658 0.18513 0.17092 -0.6947 0.64301 0.053546 -0.45991 -0.17761 -0.19595 0.4256 0.042524 -0.70944 -0.92625 1.2065 0.14414 0.14133 0.68685 0.048357 0.28694 -0.40552 0.75591 -0.79224 -0.045448 -0.35752 0.14998 -0.9691 0.15583 0.7813 -1.0131 -0.39037 0.053001 0.0077983 -0.16409 -0.055156 -0.78147 -0.1504 0.23792 0.29348 1.8954 -0.44203 -2.7568 -0.13436 -0.30857 1.3768 0.49351 0.15503 1.206 -0.51755 0.42908 0.32301 -0.25093 1.4077 0.99191 -0.34695 0.4574 -0.45169 -0.27574 -0.040537 -0.69482 0.16941 0.17086 -0.40722 -0.1175 -0.51941 0.0846 0.88102 0.30067 -0.64169 -0.28448 -1.2321 -0.3771 0.13661 0.38983 -0.58003 -0.69147 -0.12045 -0.72923 0.41652 -0.11802 -0.21631 -0.72523 -0.27257 0.21283 0.13566 -0.068467\n",
      "\n",
      "united 0.21733 0.56116 0.63062 -0.1413 0.61665 -0.4148 -0.024356 0.48407 0.032723 -0.3433 0.13589 -1.4812 0.18595 0.3452 -0.010685 -0.11982 0.96111 -0.05364 -0.92531 -0.38868 0.70143 0.31449 0.86354 0.13721 -0.046118 0.14104 0.55199 -0.55399 0.59324 0.045272 0.05338 0.82637 -0.21944 0.094414 0.1116 0.48273 0.1842 0.2454 -0.4033 -0.24943 -1.4073 -0.015077 0.35675 0.081697 0.75666 -0.3174 0.84478 -0.16486 0.0021402 -0.82365 0.38922 0.32711 -0.15313 1.0527 -0.044018 -2.6557 0.16917 -0.88672 1.5012 0.30756 0.12189 -0.19041 0.062539 -0.23757 0.30959 -0.14446 -0.52472 1.4611 0.11372 0.038019 0.15905 -0.4385 -0.36327 -0.51613 0.55316 0.095805 0.18355 -0.31618 -1.5098 0.25093 0.99394 0.070954 -0.25268 0.7002 -0.86416 -0.18965 -0.15597 0.29606 0.17101 -0.68518 -0.99421 -0.24292 -0.36938 0.063446 -0.95062 0.18201 -0.096244 0.0098267 0.39708 -0.34708\n",
      "\n",
      "during -0.27891 -0.22974 -0.47454 0.068965 -0.29366 0.16081 0.56102 0.73436 -0.73745 -0.10254 -0.090765 -0.61136 0.36932 0.38535 0.35548 -0.37326 -0.15222 -0.50306 -0.59458 -0.47526 0.40403 -0.34318 0.18504 0.57305 0.22752 -0.19725 -0.49467 -0.1684 0.874 0.01405 -0.27933 0.11743 0.12937 0.039187 -0.23168 0.4281 -0.12645 0.37137 -0.32466 -0.08876 -0.55695 -0.27712 0.35221 0.61908 0.29789 0.099367 0.77284 -0.94009 0.6737 -1.043 -0.056255 -0.06086 0.58809 1.3481 -0.0069607 -2.5155 -0.56072 -0.61131 1.7696 1.0041 -0.79759 1.3624 0.093775 -0.026866 0.69347 -0.10853 0.16738 -0.81752 0.41267 0.30658 0.032399 -0.3848 -0.10894 0.00077002 -0.60538 0.068719 -0.60942 0.33904 -1.2881 0.064366 0.85468 0.23217 0.057541 0.57282 -1.3892 0.27166 -0.37663 -0.095078 -0.12274 -0.46974 0.31321 -0.14088 -0.12172 0.99531 -0.42987 -0.19791 -0.17884 0.42433 0.14577 -0.59513\n",
      "\n",
      "before 0.36281 -0.18559 0.46119 0.072988 -0.093083 0.50692 0.42306 0.69663 -0.10936 -0.36433 0.8099 -0.19089 0.21541 0.27386 0.59262 -0.70742 0.11229 0.21436 -0.50418 -0.59406 -0.11661 -0.22116 0.25401 0.16651 0.44469 0.3437 -0.50661 -0.56653 0.37932 0.10637 -0.34629 0.12174 -0.27635 -0.14158 -0.34986 -0.23913 -0.43585 -0.18304 -0.34334 -0.3378 -0.86635 -0.50164 0.4578 -0.002006 0.02822 0.093612 0.2848 -0.87453 0.35247 -0.97725 0.22705 -0.052187 0.26154 1.6054 -0.69212 -2.6977 -0.69524 -0.19921 1.8225 0.86595 -0.51825 1.0439 -0.015672 0.16532 0.46015 0.056994 0.22914 0.36015 -0.11452 0.021205 -0.35505 -0.16756 -0.26181 -0.91036 -0.36393 0.10294 -0.45547 -0.27404 -0.83145 -0.084725 0.61138 -0.12894 -0.35169 0.16639 -1.1824 -0.38447 0.22745 -0.015933 -0.26651 -0.38409 -0.072182 -0.32193 -0.34742 0.69 -0.76737 0.0086468 0.53689 0.6996 0.35549 -0.24589\n",
      "\n",
      "may 0.082528 -0.07529 0.014696 -0.31124 -0.24006 -0.48255 0.13352 0.043161 0.19054 -0.52213 -0.12504 0.029079 0.34909 0.42935 0.66659 -0.28475 0.38381 0.33901 -0.57171 0.11767 0.20123 -0.79966 0.82828 0.5954 -0.25487 0.024701 -0.14971 -0.36822 0.55806 -0.18942 0.2369 0.73506 -0.082499 -0.58812 -0.085873 0.60374 0.2383 0.44659 -0.074284 -0.33467 -0.26498 -0.32318 -0.11407 -0.67289 0.033642 -0.34304 0.54404 -0.5819 0.23615 -1.0738 0.21622 -0.50559 0.16373 1.1277 -0.3188 -2.4838 -0.17653 -0.87256 1.5571 0.62638 -0.63166 1.333 -0.61523 0.14067 1.0511 0.40579 -0.11776 -0.077961 0.058356 -0.66891 -0.36699 -0.54975 -0.50666 -0.1224 -0.32144 -0.50845 0.31016 0.28385 -1.1605 -0.26593 0.2919 -0.51251 -0.83355 -0.11263 -1.3265 -0.716 0.5134 0.15796 0.058603 -0.1824 -0.55904 0.0039802 0.11794 0.26451 -0.24625 0.56471 0.072148 -0.92017 0.36852 -0.38653\n",
      "\n",
      "since 0.42261 0.30945 0.21854 0.031845 0.071375 -0.094332 0.41375 0.091462 0.068421 -0.0091774 0.11548 -0.29799 0.71572 0.26885 0.15715 -0.11581 -0.052275 -0.14545 -0.4396 -0.057754 0.39526 -0.0046843 0.41918 0.63758 0.54708 -0.45561 -0.22062 -0.76163 0.20543 -0.0974 0.52762 0.39617 -0.17724 0.057669 -0.37362 0.15077 -0.2588 0.51701 -0.7923 -0.57228 -0.43158 -0.0066014 0.61488 0.24314 0.30344 -0.11533 0.85285 -0.86439 0.52463 -0.93728 -0.12563 -0.23571 0.24105 1.5613 0.024711 -2.6511 -0.47651 -0.24653 1.7767 0.74858 -0.6595 0.7267 -0.15321 0.11986 0.82973 0.36084 0.055967 0.099942 0.71003 0.45763 0.11246 -0.08918 -0.82032 0.12385 -0.553 -0.39812 -0.060371 -0.022127 -1.2908 0.28121 0.73203 -0.27482 -0.40098 0.6011 -0.78143 0.29783 0.14597 -0.21302 -0.58773 -0.35827 0.054432 0.26549 -0.74025 0.8275 -0.86179 0.45864 -0.17767 0.3984 0.14492 -0.55325\n",
      "\n",
      "many -0.32914 0.82887 -0.14182 -0.27705 0.010944 0.42952 -0.56005 -0.07194 0.080524 -0.40554 0.043851 -0.31766 0.52202 -0.16149 0.043372 -0.30606 0.035574 0.10558 -0.13047 0.67779 0.45329 0.0075139 0.30743 -0.25804 0.0085955 -0.93448 0.00061153 -0.58644 0.06784 -0.019375 0.33947 0.30926 -0.39635 -0.094199 0.01055 0.52399 0.084729 0.28158 -0.33752 -0.19876 -1.1249 -0.19234 -0.012407 -0.19436 0.10601 -0.18132 0.67892 -0.14356 -0.0063351 -0.10511 -0.15675 -0.28684 -0.078341 0.82427 -0.021684 -2.1601 0.30517 -0.33368 1.7488 0.70295 -0.38371 1.611 0.43974 0.30393 0.89573 -0.093649 0.72926 0.10061 0.72969 -0.46212 -0.42695 0.020632 -0.21447 -0.19951 0.090397 0.13428 0.1011 0.13732 -1.0366 0.01218 1.309 -0.21578 -0.3893 0.22264 -2.1672 -0.048762 -0.4835 -0.35004 -0.69585 -0.25604 0.035456 -0.29506 -0.30143 -0.16867 -1.4708 -0.21042 -1.0739 -0.057574 0.62466 0.59499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "while 0.094157 0.46457 0.4535 -0.15074 0.27223 0.4545 -0.14906 0.15345 -0.061775 -0.080787 0.53914 -0.39179 0.083668 -0.10328 0.27425 -0.80995 -0.11588 -0.32288 -0.23434 0.19782 0.47749 0.027463 0.49629 0.41455 0.55198 0.13814 -0.14193 -0.65181 -0.055301 -0.026074 -0.26557 0.16076 -0.32292 -0.10203 0.08234 0.13615 0.27754 0.19405 -0.2348 -0.12201 -0.39889 -0.6782 0.42633 0.21963 -0.20309 0.16836 0.013425 -0.35281 -0.069011 -0.93563 0.16361 -0.13117 0.099808 1.8998 -0.26605 -2.4321 -0.34386 -0.46084 1.3691 0.72702 -0.18504 0.18016 0.085648 0.46807 0.12802 0.28034 0.68951 0.36221 0.66845 0.32295 -0.58005 -0.27069 0.15057 -0.46084 -0.21336 0.36952 -0.23539 0.075712 -0.71302 -0.27551 0.64845 0.10345 -0.64706 0.29101 -1.4154 -0.31586 -0.26086 0.24959 -0.20852 -0.28688 -0.075658 -0.63833 -0.0040848 0.21971 -0.91796 0.271 -0.30677 -0.23741 0.69147 -0.16581\n",
      "\n",
      "where 0.051044 0.59824 0.31195 -0.066913 -0.29111 0.46091 -0.27781 0.44026 0.044429 -0.22897 0.20414 -0.1458 0.77876 0.28536 0.32293 -0.42382 0.46866 -0.068769 -0.57332 0.02484 0.18865 -0.25324 0.63914 -0.22605 -0.094128 -0.33878 -0.027334 -0.97173 0.22157 -0.45171 -0.42427 0.11795 0.21835 0.59762 0.083504 0.03862 0.21675 -0.011594 0.15081 -0.33137 -0.34324 -0.36142 0.17621 -0.47619 0.11287 0.063469 0.23026 -0.10055 0.4327 0.14464 -0.3986 -0.26808 0.3182 1.187 -0.53799 -2.7762 -0.36056 -0.52403 1.4961 0.5852 -0.45835 0.90748 0.19071 -0.083699 1.1361 0.18972 0.66498 0.19299 0.52448 0.42907 -0.12227 -0.63509 0.34567 -0.27887 0.20995 -0.10872 0.57247 -0.14223 -0.76988 0.20549 0.697 0.031403 -0.82066 0.00055807 -0.95539 -0.24125 -0.14125 -0.39647 -0.15662 -0.1668 0.16383 -0.2677 -0.077713 0.46474 -0.82278 0.18345 -0.33744 0.058495 0.72002 0.34357\n",
      "\n",
      "states 0.13815 0.45166 0.93858 0.055307 0.70642 -0.54046 -0.23845 0.4287 0.044248 -0.22925 -0.35244 -1.2327 0.014078 0.3902 -0.022401 0.063482 0.93077 0.18331 -1.0044 -0.3925 0.75232 0.50908 1.021 0.36036 -0.32886 0.20715 0.61499 -0.62382 0.69708 0.025825 0.31019 0.57879 0.014539 -0.16507 0.23301 0.43865 0.17361 -0.30823 -0.2958 -0.0056279 -1.5188 -0.32979 0.18574 0.10105 0.21442 -0.68405 0.7238 -0.43453 -0.14812 -0.6115 0.024181 0.24927 0.20014 0.95541 -0.0049571 -2.3214 0.42327 -1.0557 1.7521 0.8185 -0.071669 0.022392 0.64796 -0.14929 0.71763 -0.00068196 -0.40566 1.2262 0.35112 0.25473 0.19905 -0.46895 -0.27762 0.26085 0.39509 0.45209 0.19065 -0.40421 -1.253 0.19878 0.8517 -0.15268 -0.63545 0.47758 -1.0593 -0.073282 -0.0039082 -0.17523 0.07036 -0.80366 -0.74478 -0.3666 -0.41988 -0.31086 -1.0702 0.51695 -0.16042 -0.36675 0.26479 -0.21382\n",
      "\n",
      "because 0.067634 0.41595 0.58451 -0.58491 -0.72451 -0.072752 -0.37836 0.0066186 0.4461 -0.15757 -0.12945 0.14262 0.6791 -0.29128 0.13926 -0.33211 -0.24274 0.27152 -0.0022924 0.41131 -0.10664 -0.20937 0.24291 0.21721 0.21121 0.08819 -0.34199 -1.0019 -0.04584 -0.19437 0.39427 0.60687 -0.069932 -0.21877 -0.022186 0.46304 0.16068 0.28462 -0.25492 -0.34909 -0.63743 -0.17272 0.32558 -0.31776 -0.11402 -0.092384 0.67199 -0.38248 -0.10641 -0.99178 0.25119 0.020442 0.14007 1.0752 0.0086798 -2.5789 -0.018486 -0.27098 2.0873 0.54331 -0.37876 0.64635 0.095271 0.1932 0.97416 -0.1454 0.77525 0.062012 0.16309 -0.12143 -0.11721 -0.5531 0.12182 -0.040885 0.2784 -0.031441 0.062561 -0.22321 -0.94733 0.35743 0.80591 -0.22318 -0.82193 0.13669 -1.8634 -0.24686 -0.13698 0.11041 -0.56013 -0.43488 -0.29646 -0.52113 -0.16884 0.24752 -0.5443 0.25473 -0.16929 0.058011 0.30037 -0.011096\n",
      "\n",
      "now -0.014495 0.59107 0.70469 -0.40429 0.18782 0.12694 -0.14758 0.34587 0.22103 -0.28488 0.35046 0.064137 0.59154 0.11654 0.039906 -0.80121 0.3754 0.29344 -0.10369 0.27907 0.19986 0.68369 0.37554 -0.0072725 0.047113 -0.25263 0.0082005 -0.69781 -0.0068522 -0.21435 -0.12159 0.65265 0.061688 -0.064054 0.22297 0.50536 -0.15272 -0.061132 0.1619 -0.5441 -0.51882 -0.27868 0.15457 -0.53929 -0.099526 0.080574 0.65259 -0.37347 0.11975 -0.69007 0.090121 -0.55768 0.056925 1.2212 -0.19242 -2.6716 -0.32075 -0.37684 1.2959 0.66362 0.088504 0.59448 -0.063146 -0.084163 0.82726 0.31584 0.57423 0.52881 0.066457 0.20286 0.56204 -0.48379 -0.45163 -0.21738 0.12476 -0.44268 0.018879 0.17531 -0.90884 -0.21879 0.68435 -0.1656 -0.70718 0.10911 -1.2717 -0.15881 -0.084324 -0.39182 -0.22881 -0.15462 0.04707 -0.38092 0.062735 -0.00020286 -0.83754 0.20597 0.079004 -0.3428 0.73813 0.59255\n",
      "\n",
      "city 0.26572 0.034857 0.49055 -0.67402 0.32883 0.99917 -1.0305 0.69492 0.34358 0.85792 -0.43804 -0.81835 0.26868 -0.25083 0.15466 -0.83224 1.1727 -0.18604 -0.52929 0.17653 0.80747 -0.14031 -0.024031 -0.0061158 -0.60217 -0.67171 0.20698 -0.50948 -0.3299 -0.39454 -0.45927 -0.28931 0.78811 0.44483 -0.48697 -0.2363 -0.015081 0.16606 -0.034172 0.084049 -0.086082 -0.9015 -0.20199 -0.56048 0.86435 0.32742 0.38184 -0.24334 0.59723 0.17464 -0.44756 -0.33477 0.32423 1.0231 -0.54946 -2.852 -0.5764 -0.3576 1.4104 0.30757 -0.38852 0.74302 -0.40562 0.091479 0.1746 -0.19251 0.16341 0.18197 -0.3522 0.29123 -0.2839 -0.73045 -0.1555 -0.38138 0.39395 -0.49008 1.0277 0.0028462 -1.4342 -0.12607 0.33338 0.48741 -0.9977 0.40783 -0.79581 0.49322 -0.48803 -0.29408 0.5714 -0.46724 1.0987 -0.60398 -0.46683 0.14795 -1.3415 0.7188 -0.28589 0.39124 0.98232 0.069209\n",
      "\n",
      "made -0.1982 -0.28405 0.14584 -0.31859 0.35192 -0.17536 0.51453 0.22287 -0.3827 -0.015509 0.33557 -0.3486 0.18489 0.50886 0.45933 -0.14549 0.42201 0.42403 -0.32605 0.058963 0.27354 0.096216 0.048534 0.21598 -0.030559 -0.12433 -0.3235 -0.15598 -0.097838 -0.50238 -0.14542 0.82071 -0.038955 -0.018687 0.19786 0.76392 0.086559 0.39519 -0.54875 0.02119 -0.67961 -0.028829 0.43082 -0.19367 0.062897 -0.30931 0.2046 -0.55095 0.045624 -0.94535 -0.11373 0.014583 0.44611 1.0166 -0.58189 -2.4982 -0.35776 -0.041392 1.3316 0.67553 -0.21153 0.73355 -0.32923 0.63869 0.46185 -0.76642 0.55702 0.64883 -0.22308 -0.56285 0.082082 -0.22448 0.16296 -0.38004 0.32773 0.34815 -0.41732 0.43754 -0.98953 0.24076 0.82662 0.094291 -0.076368 0.38624 -1.5678 -0.38339 0.20464 -0.11202 -0.34939 -0.50683 -0.07543 -0.51435 -0.51315 -0.048638 -0.6143 -0.39498 -0.80424 -0.53594 0.52015 0.064196\n",
      "\n",
      "like -0.2687 0.81708 0.69896 -0.72341 0.091566 0.19557 -0.52112 -0.24313 -0.44701 -0.27039 -0.34126 -0.46898 0.42583 0.46289 0.17106 -0.26795 0.23162 0.46568 -0.31808 0.75875 0.31857 0.64124 0.067042 -0.18517 0.49996 0.36964 -0.31172 -0.73098 -0.26902 -0.32058 0.23394 0.24276 0.1426 -0.2793 0.38823 0.42398 0.1021 0.33316 0.3015 -0.52711 -0.024475 -0.15301 -0.3224 -0.51231 -0.5525 0.29819 0.10847 0.052334 -0.2298 -0.77889 -0.08928 0.48109 0.015368 0.92544 -0.26122 -2.4759 -0.019825 0.58281 1.306 0.73512 -0.34372 1.5829 -0.10814 0.11388 0.7922 0.18347 1.2232 0.35697 0.17504 -0.16527 -0.012827 -0.47918 -0.32111 -0.40573 -0.37151 0.086323 0.25172 -0.082751 -0.25584 -0.19178 1.0474 -0.51984 -0.71463 0.38827 -1.6722 0.015986 -0.22668 -0.26602 -0.57925 -0.85651 0.20543 -0.46372 -0.065652 -0.061944 -0.57233 -0.46406 -0.41405 -0.4011 0.74657 0.31122\n",
      "\n",
      "between 0.082441 -0.04076 0.52517 0.89712 -0.013364 -0.018666 0.16234 -0.015578 -0.30225 -0.37165 -0.051907 -0.14994 0.83097 0.54099 1.3509 -0.48953 -0.027815 -0.69868 -0.72504 -0.1469 0.61718 -0.6528 0.68797 0.013025 0.65707 -0.80827 0.2009 -0.2751 0.15607 -0.27864 -0.14049 0.48614 0.57921 -0.90647 -0.30117 0.55824 0.49224 0.36134 -0.38736 0.067848 -0.23191 -0.75179 -0.07839 -0.38973 -0.27977 -0.41476 0.25365 -0.46669 -0.14841 0.24835 -0.21373 0.25886 0.074922 1.3842 -0.092394 -2.6848 -0.185 -0.85553 1.687 0.58661 -0.47027 0.93776 -0.19932 -0.087569 0.19558 0.88214 0.33803 0.3977 0.88468 0.073823 0.16302 -0.23988 0.018631 -0.28011 0.4174 -0.48329 0.56282 -0.34626 -0.98178 0.35743 0.58303 0.32318 0.028338 0.36296 -0.88524 0.48406 -0.55206 0.18507 0.016378 -0.43148 -0.53805 0.4495 -1.2263 0.48146 -0.47856 0.084133 -0.43795 0.27416 0.80937 0.19743\n",
      "\n",
      "did 0.30449 -0.19628 0.20225 -0.61687 -0.68484 -0.11887 0.081626 0.18906 0.22438 0.033433 0.24649 0.10935 0.054269 0.17402 -0.24224 -0.17824 0.15826 0.632 -0.85981 0.32896 0.3327 -0.063516 0.048307 -0.038986 -0.1584 -0.11808 -0.48404 -0.8478 0.58427 -0.46182 0.20295 0.86363 -0.10949 0.32921 -0.070915 0.59007 -0.027524 0.5797 0.0308 -0.038447 -0.5768 0.19498 -0.12224 -0.3847 -0.51111 -0.23528 0.49282 -0.794 0.08403 -1.5505 0.18491 0.26173 0.20716 0.91648 -0.0009998 -2.4516 -0.24172 -0.1973 1.2655 1.0784 -0.47938 0.95467 -0.41407 0.093636 1.0538 -0.099137 0.60201 0.69952 -0.50105 -0.049146 -0.23436 -0.49729 0.022624 -0.69352 0.0085958 0.10715 -0.30272 0.038256 -0.80478 0.043408 0.51528 -0.39395 -0.44114 -0.1367 -1.7644 -0.65928 0.34359 -0.088928 -0.29006 -0.071052 0.19439 -0.62846 -0.0088072 -0.0085936 -0.64929 -0.035085 -0.37907 -0.18385 -0.12432 0.27468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "just 0.075026 0.39325 0.90314 -0.30451 -0.32768 0.5963 0.22834 0.59028 0.13495 -0.26515 0.77353 0.22579 0.099035 -0.30459 0.66393 -0.33059 -0.23244 0.50205 -0.41178 0.48518 0.81604 0.79918 0.15908 -0.39856 0.30397 0.16379 -0.50475 -0.41057 0.15685 -0.63114 -0.35185 0.65554 0.53268 -0.18448 -0.068132 0.22603 -0.33779 0.17877 0.10681 0.0042481 -0.38168 -0.34329 0.19398 -0.58059 -0.31946 -0.073714 0.48785 -0.18261 0.012377 -1.0071 0.043909 -0.44222 -0.17537 1.3441 -0.64945 -2.9087 -0.35697 -0.013799 1.5677 0.7974 0.099162 1.0972 -0.70742 0.0083953 0.44134 0.085282 0.82608 0.33661 -0.21601 -0.065609 -0.13995 -0.34545 0.045238 -0.29333 0.15719 0.12832 -0.1509 -0.0070848 -0.48914 0.060983 0.40143 0.12505 -0.67478 -0.049925 -1.3447 -0.24093 0.33254 -0.084075 -0.14705 -0.41043 0.181 0.096809 -0.2399 -0.05232 -0.9476 -0.041478 0.02727 -0.18816 0.46636 0.66819\n",
      "\n",
      "national -0.0033138 0.38946 0.2635 -0.29199 0.38065 0.001211 -0.1323 0.1252 -0.81223 -0.16678 -0.19634 -0.34263 -0.097131 -0.42568 -0.41623 0.20731 1.9131 -0.058349 -0.69912 -0.36986 -0.31993 0.28531 0.56709 -0.64401 -0.36149 -0.51523 0.030871 -0.76157 0.26279 -0.16878 -0.097513 0.59321 -0.15835 0.36972 -0.22405 -0.46132 -0.12962 1.1369 -0.73445 0.36339 -0.31677 0.002493 -0.049348 -0.020933 0.77567 0.52745 0.40086 -0.42453 -0.82741 -0.96726 -0.11817 -0.37618 0.38493 0.52098 0.11486 -2.4095 0.62383 0.50036 2.1665 1.0069 -0.45087 0.2816 0.1591 -0.32918 0.59067 0.34739 -0.23214 0.30444 1.1561 0.59677 0.76722 -0.081564 0.14307 -0.64113 0.067265 0.22025 -0.20778 0.48059 -1.5678 -0.36953 0.50747 -0.20681 0.28354 -0.1962 -0.36708 -0.31784 -0.26216 0.69185 0.9425 -0.18548 0.35313 -0.34297 -0.80473 0.96275 -0.858 -0.00077044 0.099609 -0.59454 1.2107 0.0015693\n",
      "\n",
      "day -0.36689 0.4154 0.13478 -0.17841 -0.4005 0.20647 0.42742 1.129 -0.83981 -0.47955 0.021115 -0.18319 0.24588 -0.029048 0.29683 -0.44613 0.17493 -0.16619 -0.49573 -0.026547 0.76369 -0.2081 0.26494 0.33085 0.57039 0.12832 -0.28324 0.078652 0.3833 -0.074012 -0.56725 -0.25826 0.14013 0.11077 -0.21629 0.40637 -0.4574 0.27936 0.16717 -0.16388 -0.69184 -0.31493 0.65592 -0.0022426 -0.016975 0.10777 -0.18928 -0.77079 0.24333 -1.1467 0.12931 -0.59668 0.5235 1.0476 -0.83517 -2.6268 -0.32837 0.24767 2.1535 0.60561 -0.54019 0.99737 -0.18484 -0.47044 -0.019265 0.34195 0.31006 -0.31499 0.2999 -0.37647 -0.10041 -0.48373 -0.38982 -0.34424 -0.12195 0.58945 -0.15675 -0.33774 -1.0446 -0.037927 0.38237 -0.063733 -0.1299 0.12899 -1.4625 -0.22875 -0.3641 -0.10337 0.23737 -0.23498 0.15801 0.038339 0.013424 0.32535 -1.1755 0.049695 0.57227 0.02438 0.22106 0.43169\n",
      "\n",
      "country -0.10935 0.57109 0.98214 -0.070305 0.27466 0.13901 -0.6725 0.49899 0.21726 0.57449 -0.43821 0.13026 0.25537 -0.062223 -0.52117 -0.19787 0.67209 -0.43658 -0.59177 0.063773 0.46073 0.66708 1.1919 -0.19182 -0.13415 -0.045557 0.052066 -0.37631 0.49671 0.13239 0.22187 0.12634 0.34833 0.096496 0.28304 0.26515 0.41935 0.27191 -0.78991 -0.57656 -0.97647 0.23095 0.48374 0.031668 0.33351 -0.027533 0.61034 -0.4384 0.0063261 -0.97056 -0.71083 -0.1317 -0.047665 1.1634 0.035339 -2.5317 -0.21682 0.082702 2.1301 0.59453 0.1801 0.30265 -0.12746 -0.6372 0.68515 0.51137 0.19605 0.22645 0.84234 -0.45817 0.30016 -0.28873 -0.64023 -0.034291 0.66036 -0.090798 -0.32405 0.28764 -0.9558 0.057511 1.1268 0.262 -0.23766 0.33101 -0.71519 0.037798 -0.40321 -0.11256 -0.37912 -0.43801 -0.14248 0.21925 -0.35928 -0.032169 -1.2293 0.20613 -0.20616 -0.5839 0.66622 0.22983\n",
      "\n",
      "under -0.16233 0.064641 0.25104 -0.29753 0.12446 0.44832 -0.15541 0.41082 -0.68921 0.053858 0.64758 0.17853 0.4678 0.091596 0.47106 -0.90884 0.61552 -0.11448 -1.1831 -0.51489 -0.20773 0.22871 0.5811 0.063037 0.073553 -0.5261 -0.12066 -1.0567 -0.18102 -0.42476 0.4859 0.21258 -0.061614 -0.2823 -0.39257 0.85038 0.47131 -0.25344 -0.22843 -0.013176 -0.33015 0.0912 0.48117 0.16644 0.393 -0.072641 0.052517 0.082093 -0.40998 -0.68432 0.059367 0.20268 0.22247 1.3724 -0.15495 -2.5308 -0.22164 -0.5603 2.0696 0.24326 0.40051 0.50073 -0.80326 0.53268 0.73615 -0.079616 0.068064 0.1743 0.082143 0.28415 0.11307 -0.41879 0.004954 -0.19094 -0.30233 0.012008 -0.67887 0.22035 -1.0438 -0.27316 1.281 -0.17488 -0.40005 0.73022 -0.76195 0.46083 0.0049315 -0.46435 0.076681 -0.22224 0.107 -0.5882 -0.60688 0.50566 0.36358 0.058679 -0.46584 -0.23457 0.35065 -0.63155\n",
      "\n",
      "such -0.55579 0.411 -0.056376 -0.17861 0.3167 -0.29112 -0.95892 -0.27791 -0.45842 -0.054586 -0.49591 -0.77259 0.13252 0.32002 0.17383 -0.23332 0.1501 0.3166 -0.32644 0.70956 0.076804 -0.078303 0.16906 0.042042 0.038783 -0.01848 0.13735 -0.44243 -0.44468 -0.11372 0.13618 0.024879 -0.62403 -0.087121 0.45986 0.50535 0.47502 -0.026935 -0.5409 -0.41838 -0.11559 0.1777 -0.092615 -0.36311 -0.04453 0.015722 0.28546 -0.14785 -0.58193 -0.56128 0.6025 0.79511 -0.031751 0.69088 0.075027 -2.1401 0.38418 -0.15997 1.483 0.68202 -0.7515 1.1908 0.11901 0.3035 1.1821 -0.15734 0.8468 -0.0025797 0.77233 -0.54517 -0.7382 -0.48891 -0.22168 -0.2713 -0.1845 0.04291 0.13681 0.053616 -0.92298 0.01167 1.2265 -0.36393 -0.22368 0.58218 -2.2609 0.23546 -0.052839 -0.32235 -0.14765 -0.73103 -0.28457 -0.4235 -0.37963 0.05226 -0.46462 -0.30236 -0.70037 -0.7378 1.1683 0.11592\n",
      "\n",
      "second 0.09453 0.010432 0.73332 0.059561 0.16682 0.1377 0.6971 0.048718 -0.50622 0.052761 0.41331 -0.014989 -0.55503 0.17577 0.088009 -0.10314 0.052284 0.065471 -0.1492 -0.157 0.88538 -0.26589 0.33043 0.99444 0.68783 -0.48807 -0.056512 -0.24771 0.46435 -0.051928 -0.87698 0.27429 0.35614 0.0153 0.22793 -0.040942 -0.40467 0.70955 -0.23092 0.32607 -0.035074 -0.03772 0.73598 -0.023546 0.3755 0.27944 0.39402 -0.55762 0.19646 -0.73001 -0.39664 0.14038 -0.09072 0.99454 -0.44194 -3.2766 -0.6094 0.050058 1.4953 0.7415 -0.4835 0.14011 -0.073751 0.57191 0.75375 -0.29215 -0.045635 0.31105 -0.23309 -0.12204 -0.31052 0.36698 -0.094798 -0.33377 0.28714 0.49206 -0.90335 -0.45856 -0.74606 -0.19034 0.38919 0.458 -0.5935 0.36091 -1.3423 -0.60771 0.24068 -0.21933 0.65262 -0.31073 -0.11198 0.081999 -0.27444 0.50251 -0.68017 -0.10242 0.23149 0.1249 0.14628 -0.58969\n",
      "\n",
      "then -0.094451 -0.0096052 0.1793 -0.077098 0.25498 0.62439 0.2061 0.61494 0.1499 -0.2032 0.63693 0.074645 0.35557 0.63381 0.53686 -0.32466 0.14173 0.47781 -0.6301 -0.36406 0.20606 -0.16101 0.38031 0.074027 0.36054 0.19276 -0.58823 -0.52623 0.59063 -0.36221 -0.2353 1.3069 -0.22175 0.11336 -0.16082 0.11355 -0.22383 -0.057228 0.063944 -0.34529 -0.8328 -0.72404 0.025328 -0.58935 0.038914 0.26117 0.037438 -0.31577 0.50425 -0.5781 -0.26901 0.018209 0.44694 1.5009 -0.26079 -2.6008 -0.66205 -0.06467 1.2872 0.89879 0.087995 0.69217 -0.25677 0.36561 0.95983 -0.27552 0.36748 0.50315 0.050824 0.42015 -0.17387 -0.26397 0.16468 -0.53577 -0.027637 -0.032541 -0.30834 -0.21001 -0.40584 0.038996 0.57417 0.065325 -1.0168 -0.14231 -1.1527 -0.50446 0.31685 -0.44061 -0.10737 -0.44433 -0.34734 -0.63747 -0.36261 0.44908 -0.71896 -0.076308 0.30626 0.21709 -0.046804 0.13551\n",
      "\n",
      "company 0.60631 -0.37365 -0.31874 0.030332 0.18667 -1.1275 0.18433 0.16875 -0.20626 0.0087478 0.86313 -0.25189 0.40721 -0.51964 -0.79094 -0.72724 1.0097 -0.11107 0.42026 0.16993 0.076219 -0.11326 0.23855 0.41726 -0.57761 -0.14314 -0.19051 0.33742 -0.49969 -0.28331 0.25436 1.5028 -0.50439 -0.68896 -0.42216 0.46763 0.34395 0.00036374 0.26785 -0.71838 0.73955 -0.1161 -0.48352 -0.21261 -0.11326 0.59394 -0.26361 -0.25681 0.28549 -0.85409 -0.24955 0.087372 0.3393 0.86974 -0.37837 -2.4103 -0.67233 -0.39496 2.2253 0.444 0.73806 -0.23085 0.0014253 0.63356 0.2074 -0.351 0.66789 0.74544 0.42554 0.40073 -0.14964 0.37755 -0.66066 -0.47359 0.39401 -0.34221 0.4854 -0.34195 -1.7212 0.41925 0.75561 -0.29337 -0.35955 1.059 -1.3379 -0.85894 0.074892 -0.5918 -0.1528 -0.44238 0.096469 -0.538 -0.049649 -0.47351 -0.27107 0.62337 -0.11412 -0.35592 1.0019 0.14789\n",
      "\n",
      "group 0.060976 0.38679 -0.25953 -0.45005 0.63819 0.35468 0.36497 -0.0064529 -0.49058 0.32951 0.86002 -0.5863 0.38647 -0.27503 -0.48817 0.27045 0.158 -0.91305 0.17947 0.11964 0.66995 -0.50165 0.43762 0.33298 -0.67132 -0.25389 0.14377 0.3523 0.16418 -0.16632 -0.11202 0.76496 -0.44687 0.11007 0.52306 -0.19121 0.69482 1.2401 0.10421 -0.63267 -0.59908 0.19479 0.12724 -0.037123 0.21433 -0.23203 -0.51001 1.0355 -0.32743 -0.21527 -0.50623 0.082396 -0.2465 0.80009 -0.44878 -2.6585 -0.13689 -0.30755 1.8495 1.4421 0.32334 -0.29875 0.09452 -0.349 0.7357 0.37951 0.12885 1.3724 0.85535 0.12818 -0.30751 -0.59735 -1.0629 -0.83931 -0.61702 -0.34366 0.030624 -1.0825 -1.08 -0.063582 0.64191 -0.034814 -0.16023 0.65881 -1.2872 -0.69742 -0.27271 -0.32249 0.34637 0.22391 -0.20412 -0.91463 0.15462 0.14573 -0.20632 0.41097 -0.6167 -0.41001 0.86225 0.066058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "any -0.23676 0.15659 0.30243 -0.15578 -0.39025 0.11214 -0.42827 -0.13996 0.5 0.24438 0.078795 0.46397 0.14506 -0.046024 0.37514 0.0081255 -0.36021 0.44383 -0.0091813 0.52886 -0.20502 -0.32799 -0.17999 -0.64124 -0.15088 -0.040234 -0.12477 -0.7817 0.054377 -0.25998 0.030353 0.38581 -0.13027 0.034101 0.35053 0.26333 -0.14078 -0.074158 -0.40511 0.016175 -1.035 0.12603 0.70112 -0.4682 0.10376 -0.075453 0.38597 -0.44495 -0.62957 -1.2194 0.71918 0.27349 0.23366 0.8171 -0.10224 -2.4382 0.40213 -0.34281 2.4308 0.53376 -0.43856 0.65178 -0.63478 0.47431 1.3655 -0.32165 0.88175 0.46126 -0.036273 -0.089717 -0.48678 -0.628 -0.0014089 -0.4462 0.76526 0.18874 -0.28533 -0.71153 -1.5213 0.32432 0.65619 -0.46233 -0.23229 0.52435 -1.6439 0.047064 0.23839 0.41897 0.40535 -0.49672 -0.66878 -0.008549 -0.26385 -0.31069 -0.44691 -0.32531 -0.17917 -0.16095 0.75331 0.29294\n",
      "\n",
      "through 0.059074 -0.042707 0.2587 -0.18368 0.81468 0.58292 -0.26083 0.82002 -0.23465 -0.33098 0.25085 0.070227 0.24189 0.094269 0.39473 -0.34851 0.34429 0.23637 -0.45752 -0.45037 0.14996 0.66847 0.46176 0.52291 0.2215 0.016204 -0.041678 -0.3376 -0.029949 -0.19353 -0.2832 0.66651 -0.141 -0.24017 -0.10974 -0.050707 -0.29626 -0.19629 -0.44914 0.31468 -0.37233 -0.4456 -0.46963 0.12732 -0.18592 0.045871 -0.013382 -0.048189 0.52943 -0.34906 -0.18348 0.54491 0.090524 1.1006 -0.1781 -2.8729 -0.09575 -0.33037 2.3164 0.24153 0.070569 1.208 0.13551 0.0084232 0.44702 -0.063722 0.1993 -0.328 0.51665 -0.16745 -0.02526 -0.22935 0.35127 -1.0707 0.25037 -0.13702 -0.24066 -0.1886 -0.7476 -0.14333 0.92814 0.89124 -0.31394 0.40284 -1.2105 0.26758 0.10446 -0.21701 -0.63285 -0.13103 -0.29635 0.16082 -0.63475 0.62492 -0.25714 -0.24293 0.32918 0.10145 0.50609 -0.0081603\n",
      "\n",
      "china 0.35996 0.45571 1.1417 0.45611 0.9601 -0.95995 -0.9254 0.127 0.51292 -0.039701 -0.26548 -0.013654 1.0868 0.30419 -0.77058 -0.054124 0.4386 -0.51727 -0.61157 -0.78045 1.108 0.17912 0.13787 1.0502 -0.10599 0.28804 0.084783 0.7445 -0.072019 -0.30362 -1.1903 1.3395 0.80093 -0.11032 0.0036099 0.075959 0.38789 -0.19721 -1.276 -0.59604 -1.4247 -0.61494 0.26129 -0.4681 0.4763 -0.12843 0.24786 0.1086 0.36115 -1.4185 0.27347 0.050184 -0.21788 0.90376 -0.29338 -1.9606 0.16602 -0.10826 2.1584 -0.024248 -0.82154 -0.01291 -0.33662 0.47957 0.054286 0.54413 -0.31565 0.79236 0.579 0.06817 0.27004 -0.097153 -0.043945 0.26236 0.18507 0.38865 -0.24258 -0.063185 -1.1143 0.034371 1.3703 0.35007 -0.46679 0.26667 -1.1704 0.25955 -0.73628 0.04368 0.36358 -0.27023 1.2032 0.091401 0.028696 -0.54619 -0.56014 0.26147 -1.3244 -1.446 0.54944 -0.49465\n",
      "\n",
      "four -0.090728 0.48545 0.10712 -0.39908 0.47852 0.46725 0.64062 0.91219 -1.0121 0.12732 0.92199 0.023376 0.19655 0.81392 0.1694 -0.13135 -0.26604 -0.025472 -0.61158 0.34201 0.93331 -0.13451 0.0032018 0.39143 0.7596 -1.0016 -0.2789 -0.32642 -0.30005 0.35931 0.19587 0.027893 -0.31228 -0.33626 0.28018 0.119 -0.1763 0.31186 -0.37048 0.54743 -0.51829 -0.21469 0.70186 -0.24548 0.46121 0.34881 -0.025165 -0.67807 -0.17137 0.044035 -0.78905 -0.51038 -0.26326 1.2835 0.051326 -2.534 0.043636 -0.022349 1.7996 1.3997 -0.74978 0.92709 0.18243 0.48048 0.2948 0.27161 0.269 0.71182 -0.11844 0.11265 -0.34046 0.12676 -0.020973 -0.1484 0.18365 0.60186 -0.30049 -0.4193 -0.92757 -0.1838 0.99586 0.22228 -0.37084 0.35611 -1.2263 -0.042407 0.18479 -0.28635 0.21577 0.1357 -0.18895 0.32994 -0.52937 -0.18664 -1.2578 0.15864 -0.73338 0.14197 0.60873 -0.16741\n",
      "\n",
      "being -0.066713 0.16312 0.39104 -0.43637 0.20587 0.5614 0.14023 0.28105 0.0067757 -0.057836 0.22737 0.2445 1.0082 0.051993 0.34479 -0.7716 0.42875 0.044328 -0.73092 0.10703 0.3736 0.15977 0.58248 0.031623 0.1512 -0.11788 -0.12138 -0.24645 -0.11342 0.63284 0.29311 0.39929 -0.23115 0.4173 0.78362 0.068325 -0.48132 0.30337 -0.3757 -0.04 -0.55809 -0.29392 0.41674 -0.061356 0.65692 0.29186 0.49982 0.10704 0.16136 -0.8466 -0.065287 -0.29757 0.46649 1.499 -0.70022 -2.1676 -0.26145 -0.12021 1.5073 0.48453 0.033081 0.83274 -0.92158 0.30245 0.95851 -0.42331 0.58268 0.23614 0.089298 0.10154 -0.14591 -0.47707 -0.092998 0.14452 -0.090049 -0.1429 0.1608 0.14361 -1.0624 0.11548 0.21195 -0.12 -0.65965 0.17182 -2.1704 -0.46459 0.048327 -0.2218 -0.65073 -0.95879 -0.73309 -0.58657 -0.10877 0.27312 -0.32786 0.071058 -0.23118 -0.075121 0.52134 -0.1822\n",
      "\n",
      "down 0.45295 -0.028874 0.53708 -0.44492 0.12465 0.069802 -0.61197 -0.020487 -0.19857 0.033731 0.4908 0.47231 -0.27657 -0.049415 0.48814 -0.041704 -0.50964 0.17315 -0.34757 0.035868 1.0729 0.28502 0.87049 0.4053 0.82742 -0.080395 -0.86551 -0.41816 0.098446 -0.21815 0.17315 -0.0073093 0.13693 0.19441 -0.38958 0.65683 -0.029769 -0.49179 0.087391 0.21117 -0.59994 -0.7492 -0.14441 -0.54999 -0.17102 0.50914 -0.50235 0.31347 0.049133 -1.1953 0.10967 -0.20421 0.40213 1.7408 -0.76203 -2.4653 -0.38833 0.41629 2.5546 0.7788 0.33431 0.64454 -0.67114 0.27918 -0.058202 0.096197 0.65124 0.33595 0.18495 0.44382 -0.057823 -0.33577 -0.11045 -0.62421 -0.17163 -0.35289 -0.39223 0.48951 -0.11894 0.064803 0.48449 0.35404 -0.64609 0.056483 -1.0333 -0.34734 0.32433 0.34618 -0.21469 0.4238 -0.43765 -0.18232 -0.44092 0.33762 -0.79835 -0.3374 0.087652 0.18923 0.6027 0.055227\n",
      "\n",
      "war -0.39505 1.0285 -0.21556 0.36596 -0.34455 -0.23504 -0.18961 -0.81103 0.17359 -0.017903 0.059981 -0.35961 0.16133 0.40454 0.24142 -0.19124 0.69767 -0.91375 -0.85926 -0.44406 0.89111 -0.18274 0.67546 -0.16622 0.36994 -0.057873 -0.70059 -0.16782 0.88317 -0.40334 0.31702 -0.51506 -0.22962 -0.24648 -0.097251 0.020845 0.25491 -0.36664 0.13173 -0.23379 -0.7412 0.10375 0.29233 -0.37992 0.73483 -0.54725 0.84773 -0.61281 -0.1322 -0.63366 0.26397 0.21946 0.319 1.7453 0.13056 -2.1285 0.36317 -0.56862 1.7781 0.60787 0.16995 1.3091 -0.053037 -0.63826 1.0207 -0.0066656 -0.11157 0.52914 -0.38402 0.83575 0.83537 -0.71898 -0.77896 0.21978 -0.12994 -0.0078993 -0.13605 1.1705 -1.7038 0.5049 0.88308 0.32708 0.33586 1.0149 -1.2046 0.30523 -0.74755 -0.62652 0.086079 -0.70008 -0.57518 -0.1737 0.08678 0.9448 -0.42012 -0.39687 0.21711 0.4731 -0.16715 0.18784\n",
      "\n",
      "back 0.081466 0.26043 0.57471 -0.86109 -0.040796 0.38158 -0.3095 0.25425 -0.20413 -0.41511 0.09182 0.45834 -0.016965 0.42885 0.23142 -0.14881 -0.30889 0.25861 -0.4643 -0.18131 0.78614 0.58659 0.065481 0.38943 0.5905 -0.070652 -0.71482 -0.99128 0.64624 -0.40502 -0.26623 0.26768 -0.12351 0.25146 -0.17331 0.44574 -0.52414 0.17666 -0.3309 -0.131 -0.58892 -0.54684 0.28784 -0.4185 -0.3522 0.18347 -0.2551 0.034252 0.58586 -0.7979 0.008667 0.35888 -0.0096041 1.2454 0.15646 -2.9822 -0.49468 -0.44974 1.9194 0.62321 0.37604 1.1286 -0.37189 0.45544 0.37194 0.14495 0.50771 0.2642 -0.45295 0.28479 0.19262 -0.00015116 -0.25621 -1.0478 0.052751 -0.26299 -0.604 0.30831 -0.32731 -0.11796 0.54481 0.41001 -0.58269 0.090019 -0.90827 -0.23671 0.13968 -0.051941 -0.76537 -0.21075 -0.46803 0.084476 -0.31192 0.25794 -0.64003 -0.31881 0.089077 0.51851 0.47458 -0.076461\n",
      "\n",
      "off -0.29462 -0.20737 0.49109 -0.79814 0.008747 0.092181 -0.47655 0.16151 -0.51687 -0.0168 0.51022 0.74934 -0.1134 0.1749 0.49024 -0.45299 -0.40995 0.4446 -0.15581 -0.010142 1.1087 0.5946 0.82161 0.18967 0.74758 0.6662 -1.155 -0.038335 -0.15052 -0.42034 -0.4453 0.25774 0.16361 0.32551 -0.26821 -0.13163 -0.40298 -0.2704 -0.28804 0.31894 -0.55611 -0.66131 0.27226 -0.33844 -0.015587 0.27291 -0.11855 0.15712 0.12577 -0.26712 -0.28589 0.57764 -0.39933 0.93056 -0.46425 -2.9012 -0.74093 -0.067453 2.3852 0.94418 0.075616 1.2029 -0.22625 0.5203 0.027958 0.0093798 0.58799 0.014338 -0.16291 0.076174 -0.69147 -0.10234 -0.17892 -0.83485 0.028962 -0.085684 -0.46861 0.12444 -0.38236 0.24328 0.5638 0.014457 -0.65938 0.19099 -1.0456 -0.69703 0.20032 0.53253 -0.24324 -0.4915 -0.34631 0.067113 -0.17565 -0.23811 -0.62531 0.13135 -0.0026693 0.57755 0.52168 -0.027015\n",
      "\n",
      "south -0.46355 0.30011 1.4216 0.20368 -0.20557 -0.20062 -0.014366 0.74383 -0.26145 0.044727 -0.15666 0.23816 0.385 0.48355 0.43066 -0.36029 0.69783 -0.70803 -0.95371 -0.51525 1.4756 0.66914 1.1782 -0.04645 -0.11749 0.28231 -0.13011 -0.078877 0.042502 -0.029801 -0.61864 0.37803 0.18971 0.012878 0.38955 -0.45662 0.39821 -0.036629 -0.77164 0.31884 -1.397 -0.45901 -0.10381 -0.24859 1.0008 0.32565 0.8159 -0.23511 -0.043356 -0.073391 -0.67623 0.31377 0.05917 0.089361 0.050295 -2.7353 -1.3232 -1.1276 1.617 -0.27439 -0.49666 0.062247 0.11442 -0.20227 0.25736 0.23519 0.21225 1.0193 0.28335 0.4926 -0.055166 -0.028122 -0.68347 -0.9725 0.39258 0.030163 0.42864 0.68363 -0.3386 -0.10153 0.72535 0.31754 -0.054737 0.03218 -0.85991 0.086093 -0.62255 0.11683 0.87957 0.184 -0.066925 -0.20197 -0.32427 0.52992 -0.88029 0.62955 -0.70924 -0.046156 0.98717 0.23584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "american 0.38666 0.64827 0.72807 -0.077056 0.1545 -0.19704 0.092145 -1.1485 -0.37113 0.24019 -0.24023 -0.89308 0.12862 0.013445 -0.29047 0.26244 0.87932 -0.18065 -0.44722 0.21253 0.76651 0.17738 0.57841 -0.23391 0.93055 0.16157 0.10065 -1.445 0.58182 -0.1713 -0.42354 0.50944 -0.41707 0.060952 -0.25351 0.08773 -0.078008 0.75344 0.28149 0.43221 -0.95782 -0.48864 -0.43305 0.98063 0.41688 -0.25731 -0.11483 -0.22824 -0.3195 -0.92569 -0.51809 0.11046 0.25846 0.31938 0.031387 -2.1859 -0.18954 -0.3671 2.2003 0.74812 0.36128 0.72895 0.53194 -0.45152 0.55719 -0.91869 -0.0022865 0.8208 0.35648 0.29181 -0.25615 -0.036604 -0.81117 0.16188 -0.046315 0.47251 0.57548 0.17264 -1.2795 -0.54711 0.39105 -0.046598 0.025004 0.93374 -1.113 -0.37478 -0.092246 0.30074 0.22905 -0.91934 -0.26528 -0.26074 0.043642 0.077836 -0.9252 -0.16832 -0.62285 -0.45475 0.39219 0.54088\n",
      "\n",
      "minister -1.5181 -0.74831 0.26892 0.63476 0.32357 -0.95472 -0.6337 0.13456 0.49422 0.24721 -0.63219 -0.010343 -0.54033 1.1952 0.36358 -0.79503 -0.4432 -0.71616 -1.8854 0.15682 -0.34752 -0.035672 0.26559 -0.17897 -0.30657 -0.45384 0.32952 -0.008902 0.49161 0.3208 -0.27389 0.56355 -0.13763 -0.23259 -0.81422 0.23789 -0.079768 0.55482 -1.0685 -0.37092 -0.57952 -0.19947 1.7246 0.037072 0.90249 -0.38501 -0.27391 -0.0041161 -0.49732 -0.29864 0.52018 -0.46142 0.73608 1.4288 -0.28658 -1.7579 -0.96038 1.4184 1.3213 0.90666 0.098256 -0.68313 -0.08813 -0.7653 0.60155 0.56895 0.66207 0.68117 0.68739 -0.2526 0.36769 -0.71332 -0.57661 -1.4502 -0.26336 0.51095 -1.0664 1.3521 -0.91373 -0.15711 0.77271 0.23702 0.41194 -0.59416 -0.61156 -0.82853 0.14085 0.75951 0.45324 -1.6896 1.1959 -0.67534 -0.44309 0.75635 -0.69421 0.47412 -0.15753 -0.50286 0.05486 -0.81682\n",
      "\n",
      "police 0.1049 -0.86604 -0.18197 -0.72754 -0.3902 1.5339 -0.44362 1.0582 0.27323 1.0541 -0.097547 0.11879 0.21506 0.41213 -0.015035 0.085687 0.62685 -0.48606 -1.4832 0.054788 1.2538 -0.7414 0.45702 -0.45541 -0.60237 -0.43152 0.41912 -0.75058 0.047356 0.4164 0.067822 -0.51007 -0.43174 0.62177 0.26511 -1.1647 -0.11742 -0.054609 1.2499 0.46319 -0.37561 -0.20357 0.68529 -0.17209 0.27742 0.13279 -0.16843 -0.091958 0.26287 -0.21859 0.63782 -0.42959 -0.22181 1.7921 -0.34111 -1.5887 -0.82064 0.23953 1.8556 0.55727 -0.35967 0.70907 -0.024148 0.57945 0.48563 -0.21334 0.060956 0.35844 0.79628 1.2526 -0.093424 -0.78124 0.2308 -1.0177 0.43092 0.29937 0.52166 0.40109 -1.4047 0.084946 1.6508 -0.16834 0.063023 -0.45757 -1.1389 -0.4777 0.017596 0.62116 -0.58329 0.20403 0.75644 -0.87732 -0.73291 -0.23177 -0.41446 0.44718 -0.092381 0.079014 0.1935 0.026815\n",
      "\n",
      "well -0.53086 0.51404 0.087599 -0.37314 0.2747 0.07947 -0.0085023 0.028399 -0.35114 0.094339 0.087771 -0.38307 0.43129 0.15261 -0.1512 -0.4607 0.080433 0.037627 -0.43959 0.42451 0.16058 0.26608 0.35311 0.014055 -0.052771 -0.1615 -0.299 -0.56214 -0.18742 0.044237 -0.28118 0.36594 -0.26226 0.11013 0.44358 0.43131 -0.0053095 0.34705 -0.44883 -0.33727 -0.13281 -0.35542 -0.081663 -0.12983 0.080606 -0.161 0.367 -0.30568 0.057269 -0.794 -0.24581 0.027115 0.13203 1.2262 -0.19183 -2.5497 0.055273 -0.1378 1.4552 0.53697 -0.12337 1.1278 -0.16365 0.21871 0.82735 -0.30681 0.65456 0.17636 0.6172 -0.18425 -0.029966 -0.098315 0.32056 -0.28124 0.25684 -0.034462 -0.12968 0.1944 -0.42318 -0.12843 0.84729 0.17807 -0.39679 0.29828 -1.7337 -0.037541 -0.02989 -0.14391 -0.33299 -0.52234 -0.12178 -0.2509 -0.17904 0.049504 -0.62184 0.20902 -0.55805 -0.55397 0.56137 0.39822\n",
      "\n",
      "including 0.0139 0.39163 0.25724 -0.25674 0.94204 0.12373 -0.20938 0.054664 -1.0115 0.25815 0.12685 -0.7307 0.32935 0.53763 0.14463 -0.23226 0.28712 -0.35405 -0.73811 0.81866 0.35511 -0.056426 0.49399 0.23793 0.11212 -0.43093 0.15842 -0.22002 -0.25638 0.13653 0.18768 -0.26305 -0.36465 -0.12591 0.17239 0.22494 0.56454 0.33783 -0.60966 0.28113 -0.14947 -0.09533 0.17146 0.059489 0.27415 0.18265 -0.049619 -0.47251 -0.50669 -0.1765 0.014693 0.40691 -0.42133 0.91894 -0.060366 -2.036 0.032837 -0.3189 1.3219 0.78211 -0.9035 1.0438 0.35604 0.25706 0.7254 -0.074623 0.58051 0.38709 0.86653 0.029172 -0.59042 -0.015967 -0.41565 -0.069928 -0.52851 0.34929 0.087923 0.64766 -0.91468 -0.099785 1.3607 -0.081286 0.33493 0.49856 -1.6837 0.19294 0.24941 -0.1423 -0.11735 -0.30267 0.19788 -0.487 -0.22453 -0.20016 -0.95076 0.061864 -0.56481 -0.45471 1.5334 -0.27995\n",
      "\n",
      "team 0.11103 -0.18744 0.70696 -1.2253 -1.1401 0.051085 0.7636 0.56708 -1.0994 0.07472 0.58216 -0.6536 0.50644 -0.44575 -0.85106 0.14226 0.65977 0.39684 -0.39651 0.16967 -0.46576 0.39121 -0.25304 0.137 -0.18157 -0.44593 -0.089636 -0.34056 0.58617 -0.52245 -1.1103 0.94575 -0.91187 0.10707 0.42819 -0.34262 -0.8236 1.1897 -0.68036 -0.083254 -0.092193 -0.037007 0.73434 -0.52417 0.38396 0.56477 0.51137 -0.047542 0.13947 -1.0207 -0.29109 0.043556 -0.28533 1.1217 0.29893 -2.691 -0.7453 0.1347 0.97448 1.2629 -0.49354 0.36527 -0.04841 0.49881 0.66412 -0.044346 -0.22314 1.0335 0.01257 0.83781 0.39779 -0.44725 -0.61794 -0.47753 0.6616 0.53672 -0.21837 0.066238 -0.64173 -0.14777 0.68965 0.58756 -0.77743 -0.18797 -0.91153 -0.45953 -0.12344 0.30792 -0.015992 -0.1678 -0.61115 0.19262 -0.13384 0.70767 0.060688 0.0066863 -1.2444 0.44056 0.67693 0.146\n",
      "\n",
      "international 0.34517 0.54783 0.081653 0.17274 0.44512 -0.22739 -0.50975 -0.034941 -0.70357 0.044725 -0.067975 -0.74376 0.19325 -0.32008 -0.39996 0.38556 0.81632 -0.0012905 -0.69377 -0.19589 -0.40613 0.32552 0.71915 -0.28449 0.52037 0.43181 -0.068946 -0.75758 0.34758 0.17501 -0.73412 0.24389 -0.99088 0.90901 -0.24017 0.18413 0.88445 1.1091 -0.84288 -0.06175 -0.82852 0.17286 0.071618 -0.0018393 0.70276 0.32097 -0.017803 -0.05287 -0.25393 -0.45338 -0.059925 0.12189 -0.50072 0.55082 -0.22917 -2.4402 -0.27099 0.33838 2.2478 0.053165 -0.33948 0.16006 -0.81612 0.047171 0.03821 -0.59658 -0.95339 0.63153 1.081 0.25331 -0.11029 -0.45493 0.087209 -0.65615 0.15242 -0.54559 -0.18514 -0.37079 -1.9854 -0.11369 1.5798 -0.065584 0.51212 0.36988 -1.3036 0.016299 -0.36312 0.23785 0.5478 -0.56036 -0.37925 0.37737 -0.39932 -0.26676 0.1382 1.225 0.029947 0.10666 1.0845 0.78914\n",
      "\n",
      "week -0.1428 0.038197 0.5243 -0.43998 0.065116 -0.17529 0.43673 0.85459 -0.58619 -0.15223 -0.078494 -0.040785 0.033358 -0.31544 -0.19734 0.14097 -0.33652 -0.49646 -0.57489 -0.070677 0.5731 -0.12777 -0.05229 0.69314 0.28072 -0.2946 -0.14781 -0.24021 0.055859 0.050689 -0.043951 -0.36493 -0.19972 0.10731 -0.30899 0.56589 -0.3767 0.028393 -0.22639 -0.11031 -0.55298 -0.29535 0.43896 -0.00644 -0.35717 -0.044278 -0.07768 -0.72471 0.5131 -1.2522 0.46653 -0.71526 0.32788 0.85319 -0.54161 -2.6231 -0.513 -0.26244 2.055 0.94851 -0.51229 0.65247 -0.098867 -0.52455 -0.051452 0.3422 0.06331 0.63046 0.0060254 -0.041402 -0.16317 -0.27704 -0.81532 -0.23948 -0.63513 0.49969 -0.13177 0.18217 -1.3799 0.18881 0.77237 -0.30225 -0.51549 -0.15619 -1.2113 -0.72584 0.32968 0.41439 -0.29326 -0.32925 0.23301 0.2969 -0.35814 0.34856 -0.86306 0.51287 0.32494 0.3909 0.78097 0.36744\n",
      "\n",
      "officials 0.063801 -0.61432 0.57455 -0.23404 -0.38216 -0.021625 -0.34182 0.56059 0.15122 0.15538 -0.3782 0.061681 0.79727 -0.00061645 -0.64067 -0.31102 0.24812 -0.59337 -0.86212 -0.18156 0.15097 0.045826 0.072973 -0.22128 -1.2829 -0.38266 -0.14573 0.01544 -0.079678 -0.033909 0.52671 -0.053086 -0.48953 0.66801 -0.0037587 0.22434 0.055413 0.31869 0.03163 0.46196 -1.215 -0.54407 0.1619 -0.22135 0.52349 -0.39311 0.11697 -0.40948 -0.026347 -0.65472 0.56113 -0.62753 -0.11244 1.2978 -0.19826 -1.6368 -0.5039 -0.12567 2.1101 1.0048 -0.63807 0.4485 0.36324 0.040423 0.19986 -0.29579 -0.027335 1.2083 0.24439 0.23955 -0.22642 -0.8055 -0.49464 -0.78149 -0.046005 0.19098 0.46848 0.42185 -1.5833 0.43054 1.1885 -0.31252 -0.32957 -0.2324 -1.5889 -0.21648 0.29481 0.64352 -0.25566 -0.1612 0.61975 -0.23812 -0.84205 -0.32452 -0.6881 0.29374 -0.45491 -0.013856 0.5996 -0.04223\n",
      "\n",
      "still -0.04248 0.80249 0.51451 -0.55427 -0.13799 0.079889 -0.11496 0.1452 0.35848 -0.47221 0.37094 0.4251 0.76855 0.069762 -0.21962 -0.59782 0.29315 0.054046 0.13168 0.1569 0.29535 0.58609 0.5583 -0.098406 0.075102 -0.41336 -0.16779 -0.65766 -0.10377 -0.43977 -0.038655 0.17153 -0.31522 0.27599 0.37496 0.21466 0.21868 0.047922 0.26769 -0.27468 -0.64192 -0.14946 0.62803 -0.35354 0.018262 -0.31361 0.62542 -0.2398 0.40872 -1.028 0.36542 -0.69959 -0.045517 1.4999 0.16624 -2.5515 -0.20047 -0.56672 1.2009 0.8979 -0.05592 0.78017 -0.14677 0.072153 0.6276 0.34021 0.7572 0.22929 -0.2739 -0.28335 0.33604 -0.58072 -0.2415 -0.023542 0.17794 -0.17189 -0.19986 0.11794 -1.0303 -0.19335 0.62205 -0.073824 -0.91812 0.15507 -1.6302 -0.51868 -0.17676 -0.30267 -0.31163 -0.045452 -0.015646 -0.22552 -0.053651 -0.047551 -0.79241 0.37514 -0.31145 -0.64216 0.49236 0.56897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "both -0.30687 0.16697 0.0040692 0.016687 0.45926 -0.025039 -0.21391 -0.030337 -0.42455 -0.23867 0.16391 -0.3877 0.4979 0.28546 -0.060268 -0.3068 -0.15175 0.21051 -0.58625 0.36114 0.41502 -0.38151 0.50545 0.24807 0.25556 -0.42775 0.10067 -0.57864 0.11877 0.2288 0.21748 0.75993 -0.22467 -0.087453 0.45775 0.46619 0.4355 0.561 -0.59455 0.21146 -0.53216 -0.40402 0.11441 0.033054 -0.039832 0.11314 0.42284 -0.27464 0.013095 -0.39035 -0.010706 0.28875 0.04262 0.97421 0.067472 -2.3519 0.21347 -0.29792 1.2593 0.65242 -0.29867 0.80374 0.10806 0.41319 0.66098 0.14132 0.57976 0.7575 0.30592 -0.032431 -0.19615 -0.01936 0.18367 -0.40983 0.32454 0.022286 -0.31395 -0.20126 -0.71793 -0.22198 0.822 0.047957 -0.38488 0.17195 -1.6246 -0.080424 -0.33738 0.12957 -0.22077 -0.61825 -0.47566 -0.31097 -0.48041 0.57019 -0.8467 -0.014167 -0.84985 -0.082094 0.70251 0.2309\n",
      "\n",
      "even -0.15308 0.63194 0.65512 -0.30706 -0.23919 0.137 -0.29819 -0.1408 0.36013 -0.13795 -0.12024 0.006781 0.073785 -0.16167 0.23611 -0.40154 -0.18858 0.32314 -0.26595 0.35438 0.29291 0.1865 0.072136 -0.13468 0.35318 0.024417 -0.58386 -0.91234 0.077784 -0.40844 0.47068 0.33693 -0.29663 -0.065589 0.27033 0.67653 -0.073111 -0.069452 -0.28631 -0.1368 -0.37174 -0.14945 0.026424 -0.26575 -0.2811 -0.24439 0.72433 -0.29779 0.13543 -1.1994 0.31849 -0.3171 0.075688 1.1812 -0.033059 -2.5578 0.064297 0.018054 1.6014 0.58689 -0.21065 0.98208 -0.11958 0.038047 0.7574 -0.024381 0.82353 0.36967 0.20951 -0.13652 -0.1313 -0.4226 -0.079352 -0.20586 0.03849 -0.0017904 -0.18728 -0.36023 -0.71718 0.11153 0.70959 -0.13703 -0.80791 0.064399 -1.8962 -0.32261 0.076969 0.02553 -0.33365 -0.47001 0.18823 -0.28496 -0.14806 -0.12197 -0.85082 -0.16775 -0.52967 0.003499 0.36214 0.41474\n",
      "\n",
      "high 0.23191 0.12153 0.29386 0.072764 -0.38304 -0.4951 -0.4896 0.38106 -1.1385 0.72336 -0.27039 -0.39156 -0.081569 0.32687 0.50732 -0.50271 0.026935 -0.09699 -0.0843 0.075377 -0.098599 0.080029 0.66816 0.21625 0.40719 -0.076217 -0.24698 -0.76626 -0.45836 0.52711 0.21671 -0.0091295 0.061904 0.60053 0.062351 0.032397 -0.022087 0.28614 0.25518 0.36703 -0.57334 -0.29495 -0.64289 -0.96003 0.66243 -0.012565 0.713 -0.15858 0.55935 -0.56437 -0.16974 -1.1061 -0.23437 0.6346 -0.1439 -2.9843 0.34014 -0.15353 2.2771 -0.071274 -0.23329 0.3215 -0.34058 0.19131 -0.26933 -0.49666 0.82841 -0.1702 0.60831 0.27916 0.17661 -0.50922 0.14518 0.086441 0.098435 0.18201 0.2101 0.10286 -0.521 -0.48947 0.95318 0.4882 -0.58288 -0.72219 -0.79387 -0.39084 0.36444 -0.46133 0.23086 -0.41769 -0.88894 0.05182 -0.72167 0.25381 -1.0825 -0.093222 -0.35774 -0.69988 0.98634 -0.12299\n",
      "\n",
      "part -0.3199 -0.024433 0.20789 0.16225 0.15196 0.17858 -0.13724 0.54366 0.02117 0.23395 -0.23099 -0.24552 0.16699 0.011151 -0.21812 -0.3093 0.61586 -0.10272 -0.11437 0.20891 0.32216 0.1097 0.12841 -0.094923 -0.10593 -0.19763 0.080573 -0.061984 0.042453 -0.074485 -0.38931 0.35404 0.28063 -0.33088 -0.12692 0.14309 0.36154 0.43514 -0.033155 -0.1264 -0.22033 -0.092006 0.52187 0.027293 -0.0353 0.48102 0.7839 -0.25325 0.10255 -0.21911 -0.28266 0.54783 0.38431 0.72488 -0.64801 -2.968 -0.086925 -0.78882 1.9083 0.44895 0.19248 0.94306 0.068897 0.14307 1.2094 -0.33741 0.13741 0.39818 0.19399 0.053126 0.36169 -0.45354 -0.21171 -0.012892 -0.049958 -0.099733 -0.098361 0.45214 -0.90885 0.0765 0.34954 0.28382 -0.12574 0.32786 -1.412 0.11607 -0.40668 -0.30004 0.36986 -0.15685 -0.33788 -0.44267 -0.28023 0.31825 -0.29852 0.075114 -0.070026 -0.36156 0.66558 0.29917\n",
      "\n",
      "told -0.36487 -1.0253 0.37449 -0.57999 -0.323 -0.059796 -0.048491 0.69654 0.33505 0.21342 0.042622 0.23875 0.34311 -0.038404 -0.1489 -0.39182 -0.024462 -0.23775 -1.191 -0.22377 0.10612 0.14198 -0.42915 -0.46446 -0.33337 0.078067 0.11659 -0.13983 0.57036 0.78466 -0.23472 0.35145 -0.035151 0.29467 0.16995 0.52231 0.32086 1.0108 0.44129 0.083439 -1.4022 0.030807 0.19237 -0.12492 -0.41161 -0.15225 -0.42076 -0.69464 -0.16314 -0.44731 0.31502 -0.39191 0.72395 0.50493 -0.48478 -2.1741 -0.66258 0.2004 1.2748 1.4007 0.23057 0.17452 -0.46783 -0.70192 0.4892 -0.011783 0.79652 1.2713 0.012877 0.532 0.30135 -0.1722 0.30012 -1.075 0.88672 0.52228 0.10207 0.13795 -1.432 0.13408 0.99242 -0.38746 0.12052 -0.57338 -1.1177 -0.83004 -0.51032 0.62698 -0.53809 -0.45787 1.1473 -0.96821 0.56373 0.16886 -0.59601 0.57969 0.15051 0.065298 0.19018 0.61932\n",
      "\n",
      "those -0.086328 0.71453 0.24515 -0.58431 -0.14725 0.22567 -0.7255 0.051926 -0.23524 -0.049389 0.44426 0.058565 0.27032 0.091887 0.60136 -0.52043 -0.078429 0.30721 -0.6383 0.55291 0.076026 -0.1549 0.2816 -0.14582 -0.12192 -0.56131 -0.017986 -0.67966 -0.0028712 -0.31489 1.061 0.16814 -0.26173 -0.18811 0.17729 0.32642 0.19411 0.13581 -0.41791 -0.04444 -0.70464 -0.26934 0.21708 -0.20774 -0.23883 -0.074987 0.21967 -0.46312 -0.43865 -0.97407 0.16563 -0.11702 0.020578 1.0685 -0.070974 -1.7308 0.49863 -0.3951 1.9798 0.68856 -0.37869 1.2525 -0.023968 -0.07395 0.49882 0.21943 0.73856 0.35784 0.25945 -0.32934 -0.44093 -0.37538 -0.28288 -0.37153 0.087367 0.044926 -0.093523 -0.19693 -0.90824 -0.075175 0.85121 0.19206 -0.53852 0.53345 -2.3331 -0.1866 -0.33459 -0.31109 -0.42566 -0.25325 -0.4154 -0.12662 -0.25036 -0.35017 -1.2349 -0.12078 -0.99782 -0.31173 0.59295 0.20544\n",
      "\n",
      "end 0.14726 0.088411 0.28386 -0.2797 -0.32498 -0.31503 -0.16317 0.49901 -0.050685 -0.13406 0.18811 0.17244 0.19983 0.106 0.44888 -0.16482 -0.15723 -0.2267 -0.52985 -0.19136 0.66418 -0.34908 0.2041 0.19812 0.96231 -0.076071 -0.40823 -0.54391 0.4698 -0.42401 -0.20947 0.25178 0.081003 -0.54254 -0.15188 0.16457 0.018161 -0.074509 -0.19323 -0.27414 -0.060197 -0.16855 0.37944 0.015304 -0.10552 0.025971 0.23706 -0.46123 0.22297 -0.83349 0.15802 -0.20303 -0.12614 1.0924 -0.28469 -2.8841 -0.48502 -0.70376 1.9497 0.3812 -0.42812 0.48972 -0.41521 -0.43973 0.66411 -0.094294 0.52652 0.090762 -0.28974 -0.43982 0.088801 -0.33429 -0.77197 -0.60414 -0.077605 -0.34217 -0.68175 0.72201 -0.93264 -0.23963 0.43971 0.08899 -0.44907 0.48743 -1.0736 0.32468 -0.11632 -0.2534 0.23838 -0.18391 -0.27039 0.72632 -0.61252 0.56877 -0.7907 -0.37271 0.28074 0.28486 0.50204 0.083894\n",
      "\n",
      "former -0.11333 0.0294 0.73019 -0.66437 0.82147 0.12628 0.18466 -0.41714 -0.66157 0.45328 0.1249 -0.32654 0.36087 0.40208 -0.19117 -0.54911 1.0127 -1.0461 -0.80564 0.36514 0.15841 0.24938 0.34687 0.30408 -0.021616 -0.88272 0.0069813 -1.0256 0.37497 0.41974 0.44849 0.77445 -0.037449 -0.3453 -0.54241 0.40204 0.14671 0.50425 0.38215 -0.14022 -0.84637 -0.58039 1.0166 -0.22128 0.69268 1.0148 0.034793 -0.18384 0.051851 -0.0072091 -0.22334 -0.58012 0.15722 0.66378 0.31309 -2.1431 -0.76643 0.22772 1.1524 0.69464 -0.18987 -0.097813 0.3914 -0.057835 1.0882 -0.36599 1.1016 1.1119 0.052827 0.92158 0.19927 -0.073714 -0.72451 -0.34681 -0.014736 -0.54484 0.17422 0.39317 -0.84435 -0.13122 1.338 -0.05649 0.52051 -0.46969 -0.95412 -0.092813 -0.13584 1.0748 0.53746 -1.3292 0.18121 -0.55782 0.043063 0.99446 -0.44207 -0.53994 -0.51284 0.62304 0.11044 0.027501\n",
      "\n",
      "these -0.60457 0.5075 -0.18923 -0.44618 -0.24887 -0.047248 -0.59028 0.22217 -0.19096 -0.44195 0.44367 -0.202 0.45344 0.24877 0.29037 -0.3038 -0.22941 0.49541 -0.24886 0.3596 0.10215 -0.54331 0.27576 -0.094781 -0.28585 -0.58057 0.2032 -0.6724 -0.15914 -0.053871 0.41261 0.27296 -0.58267 -0.28004 0.24842 0.3177 0.059265 -0.3387 -0.53421 -0.45146 -0.60715 -0.23957 -0.25599 -0.51025 -0.1426 -0.22488 0.34293 -0.16729 -0.36152 -0.34747 0.077976 0.020262 0.076892 0.90408 0.094993 -2.0786 0.65924 -0.20885 1.7871 0.63572 -0.4979 1.6002 0.082439 0.2216 1.3113 -0.086767 0.52703 0.064908 0.6771 -0.60395 -0.23561 -0.46264 0.32467 -0.46109 0.44337 -0.11367 -0.00012672 -0.025889 -1.1496 0.068919 0.8014 0.11559 -0.43814 0.50888 -2.1189 0.074296 -0.12824 -0.83419 -0.29566 -0.039795 -0.73023 -0.057845 -0.29647 -0.20696 -0.78615 -0.34717 -1.2049 -0.6 0.56169 0.44728\n",
      "\n",
      "make -0.33252 0.073082 0.34678 -0.51402 -0.2465 -0.19302 -0.24952 0.21636 0.047513 -0.043 -0.04666 -0.089818 -0.11834 0.38686 -0.0033615 -0.15131 0.3567 0.75659 -0.19621 0.67868 0.27862 -0.037748 0.059066 0.035084 -0.13587 -0.035028 -0.24658 -0.53655 0.37369 -0.45819 -0.47222 0.83221 -0.066929 -0.3669 0.39463 0.64431 0.081114 0.079185 -0.25348 -0.15885 -0.48075 -0.085646 -0.099086 -0.73175 -0.5717 0.0047305 0.081587 -0.41644 -0.20028 -1.1587 -0.10217 0.0078988 -0.073715 0.98189 -0.14205 -2.5327 0.31952 -0.167 1.993 0.047356 -0.40419 0.87967 -0.28156 0.035413 0.89974 -0.19652 0.61577 0.74934 -0.31396 -1.0302 0.12434 -0.58429 -0.089311 -0.62441 0.43783 0.086182 -0.57858 -0.021869 -0.48689 0.055154 0.88025 0.12143 -0.6449 0.19987 -1.6529 -0.092651 0.31107 0.26664 -0.16642 -0.27688 0.26257 -0.1307 -0.28866 -0.88377 -0.75213 -0.5063 -0.081468 -0.46963 0.6778 0.28027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "billion 1.3143 1.1977 0.87891 0.086727 0.70483 -0.70591 0.09403 0.10557 -0.13163 0.3701 1.0045 0.86857 -0.96557 -0.059646 -0.052112 -1.2234 -0.0087372 -0.11916 -0.12142 0.45845 0.61894 0.31431 -0.56601 1.3571 0.032826 -0.30287 0.94117 0.63071 -0.70389 -0.79644 0.37281 0.8155 -0.61868 -0.95969 -0.54034 0.77195 0.448 -0.24677 -0.52844 0.71881 0.55858 -1.269 -0.13988 0.71042 -0.10272 0.30446 -0.73999 -0.71953 -1.1492 -1.462 -0.21551 -0.61601 1.6121 0.46553 -0.75174 -2.3178 -0.25878 -1.1629 2.2763 0.33507 0.69034 -0.27817 -0.73657 0.32455 -0.95897 0.47627 -0.77134 0.41595 0.78333 -0.59911 0.42468 0.058197 -1.1176 0.31961 -0.099809 -0.38049 -0.86129 0.86457 -1.6845 1.0779 1.1674 0.2116 -0.37088 0.26164 -0.92719 0.32965 0.3945 -0.57535 -0.090807 -0.15782 0.28291 0.53087 -0.5677 -1.5692 -0.85263 0.51391 0.1043 0.063291 1.1551 -1.1632\n",
      "\n",
      "work -0.11619 0.45447 -0.69216 0.03458 0.26348 -0.38139 -0.2279 0.37233 -0.20579 0.2902 0.12114 -0.42729 0.55573 -0.094286 -0.49967 -0.29478 0.74109 0.25191 -0.27468 0.23191 0.0038204 0.045252 0.2497 -0.41579 0.31307 -0.58496 -0.32739 -0.66189 0.14909 -0.25771 -0.94858 0.41809 -0.29538 -0.042711 -0.6997 0.57892 -0.069271 -0.039633 -0.0056463 -0.29616 -0.57448 0.1601 -0.10671 0.10096 -0.42956 -0.27785 -0.30017 -0.69537 0.17965 -0.46723 0.12511 -0.029022 -0.15974 1.4217 -0.26224 -2.3719 -0.11917 -0.58262 1.5548 0.42212 0.084633 1.1385 -0.31226 -0.050738 1.0936 -0.001437 0.44641 0.34625 0.43964 -0.38941 0.27082 0.080626 -0.056481 -0.41097 0.5642 -0.15158 -0.14931 0.17199 -0.61495 -0.15822 0.32818 0.49756 -0.32947 0.16222 -2.3063 0.40681 0.27702 -0.37002 -0.80969 -0.76256 0.091109 -0.61473 0.25286 0.05998 -0.21977 0.0072382 -0.33877 -0.54737 0.48822 0.32246\n",
      "\n",
      "our -0.085462 0.11468 0.63305 -0.29949 -0.75123 -0.019007 -0.71863 -0.086716 -0.32063 0.32459 -0.1916 0.099406 0.46356 -0.040958 -0.57825 -0.71853 -0.24164 0.74698 -0.76973 0.27197 -0.082126 -0.05786 -0.034238 -0.28307 0.066904 0.41605 -0.18862 -0.56993 0.36425 -0.14169 0.46529 0.62677 0.17222 -0.11196 -0.21784 0.21154 -0.34794 0.37863 -0.087138 -0.59421 -0.26697 -0.37404 0.74401 0.29701 -0.090312 0.23097 -0.1941 -0.008969 -0.54395 -1.1504 -0.017917 0.24749 -0.56197 1.2864 0.037423 -2.4713 0.99912 -0.022814 2.3491 -0.15643 -0.00023417 1.1817 -0.25566 -0.35204 0.78241 0.51578 0.2521 0.31661 0.38381 -0.51213 0.97154 -0.028643 0.57331 -1.4127 0.91893 0.15043 -0.38029 -0.31698 -1.137 0.29611 0.55418 0.86741 -0.39839 0.60217 -1.7685 -0.17747 -0.32733 -0.19192 -0.33404 -0.30596 -0.20281 0.33782 0.41471 0.17372 -0.74398 -0.38168 -0.35838 -0.25772 0.57399 1.4787\n",
      "\n",
      "home -0.092998 0.16297 0.73724 -0.37971 -0.077342 0.76823 -0.074471 0.2472 -0.29568 0.26877 0.16257 -0.57607 0.14767 0.17896 0.12081 -0.051907 0.43795 0.25028 -0.62697 0.6331 0.43648 0.43814 0.75078 0.42543 0.084075 -0.31261 -0.49792 -0.60304 -0.32523 0.1209 -0.077789 0.26272 0.29894 0.85102 0.13084 0.47982 -0.43993 0.47408 -0.06119 -0.48955 0.3537 -0.50504 0.12695 -0.6797 0.54241 0.13003 0.19291 -0.17411 0.85916 -0.26002 -0.30243 -0.51926 0.10875 0.71831 0.019399 -2.7363 -0.54512 -0.34702 1.6927 0.58821 -0.24435 0.59183 0.13423 -0.16698 0.25298 0.26853 0.63114 0.046757 -0.12598 0.045965 -0.3553 -0.020918 -0.14581 -1.2608 0.16349 0.62281 -0.12171 -0.030637 -0.89189 0.26495 0.50661 0.70529 -0.29213 0.21897 -0.87666 -0.39202 -0.015993 -0.096598 -0.54524 -0.33413 0.17284 -0.29763 0.78264 0.057181 -1.3875 -0.30772 0.36735 0.57373 0.93698 -0.30258\n",
      "\n",
      "school 0.80336 0.51195 -0.58513 -0.16546 -0.65033 0.55772 -0.051099 1.0536 -0.45597 1.0997 -0.19709 -0.69005 0.2634 1.0464 -0.31459 -0.41741 0.7036 0.27429 -0.79007 0.46717 -0.92044 0.041713 0.75599 -0.13162 -0.30481 -0.5513 -0.54436 -1.1833 -0.79556 0.55663 -0.76105 0.8459 0.30398 0.5447 -0.72929 0.11762 -0.70917 0.26656 -0.06753 0.053805 -0.87024 0.19024 -0.59956 -0.46065 0.17753 0.22736 0.40392 0.26203 0.46493 -0.099527 -0.50003 -1.0067 -0.35254 0.27211 -0.39462 -3.0186 0.46466 -0.63553 1.6931 0.078175 -0.049976 0.47524 0.13094 0.098022 -0.0098746 -0.68214 0.49468 0.11997 0.34533 1.0207 -0.26943 -0.195 -0.063351 -0.13269 -0.41522 0.49386 0.44739 0.51464 -0.71938 -1.0296 -0.055796 0.34624 -0.37473 -0.90457 -1.1705 -0.027685 -0.39492 -1.1926 0.38333 0.070544 -0.4583 -0.33669 0.0041694 0.85152 -1.3539 0.016616 -0.078291 -0.12729 0.64313 0.13859\n",
      "\n",
      "party -0.54216 0.19825 0.16977 -0.84205 0.48714 0.65229 0.31895 -0.10348 0.007623 0.16972 -0.039506 -0.083201 0.27059 0.053764 -0.17876 -0.23129 0.37007 -0.73138 -0.16774 0.0011286 0.73868 -0.063627 0.63803 -0.78116 -1.1668 -0.74872 -0.47183 -0.11901 0.95015 -0.32939 0.74637 1.3908 0.66303 -0.20867 0.016905 0.1202 0.79919 0.023339 -0.91689 -0.35891 -1.2443 -0.24395 1.4598 -0.36321 -0.23217 -1.0536 -0.16355 -0.10749 -0.21351 -0.13065 0.14598 -0.4227 0.53124 1.0251 -0.10535 -2.241 0.18753 1.0068 1.6827 0.77399 -0.44067 -0.26181 -0.2955 0.93169 1.2316 0.84726 0.11111 0.085456 0.80882 -0.11752 0.37039 -0.24129 -1.13 -0.95133 -0.4925 0.22237 -1.0336 -0.46899 -1.0215 0.04666 -0.56328 -0.035294 0.47064 -0.36057 -0.9215 -0.52212 -0.5894 1.044 0.37005 -0.58997 0.56092 -0.48624 -0.23585 1.109 -0.71644 -0.29137 0.26762 -0.36028 1.0511 -0.028692\n",
      "\n",
      "house -0.18867 -0.040943 0.58878 0.11062 0.14236 0.4885 -0.31832 0.53819 -0.018549 0.029687 0.30299 -0.16522 -0.18896 0.5148 -0.79405 0.26409 0.027747 0.041163 -0.49378 -0.14263 0.29017 -0.25369 0.70559 -1.0501 -0.49344 -0.37148 -0.85796 -0.55158 -0.60251 -0.0099676 0.8725 0.12149 0.551 0.49924 -0.3088 1.1067 -0.15494 -0.29923 0.91149 0.19859 -0.73946 -1.0182 0.37208 -0.10043 0.13537 -0.52687 -0.60437 -0.15906 0.49283 -0.61386 0.046815 -0.88806 0.60229 0.72199 -0.4316 -3.0706 -0.11233 -0.45713 0.95737 0.59174 -0.17124 0.65746 0.44741 0.6101 1.0216 -0.2458 0.90191 0.78319 0.28272 -0.4539 0.16309 -0.0078932 -0.27714 -0.87249 -0.19716 -0.076285 -0.28422 -0.089584 -1.3132 0.16372 -0.25441 -0.076529 0.44458 -0.17525 -0.74084 -0.25415 0.52886 -0.46958 0.16487 -0.57443 0.47239 -0.52798 0.65184 0.803 -0.93156 -0.055967 0.26932 0.16221 1.1238 -0.4168\n",
      "\n",
      "old 0.17187 0.62628 0.4375 -0.61145 -0.3596 0.4578 0.064163 0.032826 -0.34442 0.89331 -0.30715 0.72752 0.77566 0.12548 0.63107 -0.25439 0.95043 -0.62161 -0.48553 0.4775 0.043044 0.78751 0.6328 0.49497 1.0595 -0.70889 -0.47843 -1.504 -0.50092 0.74445 -0.3723 0.20476 0.40886 -0.18248 -0.34864 -0.13396 -0.26771 -0.24591 0.76293 -0.20513 -0.21183 -0.73675 0.79743 -0.56995 0.32439 0.64365 0.1075 -0.041559 0.36386 0.056682 -0.042991 -0.70831 0.4609 0.90684 0.30985 -2.5737 -0.97284 0.3372 1.3211 0.61935 0.37118 0.68077 0.030024 0.16423 0.26558 0.44665 1.0901 0.40632 -0.99192 -0.27947 0.38068 0.30683 -0.086059 -0.38399 0.044479 0.16529 0.18359 -0.11766 -1.1888 -0.84626 0.83008 -0.24773 0.23238 0.23131 -1.3542 0.074509 -0.67224 -0.01996 -0.17731 -0.31848 -0.084337 -0.19923 0.1824 0.24455 -1.0465 -0.61433 -0.39154 -0.58809 -0.1414 0.19377\n",
      "\n",
      "later 0.027587 -0.475 -0.22745 0.033388 0.24517 0.15746 0.47597 0.22717 0.064186 -0.024291 0.44207 0.090494 0.24425 0.33907 0.38285 -0.5376 0.46815 -0.063625 -0.70666 -0.21428 0.234 -0.50604 0.17174 0.43377 0.22311 -0.23438 -0.21264 -0.39856 0.55372 0.46415 -0.14085 0.59065 -0.69252 0.26242 -0.23958 0.35127 -0.19809 -0.072761 -0.29886 -0.28567 -0.91971 -0.35141 0.2003 -0.060185 0.1837 -0.017188 0.057039 -0.46265 0.7954 -0.41627 -0.067855 -0.057482 0.61104 1.0324 -0.19426 -2.594 -0.95114 -0.28406 1.0061 1.0809 -0.11912 1.064 -0.24482 0.43649 0.77034 -0.59828 0.014552 0.48963 0.0028154 0.36479 -0.26913 0.057471 0.080571 -0.55504 -0.13466 0.13469 -0.1548 0.021221 -1.0397 -0.063057 0.26224 -0.091979 -0.15155 0.095526 -1.3116 -0.35033 0.053767 -0.58703 -0.24133 -0.21079 -0.049315 -0.64725 -0.24883 0.81014 -0.86515 0.081943 -0.25318 0.19385 0.10123 -0.35925\n",
      "\n",
      "get 0.14433 0.43951 0.58324 -0.74477 -0.49797 0.086928 -0.29798 0.39964 0.14083 -0.36578 0.33322 0.47181 0.1996 0.18278 0.17176 -0.34297 0.04252 0.39309 -0.66272 0.62738 0.22845 0.44657 0.11174 -0.41396 0.079795 0.078823 -0.26135 -0.80966 0.52807 -0.47327 0.055901 0.85753 0.19722 0.30782 0.39039 0.31804 -0.53604 0.1835 0.081918 0.27753 -0.25792 -0.37158 -0.2215 -1.0916 -0.52179 0.12665 -0.48591 -0.26716 0.37119 -1.0525 -0.30202 -0.16279 -0.32528 0.95493 0.14915 -2.4934 0.27505 0.16308 1.8972 0.21172 0.10776 1.1318 -0.85962 0.060199 0.83713 0.030006 0.59344 0.42821 -0.13722 -0.49128 0.0043155 -0.51829 -0.059878 -0.58476 0.062621 0.31269 -0.52613 -0.35209 -0.49959 -0.13117 0.62349 -0.092936 -0.92224 -0.32487 -1.8286 -0.1284 0.50507 -0.034628 -0.79806 -0.49739 -0.096325 -0.17695 -0.036351 -0.75977 -0.49938 -0.20278 0.34232 0.50134 0.49536 0.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "another -0.13669 0.16266 0.32851 -0.23838 0.37632 0.512 0.43825 0.2659 0.10699 0.10075 0.099575 0.23082 0.030345 -0.33396 0.38298 -0.24366 -0.011523 -0.35039 -0.20009 0.46843 0.79308 -0.30159 0.032797 0.28515 0.5024 -0.27093 -0.42409 -0.41382 0.094871 0.072463 -0.55992 0.4625 0.27367 -0.11924 -0.055437 -0.053994 -0.22305 0.19366 0.42554 0.12814 -0.30268 0.11386 0.66691 -0.56826 0.49106 0.35975 0.41939 0.055777 -0.52086 -0.44886 0.058267 -0.082595 0.082818 1.148 -0.44502 -3.0693 -0.38092 -0.22423 1.6572 0.69771 0.057998 0.87766 -0.032617 0.24173 0.72651 -0.24736 0.50189 0.58423 -0.22143 0.18851 -0.46677 -0.022507 -0.21069 -0.086704 0.31232 0.27984 -0.21408 -0.41711 -1.1358 0.018686 0.89265 -0.00067525 -0.26069 -0.14054 -1.1586 -0.32087 0.4694 0.19181 0.1376 -0.55373 0.42719 0.14677 -0.18532 -0.1694 -0.27725 -0.29067 0.17023 -0.1319 0.48349 0.011721\n",
      "\n",
      "tuesday 0.035314 -0.45705 0.435 0.042954 0.16798 -0.082904 -0.00059971 0.77181 -0.57037 0.042372 -0.4633 0.11997 0.017448 -0.47586 -0.0806 -0.20982 -0.13917 -0.7315 -0.79553 -0.37344 0.65933 -0.26635 -0.027317 0.72379 0.21664 -0.11534 0.0057595 0.31991 0.021105 0.50361 0.033175 -0.44326 -0.59224 0.54382 -0.080331 0.077691 0.002612 -0.003496 -0.24959 0.27452 -0.8054 -0.61258 0.36596 -0.019922 -0.14684 -0.32406 -0.24486 -0.37433 0.317 -1.1081 0.55082 -0.56022 0.62905 1.2137 -0.70667 -2.5238 -0.91836 -0.072349 2.1422 1.1889 -0.71701 0.25923 -0.10795 -0.31674 -0.41594 0.0050092 -0.15001 1.0506 -0.22708 -0.098957 -0.34174 -0.46806 -0.63947 -0.80153 -0.070892 0.57034 -0.163 0.2886 -1.4557 0.21107 0.62371 -0.39933 -0.28478 -0.47162 -0.72213 -0.82871 0.095333 0.87038 -0.31199 -0.16187 0.83462 -0.097157 -0.40481 0.92648 -0.92007 0.85595 0.15799 0.12178 0.80515 -0.057494\n",
      "\n",
      "news -0.66842 -0.41713 0.42473 -0.9329 -0.36823 -0.26647 -0.10715 0.093359 0.25288 -0.42413 0.67356 0.092664 0.43201 -0.25714 -0.11222 0.059157 0.33147 -1.2479 -0.35577 -0.21875 -0.22346 0.10209 -0.4843 0.7824 0.3118 -0.083924 0.56489 0.98637 -0.12308 0.92539 0.28811 0.4003 -0.64225 0.12647 -0.27778 0.045568 -0.18598 -0.15247 -0.42322 0.29807 -0.68476 -0.11121 -1.1391 0.072205 -0.038877 -0.54775 -0.0032873 -0.85587 0.3267 -0.79493 0.33434 0.29464 0.44074 0.69114 -0.10615 -2.5303 -0.5923 0.4648 2.2093 0.77166 -0.60216 0.46264 -0.70728 -1.1414 0.40916 -0.31745 0.41431 0.49908 0.49434 1.0044 -0.37273 -0.16246 0.23608 -0.71456 0.53312 0.4593 0.19464 0.23521 -1.5447 -0.56067 0.37415 -0.54415 0.40648 -0.19946 -0.83036 -1.1934 -0.53713 0.43504 0.072245 -0.85625 1.2625 -0.048436 0.1157 -0.013286 -1.1952 0.3724 0.63674 -0.51084 0.99947 1.01\n",
      "\n",
      "long -0.24149 0.43733 -0.32623 -0.18897 -0.29305 -0.28223 -0.37429 0.47172 -0.61045 -0.17059 -0.05528 0.74618 0.61522 0.29321 0.23069 -0.40869 0.24888 -0.48797 0.00048047 -0.54519 0.52164 0.73132 -0.0010567 0.10601 0.84546 0.49301 -0.92694 -0.56454 -0.073725 0.087184 -0.17091 0.29569 0.41866 -0.35302 0.33314 0.23985 0.51223 0.31759 0.36975 0.52439 -0.15201 -0.5523 0.41703 -0.13236 0.095632 -0.4792 0.82726 -0.41113 0.24488 -0.11884 0.25559 -0.55004 0.48753 1.2109 0.41641 -2.8652 0.26831 -0.094585 1.8959 0.43377 -0.17049 0.82421 0.10373 -0.18124 0.63302 -0.02878 0.66535 0.12374 -0.08893 -0.18875 0.26731 0.13841 -0.1126 -0.3464 -0.044674 -0.45838 -0.66339 -0.018257 -0.89461 -0.27158 0.70234 -0.077111 -0.35251 0.1509 -1.7293 0.20593 -0.23115 0.079508 -0.092586 -0.11143 -0.18736 0.32542 -0.38972 -0.37078 -0.65679 -0.082912 -0.51918 -0.34876 0.23791 0.21206\n",
      "\n",
      "five 0.062482 0.47966 0.11041 -0.32933 0.48119 0.45071 0.53119 0.86901 -0.97004 0.099613 0.74502 0.11981 0.10476 0.60236 0.20251 -0.19277 -0.19874 -0.055986 -0.73683 0.45396 0.97334 0.022636 -0.095826 0.34307 0.73047 -0.87593 -0.095241 -0.38745 -0.23327 0.30393 0.15926 -0.018173 -0.19423 -0.34158 0.14226 0.080426 -0.27403 0.12203 -0.49623 0.55136 -0.59616 -0.1993 0.76316 -0.30328 0.49749 0.20581 -0.15059 -0.77498 -0.25081 -0.13317 -0.69645 -0.78776 -0.23693 1.2415 -0.073454 -2.3672 -0.096282 -0.13515 1.9153 1.3775 -0.53319 0.81572 0.036566 0.36214 0.22786 0.40366 0.21622 0.48993 0.094843 0.023101 -0.2485 0.24044 -0.10915 -0.06291 0.087199 0.42677 -0.58573 -0.47301 -0.96921 -0.089705 1.0737 0.22502 -0.28669 0.25863 -1.2812 0.0069472 0.12606 -0.23873 0.14965 0.25225 -0.067922 0.29653 -0.47425 -0.40843 -1.3266 0.22926 -0.57478 0.20152 0.43435 -0.28099\n",
      "\n",
      "called -0.69321 -0.4055 -0.097805 -0.24244 0.17236 0.08163 -0.047831 0.44821 -0.24222 -0.065717 -0.53115 -0.40135 0.28277 0.20314 0.30505 0.14683 0.72686 -0.12494 -0.42272 0.016589 0.2993 -0.23278 -0.34599 -0.28389 0.21532 0.072135 -0.080068 -0.097477 0.039596 0.54599 0.31484 0.75374 -0.12927 0.39599 0.5641 0.14666 0.42306 0.27284 0.1955 -0.12319 -0.65589 0.27151 0.23175 -0.35651 -0.0093057 -0.095994 -0.30947 -0.0073496 -0.078173 0.085239 -0.18881 0.14317 0.46056 0.70236 -1.0063 -2.6236 -0.20536 -0.08254 1.3812 0.91392 -0.03478 0.22303 -0.038465 -0.35531 0.97832 -0.3384 0.42215 0.504 0.24439 -0.40787 0.10728 -0.2331 -0.48326 -0.39685 -0.10597 -0.047702 0.20389 0.28397 -1.0879 0.045633 0.61092 -0.44083 -0.58399 0.007972 -1.478 0.15884 0.17551 -0.25211 0.096376 -0.50949 0.11716 -0.3603 -0.4453 0.65606 -0.046028 -0.45997 -0.47293 -0.48659 0.58224 -0.033299\n",
      "\n",
      "1 -0.32219 0.64632 0.027312 0.36579 -0.0026966 0.17245 1.1092 0.3545 -0.03288 -0.34787 1.0797 -0.20107 -0.67811 0.49211 0.57646 -0.33956 0.84992 0.13798 -0.24002 0.69785 0.52505 -0.12352 0.4468 1.0018 0.4152 0.34613 0.71314 0.10015 0.33717 -0.24967 -0.24961 0.8511 -0.038842 -0.20761 -0.66961 0.26736 -0.0071073 0.96438 -0.092126 0.0011468 0.80952 -1.574 0.27909 -0.68659 0.57055 -0.53396 -0.32982 -0.32648 -0.024373 -0.55105 -0.11989 -0.52269 -0.38958 1.5032 -1.1357 -2.7247 -0.32485 -0.66962 1.6993 0.43917 -0.34609 0.00063696 -1.0189 -0.10181 0.94105 0.21589 -0.84112 1.0778 0.54813 0.34729 -0.58702 -0.10148 -0.12418 0.79781 0.38066 -0.028399 0.063585 -0.053829 -0.73777 -0.20447 0.44023 -0.66932 -0.89774 0.26896 -0.32809 -0.1081 -0.25484 0.17018 0.50935 0.34461 -0.75759 0.77057 -0.63238 0.44622 -1.0738 -0.27448 0.28649 -0.18124 0.50172 -0.74622\n",
      "\n",
      "wednesday 0.091119 -0.39876 0.30191 0.040105 0.098664 -0.060935 -0.056491 0.84242 -0.59603 -0.059351 -0.38578 0.070836 -0.10772 -0.53592 -0.12795 -0.13862 -0.022811 -0.83802 -0.80685 -0.43434 0.74192 -0.29695 -0.056929 0.77675 0.24255 -0.12745 -0.03767 0.41364 -0.076888 0.50972 -0.099697 -0.47193 -0.6433 0.50027 -0.050218 0.070825 -0.028711 0.009701 -0.26319 0.27165 -0.7611 -0.69263 0.34349 -0.030629 -0.085926 -0.28275 -0.22707 -0.35296 0.39215 -1.1626 0.58628 -0.51249 0.55643 1.214 -0.71248 -2.5422 -0.98945 -0.091106 2.0604 1.2741 -0.62214 0.23683 -0.19328 -0.36515 -0.54457 -0.052431 -0.15433 1.0939 -0.2784 -0.13638 -0.32656 -0.52802 -0.55332 -0.89387 -0.027115 0.53956 -0.05257 0.29511 -1.5623 0.24835 0.70678 -0.28629 -0.34455 -0.50079 -0.6887 -0.86455 0.070523 0.95418 -0.36388 -0.052774 0.81575 -0.12966 -0.31916 0.90142 -0.9412 0.8588 0.18627 0.070988 0.76569 -0.12167\n",
      "\n",
      "military -0.61705 0.24556 -0.12763 0.44811 -0.31334 -0.10556 -0.57768 -0.13341 -0.1077 1.3746 0.048171 -0.12028 0.71118 0.56405 -0.12837 0.050052 0.75063 -0.66891 -0.53914 -0.17633 0.90628 -0.42358 0.47614 -0.23071 -0.66152 0.014561 -0.44352 -0.31137 0.79809 -0.048347 0.11616 -0.1643 -0.60885 -0.52113 0.17303 0.19728 -0.10716 0.021075 -0.18415 0.24534 -0.63442 0.052319 0.71578 0.22589 0.558 -0.45769 0.09837 -0.086677 -0.40387 -0.55339 0.41761 -0.17984 -0.090911 1.6564 0.26582 -1.686 0.18443 -0.89305 2.4937 0.80885 -0.30559 0.34702 0.3227 0.26455 0.48152 -0.40948 -0.47729 -0.040116 -0.42792 1.9152 0.63685 -1.1247 -0.45709 -0.7602 0.09095 0.2994 0.11447 0.9991 -1.2072 0.62891 1.345 -0.10297 0.47065 0.13026 -1.2734 0.23595 0.40469 0.0049411 0.28565 -0.96233 0.30662 -0.39246 -0.34099 0.23123 -0.76097 0.068257 -0.075085 -0.61296 0.0026871 0.64299\n",
      "\n",
      "way 0.15542 0.22266 0.66749 -0.10515 -0.13068 0.37393 -0.17058 0.009149 0.076113 -0.11288 -0.14282 0.044725 0.23126 -0.24242 0.29379 -0.18357 0.1357 0.35516 -0.58844 0.23293 0.58574 0.33664 0.34024 -0.22089 0.12549 0.25051 -0.41115 -0.53115 0.069639 -0.37244 -0.46644 0.69798 0.12978 -0.20145 0.13568 0.076107 -0.50561 -0.053813 0.044468 -0.26587 -0.55276 -0.30575 -0.054521 -0.12123 -0.74521 0.44949 0.26413 -0.26995 0.0029227 -0.69914 -0.102 0.04372 0.29726 1.2991 -0.2306 -2.7697 0.1016 0.25055 1.8448 0.13651 -0.39607 0.70957 -0.14062 0.1244 0.82492 0.029879 0.31792 0.40967 -0.18953 -0.58943 0.093985 -0.55381 0.041654 -0.93166 0.35255 -0.34163 -0.078994 -0.12563 -0.37184 0.026071 0.49271 0.47599 -0.48877 0.068624 -1.6429 0.14407 0.092517 -0.16273 -0.63504 -0.51367 -0.29237 -0.319 -0.25062 0.085763 -0.50163 -0.14954 -0.078231 -0.25213 0.38793 0.83459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "used -0.4713 0.57094 -0.50343 -0.16902 0.207 0.20779 0.087041 0.049987 -0.14483 0.26928 -0.19927 -0.6827 0.27491 0.82178 0.31601 -0.11952 1.043 0.24766 -0.34924 -0.30117 0.40915 -0.34067 0.3716 0.093613 -0.13949 -0.5461 -0.3396 -0.2273 -0.05824 0.11287 0.15304 0.86929 -0.97688 0.22133 0.6068 0.56773 0.0032481 0.19031 0.48249 -0.10228 -0.14569 -0.36808 -0.25728 -0.15778 -0.091743 0.16179 0.2565 -0.57449 -0.22681 -0.89427 -0.22239 0.52927 0.76028 1.3736 -0.17043 -2.0083 -0.51106 -0.26381 1.5273 0.13254 -0.25583 0.92246 0.22393 0.90556 1.0088 -0.16988 0.74343 -0.35974 -0.077833 -0.35517 -0.79085 -0.44397 0.47479 0.096046 0.0029278 -0.30727 0.28722 0.51917 -0.94567 0.35826 0.82026 -0.57482 -1.0386 -0.14005 -1.8042 0.51904 0.6171 -0.1785 -0.18061 0.057552 -0.49046 -0.17216 0.086454 -0.073225 -0.021868 -0.60123 -0.72079 -0.55905 0.7363 -0.069655\n",
      "\n",
      "much -0.3384 0.6032 0.61412 -0.05686 -0.37309 -0.061981 -0.40583 -0.11304 0.018956 -0.086392 -0.19767 0.30401 -0.17332 -0.58002 0.25126 -0.83477 -0.34257 0.18994 0.093403 0.68556 0.41599 0.44229 -0.080242 -0.29032 0.026182 -0.066266 -0.29265 -0.61917 -0.003058 -0.30887 0.046545 0.29265 -0.11817 -0.099073 -0.12231 0.44359 0.037724 0.34108 -0.31529 0.22834 -0.23893 -0.53311 0.09226 -0.26013 -0.345 -0.079983 0.80432 -0.34395 0.42991 -1.2454 0.22909 -0.031585 0.14377 1.1435 -0.29506 -2.7798 -0.083334 -0.50714 1.5535 0.25629 0.43922 1.3835 -0.65789 0.23549 0.66741 -0.084843 0.83077 0.18837 0.49648 0.076237 0.082209 -0.38204 0.12748 0.10687 0.19172 -0.092941 -0.31443 -0.35409 -1.0228 0.452 0.53832 0.089991 -0.67326 0.18871 -1.5292 0.083864 0.016029 -0.11863 -0.40369 -0.49117 0.36135 -0.25832 -0.24702 -0.18608 -0.48393 -0.48878 -0.30166 -0.37964 0.59431 0.61286\n",
      "\n",
      "next -0.26424 0.22626 0.82339 -0.20665 0.050187 -0.37812 0.22721 0.91286 -0.24134 -0.25365 0.12431 0.09303 -0.082057 -0.04897 0.14869 -0.35864 0.15002 0.48099 -0.25361 -0.26409 0.14916 -0.32134 0.010386 0.56583 0.18566 -0.34025 -0.2106 -0.54901 0.25186 0.15121 -0.71719 0.13736 0.44788 -0.25483 -0.062754 0.62234 -0.4021 -0.28212 -0.056098 -0.22097 -0.35708 -0.27459 0.50315 -0.13496 -0.14528 -0.012431 0.0075285 -0.60886 -0.011221 -0.81087 -0.14036 -0.58278 0.057027 1.0757 -0.48716 -3.0169 -0.56784 -0.1741 1.8463 0.62222 -0.58392 0.87285 -0.32553 -0.0078565 0.56621 0.34026 -0.24294 0.59833 -0.064817 -0.34101 0.47312 -0.48022 -0.45578 -0.66012 -0.071061 0.070737 -0.30658 -0.15749 -0.51771 -0.086326 0.66913 -0.15034 -0.34442 0.12498 -0.90054 -0.38807 0.14146 -0.098806 0.19738 -0.30631 0.18435 0.44287 0.12919 0.3176 -0.76439 0.2722 0.52238 0.34128 0.75776 0.10251\n",
      "\n",
      "monday -0.028811 -0.4672 0.24218 0.04212 0.050292 -0.028014 0.058003 0.85249 -0.55768 -0.028948 -0.39525 0.19569 0.089136 -0.52259 -0.056861 -0.32896 -0.12993 -0.7726 -0.8332 -0.37651 0.61226 -0.19536 0.016296 0.73729 0.264 -0.084888 0.056924 0.36697 0.11315 0.57599 -0.1021 -0.48387 -0.58075 0.47146 -0.13877 0.097183 0.012507 0.013841 -0.19185 0.18986 -0.84485 -0.64666 0.39325 0.013237 -0.20383 -0.20495 -0.30713 -0.32096 0.35561 -1.1269 0.51245 -0.5404 0.68098 1.1157 -0.72849 -2.4402 -0.89149 0.017431 2.205 1.2646 -0.67757 0.29867 -0.14236 -0.35604 -0.50412 -0.041173 -0.10659 0.91921 -0.18711 -0.062278 -0.40201 -0.49958 -0.58662 -0.82273 -0.048633 0.5459 -0.095086 0.39941 -1.4863 0.24823 0.76438 -0.30164 -0.27947 -0.46313 -0.70316 -0.76921 0.10392 0.91457 -0.31586 -0.040817 0.85992 -0.049918 -0.38531 0.86019 -0.93004 0.87055 0.25251 0.0059353 0.86433 0.078546\n",
      "\n",
      "thursday -0.023774 -0.5119 0.28843 0.11221 0.15057 -0.040624 0.024586 0.75241 -0.65522 -0.0034481 -0.38342 0.25094 -0.047249 -0.49308 -0.090263 -0.2084 -0.1189 -0.74925 -0.89619 -0.38798 0.70435 -0.14828 1.2347e-05 0.73792 0.29655 -0.079486 -0.013683 0.23614 0.0060694 0.39679 0.01148 -0.43982 -0.53655 0.49361 -0.060609 0.026976 -0.0039479 -0.0051518 -0.17091 0.2801 -0.89532 -0.61035 0.34637 0.05939 -0.07562 -0.23067 -0.27209 -0.3378 0.2229 -1.1564 0.46222 -0.47542 0.55939 1.167 -0.72541 -2.4762 -0.86908 0.049408 2.1593 1.2303 -0.65804 0.19894 -0.15421 -0.34573 -0.42764 -0.080179 -0.12945 1.04 -0.22065 -0.035341 -0.29893 -0.49912 -0.62703 -0.83823 -0.14057 0.62377 -0.024653 0.33003 -1.4614 0.15681 0.66904 -0.38153 -0.25407 -0.51908 -0.81074 -0.83144 0.17202 0.88758 -0.35428 -0.032168 0.84239 -0.16371 -0.26639 0.81396 -0.85984 0.82958 0.18569 0.058637 0.77887 -0.0093494\n",
      "\n",
      "friday 0.018958 -0.38485 0.35845 0.10313 0.065005 0.023816 0.05362 0.75174 -0.70126 -0.055484 -0.31195 0.21666 -0.062754 -0.40727 -0.13177 -0.17486 -0.20604 -0.78592 -0.89493 -0.33548 0.79682 -0.15986 -0.037004 0.75061 0.3101 -0.051387 0.068224 0.27047 0.099651 0.57319 -0.16354 -0.61466 -0.6033 0.53523 -0.0094301 0.0123 -0.016172 0.027472 -0.19174 0.22068 -0.78779 -0.55967 0.35618 -0.026865 -0.114 -0.30275 -0.26952 -0.39205 0.37721 -1.2669 0.43598 -0.47884 0.57228 1.1183 -0.80669 -2.4601 -0.92652 0.15528 2.2168 1.1965 -0.6726 0.20014 -0.19388 -0.29091 -0.5532 -0.094787 -0.064595 0.98719 -0.13238 -0.19128 -0.39824 -0.46292 -0.50552 -0.72638 -0.016621 0.67503 0.024609 0.31312 -1.3448 0.16547 0.63869 -0.3549 -0.3673 -0.43671 -0.7094 -0.80815 0.040289 0.85979 -0.24156 0.059653 0.7471 -0.13874 -0.28174 0.87043 -0.97651 0.77438 0.17787 0.11834 0.75161 0.13906\n",
      "\n",
      "game 0.062659 0.026003 1.122 -0.84123 -0.56907 0.86677 0.99235 -0.19032 -0.74226 -0.29547 0.037746 -0.71756 -0.44891 -0.65801 0.17469 0.50347 0.21268 1.0438 -0.60034 0.48838 0.16955 -0.070132 -0.070228 0.50193 1.3543 -0.27193 -0.16449 0.37399 -0.14291 0.19646 -0.80925 0.86245 -0.2118 -0.074321 0.38522 -0.34137 -1.2694 0.59967 -0.84746 -0.68818 0.68874 -0.19769 0.2064 -0.20387 -0.33651 0.093972 0.34192 -0.68139 0.61451 -0.88127 -0.13401 -0.088989 -0.098916 0.73405 0.33496 -3.1068 -0.22643 -0.21568 1.3375 1.5472 -1.2343 0.42964 -0.78932 -0.027462 0.34738 0.17338 0.24975 -0.010451 -0.71149 0.55568 -0.20869 0.074445 0.29833 -0.029668 -0.01595 0.46368 -0.80873 0.11564 -0.14607 0.73785 0.33548 0.18176 -1.2688 0.21146 -1.5877 0.10299 -0.55312 -0.12812 -0.035095 0.12023 -0.40742 0.21107 -0.4187 0.12999 -0.16549 -0.55023 -0.61101 0.46126 0.88781 -0.031292\n",
      "\n",
      "here -0.43061 0.28297 0.80489 0.029872 -0.041104 0.10521 -0.32806 0.63344 -0.24938 -0.35219 -0.17071 0.12919 0.42364 -0.074508 -0.079763 -0.11251 -0.041759 0.13646 -1.0031 -0.21067 0.57931 0.62496 0.19012 0.0040659 0.10989 -0.24882 0.14462 -0.29259 0.41278 -0.1431 -0.8088 0.077435 0.14726 0.0051221 0.28265 0.63608 -0.61334 0.69082 0.05134 -0.6356 -0.57578 -0.19259 0.10841 -0.71079 -0.10451 -0.69632 0.11582 -0.32466 -0.2075 -1.104 0.31189 -0.59473 0.46699 1.3131 -0.37076 -2.4242 -0.73268 0.1682 1.515 0.31627 -0.86261 1.1005 -0.33838 -0.48014 0.22646 0.23478 0.53868 0.67826 0.13785 -0.33429 0.076811 -0.54158 -0.43622 -0.82307 0.33884 0.39302 0.072379 -0.059034 -0.81964 0.12388 0.49509 -0.2499 -0.43014 -0.11875 -0.86847 -0.025802 -0.58743 -0.082977 -0.62785 -0.42915 0.32 -0.20588 0.029217 0.49212 -0.56173 0.21081 -0.28291 -0.48895 0.50908 0.60909\n",
      "\n",
      "? 0.16382 0.60464 1.0789 -1.2731 -0.77594 0.3997 0.38175 -0.17588 0.65858 -0.48456 0.39048 -0.045621 -0.025202 0.33532 0.154 -0.48567 -0.24964 0.67237 -0.77874 0.7019 0.034307 0.40121 -0.33265 -0.41518 0.49524 0.80839 -0.48684 -1.035 0.13576 -0.53717 0.28938 0.69434 0.44361 -0.26475 -0.060746 0.56873 -0.34261 0.19994 0.34199 -0.8282 -0.033282 -0.21024 -0.38447 -0.63159 -0.86065 -0.57572 0.028209 -0.55615 -0.48033 -0.98843 0.12551 0.024564 -0.1348 1.1944 -0.37943 -2.2709 0.41551 0.47853 1.3684 0.34882 -0.4189 1.3103 -0.25537 -0.62531 1.0669 -0.010762 1.0244 0.14627 -0.052959 0.064613 0.28346 -0.16252 -0.23701 -0.31666 0.061133 -0.011744 0.29962 -0.084545 -0.53406 -0.21531 0.71514 -0.5057 -0.42413 -0.27488 -1.1686 -0.01078 -0.2758 -0.42226 -0.2112 -0.39489 0.09993 -0.22539 0.5855 0.082767 -0.60131 0.15512 0.21606 -0.31399 0.18437 0.3624\n",
      "\n",
      "should -0.044737 0.47515 0.26053 -0.39256 -0.29304 0.0071112 -0.44643 0.53438 0.19748 -0.23917 -0.27637 0.32076 0.7137 0.089464 -0.29348 -0.62618 0.20319 0.65568 -1.2966 -0.02145 0.075899 -0.17694 0.093996 -0.089708 -0.35753 0.12062 -0.17008 -0.61405 0.027099 -0.29515 0.10724 0.82951 -0.15541 0.31132 0.50066 0.5156 0.2626 0.18109 -0.33884 -0.38865 -0.7261 -0.41048 0.12704 -0.67029 -0.30651 -0.27046 0.25184 -0.38691 -0.2838 -1.4529 0.42155 -0.20488 -0.26328 1.3288 -0.12419 -1.978 0.16039 -0.071989 1.7647 0.56619 -0.14267 0.56513 -0.72975 -0.13564 1.2719 0.38483 0.11503 0.82408 -0.26384 -0.73986 0.39614 -0.66244 0.061608 -0.36869 0.065238 -0.22564 -0.42271 0.26272 -0.63686 -0.067737 0.5745 -0.36493 -0.61019 -0.13889 -1.9403 -0.28439 0.37524 0.16418 -0.024866 -0.55427 -0.58489 -0.49175 -0.27284 -0.39679 -0.40491 0.35964 0.37232 -0.74777 0.37442 0.32655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "take -0.27064 0.0051896 0.1497 -0.098242 -0.34941 0.053679 -0.49698 0.65251 -0.34078 -0.23466 0.091924 0.4328 -0.05257 0.25661 -0.073174 -0.31834 0.24386 0.52261 -0.64237 0.3446 0.67449 -0.41091 -0.068067 0.11036 -0.31174 -0.1838 -0.32548 -0.56073 0.46353 -0.38417 -0.66699 0.51162 -0.19582 -0.16548 -0.11617 0.40172 -0.27041 0.12839 -0.24684 -0.086713 -0.51182 -0.11955 0.14814 -0.85205 -0.42312 0.27046 -0.19395 -0.1686 -0.022328 -0.79142 -0.13786 0.084995 -0.19315 1.2555 0.0041198 -2.7418 0.083024 -0.20155 1.8789 0.079497 -0.13951 0.83795 -0.28992 0.035695 0.81729 0.25042 0.06956 0.61749 -0.33027 -0.49086 -0.16137 -0.88796 -0.36941 -0.63618 0.11441 0.087835 -0.7473 -0.1224 -0.50284 -0.28315 0.78754 -0.41615 -0.58013 0.0071536 -1.3391 -0.21096 0.51283 0.4465 -0.0473 -0.47531 -0.101 -0.21284 0.26688 -0.43676 -0.5557 -0.0011168 -0.0016866 -0.23098 0.54587 0.49992\n",
      "\n",
      "very -0.84136 0.30985 0.05817 -0.1282 -0.57563 -0.090958 -0.14138 0.2938 -0.1028 -0.32226 -0.14369 -0.15385 0.27397 -0.41289 -0.21281 -0.41444 -0.59205 -0.15562 0.072174 0.57273 0.3717 0.2452 0.18771 -0.33327 0.046375 0.18923 -0.22685 -0.60366 -0.1936 0.45761 -0.47072 0.6637 0.06445 -0.26608 0.81504 0.41368 -0.043518 0.22036 -0.11212 -0.34958 -0.10819 -0.076099 0.38132 -0.3779 -0.21578 -0.37756 0.9711 0.44653 -0.12199 -0.91152 0.37188 -0.44775 0.3636 0.98717 -0.39104 -2.8503 0.37309 0.077979 1.455 0.12927 -0.030492 1.4656 -0.76003 -0.09991 0.9841 -0.33504 0.79977 0.42416 0.45339 -0.28529 0.13677 -0.46309 0.42411 -0.34058 1.0576 -0.07047 -0.50871 -0.29394 -0.48933 -0.18692 0.61546 0.21084 -0.62536 0.17572 -1.6444 -0.22857 0.0088218 -0.25721 -0.70053 -0.75433 0.010457 -0.026041 -0.16825 0.28899 -0.25432 0.12165 -0.96967 -0.74566 -0.11064 0.46134\n",
      "\n",
      "my 0.080273 -0.10861 0.72067 -0.45136 -0.7496 0.63782 -0.25705 0.41606 -0.054465 0.35563 0.35864 0.54004 0.4912 0.25711 -0.21468 -0.42837 -0.42318 0.3872 -0.35692 0.40116 -0.19851 0.43449 -0.36478 0.07172 0.53324 0.84556 -0.6754 -1.2527 0.83759 -0.15925 0.37801 0.94539 0.83069 0.19429 -0.58449 0.5828 -0.62559 0.4904 0.43273 -0.54246 0.1045 -0.16255 0.98995 -0.74219 -0.59784 0.10189 -0.3357 -0.39094 0.15132 -1.3533 -0.11264 0.14345 0.038112 1.1167 -0.23078 -2.6394 0.66849 0.48452 1.8796 0.080274 0.73728 1.8058 -0.51933 0.0040521 0.76986 0.36875 0.81137 0.16941 -0.1192 -0.26496 0.22689 0.76942 0.85197 -0.97773 0.23177 0.88144 -0.2709 -0.39914 -0.57188 0.075573 0.18093 0.59041 -0.1343 -0.12058 -1.8157 -0.355 -0.31721 -0.27044 -0.67226 -0.040986 -0.4433 0.35649 1.0247 0.49685 -0.51696 -0.49276 -0.33402 -0.34842 0.31466 1.0087\n",
      "\n",
      "north -0.21943 -0.10753 1.4211 0.11515 -0.11106 -0.4232 0.074172 0.46527 0.013844 0.30656 -0.018014 0.17247 0.88858 0.3302 0.3224 -0.15745 0.5737 -0.72434 -0.95151 -0.44931 1.1615 0.9244 0.89791 -0.36227 -0.44977 0.672 -0.30621 0.039734 -0.00075862 0.17978 -0.34238 0.51127 0.18377 -0.052345 0.10054 0.032593 0.79285 -0.54269 -0.4319 0.27105 -1.3452 -0.24818 -0.33608 -0.029198 0.85484 0.17007 0.93133 -0.4483 0.21869 0.38537 -0.055231 0.12827 0.10858 0.052143 -0.02478 -2.7474 -0.83842 -1.6171 1.5287 -0.2746 -0.24846 0.39971 0.0027575 -0.14538 0.39468 0.13177 0.19243 0.88939 0.031327 0.55096 -0.24992 -0.53956 -0.42975 -0.86688 0.3344 -0.11432 0.56625 0.60851 -0.46065 0.083046 0.47972 0.45317 -0.50529 0.4246 -0.82187 0.1851 -0.22909 -0.28878 1.0986 0.19668 -0.44319 -0.091739 -0.44294 0.53735 -1.2287 0.29774 -0.63012 0.16247 1.4362 0.07116\n",
      "\n",
      "security -0.54812 -0.59526 0.51106 -0.28236 0.042281 0.0098602 -0.81804 0.31707 0.72298 0.65932 -0.011503 -0.15633 0.40415 0.33654 -0.19118 0.22318 0.47275 -0.37604 -1.4368 -0.70577 0.0018936 -0.78053 0.35409 -0.39263 -0.31086 -0.4895 -0.042803 -0.36605 -0.43291 0.11099 0.13172 -0.14052 -0.5033 0.30447 0.2781 -0.035067 -0.16576 0.51028 0.39634 0.43908 -0.21582 -0.58404 1.1359 0.4934 0.35553 -0.21484 -0.27747 0.31142 -0.72995 -0.72453 0.67625 -0.20535 -0.35954 1.7725 -0.017912 -2.3846 0.88889 -0.51865 1.8868 0.43222 -0.26865 -0.080824 -0.27343 -0.31773 0.47822 0.17027 -0.20541 0.38748 0.88872 0.61151 0.48762 -0.86307 -0.21343 -0.93353 0.20169 0.11287 0.22997 0.35405 -1.0584 0.28047 1.7728 -0.18948 0.41384 -0.0099218 -1.483 0.67675 0.63901 0.16577 -0.2538 -0.46696 0.49387 0.068447 -0.28346 -0.12219 -0.57999 0.99332 0.52901 0.044368 0.35067 0.53382\n",
      "\n",
      "season -0.11922 -0.22701 0.72857 -1.2353 -0.61431 -0.21162 1.1514 0.54402 -0.90197 -0.41669 0.44351 -0.9858 0.5835 -0.56379 -0.22593 0.0039534 -0.0030483 0.42783 0.15913 0.86645 -0.39026 0.28904 -0.39147 0.78224 0.92115 -0.24897 0.037316 -0.62325 0.055504 -0.61437 -0.93197 0.53306 0.11501 -0.49384 -0.63212 0.29129 -0.78414 0.4628 -1.1368 0.2162 0.57121 -0.29181 0.48042 -0.012972 0.61193 0.43858 0.82714 -1.4074 0.72607 -1.1348 -0.33508 -0.55288 -0.30892 0.90494 0.15793 -2.9313 -0.34886 0.30364 0.64373 1.2306 -0.95707 1.0438 -0.51568 -0.24783 0.76059 -0.062309 -0.14341 -0.55713 -0.37103 0.072251 -0.01049 -0.15103 -0.49545 0.21362 -0.14376 0.49824 -0.90065 0.23313 -0.56199 -0.10691 0.10622 0.40275 -1.2343 0.11715 -0.83084 -0.62424 -0.40395 -0.19902 0.2356 0.587 -0.96245 0.18076 -0.40111 0.44392 -0.73176 -0.31401 -0.34014 1.1755 0.59137 -0.15081\n",
      "\n",
      "york 0.75694 0.033857 0.37807 -0.52494 0.66977 0.0087849 0.29571 0.2848 -0.75306 0.028146 -0.17275 -1.1051 -0.32794 -0.22629 0.26637 -0.52505 0.72795 -0.37079 -0.5931 -0.38292 0.34216 0.29214 -0.13127 -0.11968 0.56546 -0.49292 0.27956 -0.85311 -0.022971 0.40805 0.26495 0.90006 -0.28523 0.56483 -1.0308 0.046979 -0.25327 -0.59652 0.70418 -0.19515 0.13675 -0.8074 -0.67271 -0.41186 0.22363 0.096915 -0.043122 -0.69885 0.58425 -0.24611 0.33859 -0.080402 0.63474 0.15447 -0.89735 -2.8035 -0.065357 -0.057547 1.3161 0.57924 -1.0454 0.96129 -0.14676 -0.19215 0.17742 -0.69164 0.23425 0.55024 0.41043 0.49835 -0.85245 -0.18501 -1.1719 -0.10409 0.19547 -0.79822 0.10368 -0.17151 -1.3195 -0.67411 0.26547 0.49225 0.11592 -0.078138 -0.67984 -0.5437 -0.2662 -0.063557 -0.35869 -0.46687 0.018914 -0.7174 0.51893 -0.18991 -1.0533 0.65368 0.11475 0.26294 0.60536 0.62143\n",
      "\n",
      "how -0.23769 0.59392 0.58697 -0.041788 -0.86803 -0.0051122 -0.4493 -0.027985 0.065674 -0.37901 0.31175 0.20528 0.431 -0.30913 -0.084523 0.056902 0.34704 0.32332 -0.60413 0.23123 -0.095476 -0.50009 0.024528 -0.5819 0.0075945 -0.13871 -0.26417 -0.7827 -0.24024 0.081827 -0.1867 0.95404 -0.057412 0.0089747 -0.20492 0.060902 -0.018265 0.3349 0.2373 -0.17605 -0.54013 -0.32745 -0.088963 -0.38417 -1.0903 0.17028 -0.019269 -0.35687 -0.0084772 -0.81525 0.23428 0.11118 0.42356 1.111 -0.35535 -2.3889 0.6203 -0.16352 1.445 0.58333 -0.22946 1.4375 -0.063997 -0.31511 1.0875 0.39221 0.73147 0.57236 -0.050781 -0.027736 0.41085 -0.25076 -0.26331 -0.38682 0.50667 -0.178 0.018334 -0.64042 -0.83078 0.26167 0.7191 0.1505 -0.67532 -0.1487 -2.1266 -0.58562 -0.12433 -0.32622 -1.0088 -0.65506 0.12711 0.227 -0.26423 -0.44033 -0.45518 -0.28702 -0.48125 -0.58302 0.20035 0.50308\n",
      "\n",
      "public 0.27528 0.22559 -0.29646 -0.19067 0.18936 0.61855 -0.89264 0.28183 0.40389 -0.065414 -0.50938 -0.08886 -0.20152 -0.13439 -0.021077 -0.24169 0.54612 0.65442 -0.48053 0.39893 -0.28369 0.13064 0.28003 -0.58927 -0.35813 -0.58996 0.20764 0.030577 -0.24032 0.17751 0.38048 0.5518 -0.043817 0.59522 -0.83406 0.62643 -0.3808 0.0078803 -0.3535 0.4471 -0.30419 -0.55965 -0.28996 0.19426 0.20166 -0.27511 0.49588 -0.42664 0.10917 -0.87574 0.23565 -0.68206 0.22442 0.74806 -0.19348 -2.6988 0.71671 -0.45328 2.2088 0.02737 -0.099598 0.16888 0.14004 0.13678 0.78804 0.045185 0.42819 -0.036869 1.0838 0.062532 -0.040702 -0.27305 0.29982 -0.58137 -0.20059 0.29416 0.35478 0.28035 -1.3558 -0.53859 1.0382 0.010252 0.44126 0.46009 -1.5 -0.34445 -0.65755 0.6388 0.18468 -0.70406 0.25236 -0.50973 -0.17074 0.42255 -0.44975 0.63835 0.28452 -0.4795 0.59833 0.2318\n",
      "\n",
      "early 0.12177 0.039325 -0.16448 0.27675 -0.16077 -0.21445 0.60878 -0.1537 -0.2726 -0.14869 -0.10011 -0.24658 -0.0533 0.030803 0.56856 -0.39606 0.069902 -0.44124 -0.010981 -0.059621 0.4804 -0.1213 0.08366 0.50326 0.1539 -0.54766 -0.26312 -0.50794 0.10531 0.051382 -0.37347 0.032864 -0.546 0.2274 -0.22009 -0.024541 -0.41488 -0.18315 -0.15814 -0.17042 -0.83283 0.063777 -0.0073215 0.38769 0.11179 -0.5634 0.61484 -0.49758 0.73245 -0.91129 -0.098319 0.081946 0.22271 0.7645 -0.092678 -2.6782 -0.96808 -0.45957 1.9655 0.76142 -0.51924 1.3606 0.013375 0.58349 0.88712 -0.21953 -0.065078 -0.20063 0.045578 -0.38737 -0.33361 0.05592 -0.45309 -0.57474 -0.35902 -0.1428 -0.51764 -0.049755 -0.83138 -0.20949 0.56455 -0.010332 0.030278 0.17365 -1.1197 -0.043933 -0.042669 -0.25619 -0.71626 -0.082569 0.56148 0.11156 -0.38744 0.98504 -1.2083 0.10846 -0.36772 -0.096766 0.30823 -0.15651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "according -0.068258 -0.047649 0.49786 -0.40922 -0.0051759 0.012998 0.38682 0.16049 0.13969 0.3777 0.12385 -0.32508 0.76584 -0.33999 -0.032049 -0.58342 0.53179 -0.71871 -0.88889 0.095524 0.17448 -0.4459 0.19492 0.71823 -0.37576 -0.61964 0.44474 -0.5949 -0.26213 0.20811 0.13722 0.033131 -0.23305 -0.0027318 -0.46937 0.47767 0.25512 0.53612 0.045807 -0.28083 -0.415 -0.067618 -0.26842 0.36729 -0.081097 -0.52428 -0.2608 -0.85743 -0.76121 -0.56286 0.7719 -0.03485 0.61521 0.67937 0.074163 -2.0818 -0.26734 -0.17753 1.9872 0.52471 -0.23144 0.41671 0.082092 -0.18117 0.56496 0.10529 -0.24687 0.42676 0.90579 0.10628 -0.26557 0.36568 0.19739 -0.34528 0.22259 0.20374 -0.19925 -0.10147 -1.4524 -0.29982 1.029 0.33442 -0.26938 -0.26667 -0.96846 -0.12894 -0.32834 -0.21234 0.13487 -0.081348 0.81305 -0.40582 -0.19017 -0.006391 -0.31896 0.40277 -0.2682 -0.39942 0.18658 -0.47027\n",
      "\n",
      "several -0.353 0.010869 -0.72551 -0.043148 0.71976 0.46265 -0.069802 0.28948 -0.29837 -0.081974 0.27666 -0.14696 0.39687 0.28386 -0.15623 -0.039022 0.0095102 -0.21888 -0.26734 0.22894 0.49919 -0.14866 0.15337 0.088968 -0.017463 -0.94823 -0.13536 -0.35306 -0.21664 0.70863 0.27952 0.027543 -0.53891 0.028243 0.030533 0.44349 0.063133 -0.31548 -0.48361 0.26331 -0.92827 0.0029393 0.2458 0.080086 0.29391 0.15819 0.086521 -0.15885 0.044325 0.16002 -0.12453 -0.10026 0.12915 1.0034 0.15032 -2.3826 -0.085662 -0.343 1.7233 1.0636 -0.54242 1.7966 0.58394 0.54978 0.78139 -0.25629 0.41502 0.22466 0.63678 0.11993 -0.48205 0.11276 0.10453 -0.43783 0.11067 0.19199 0.042466 -0.097231 -1.1432 0.2287 1.3581 -0.20696 -0.090249 0.22993 -1.6496 -0.13351 -0.056252 -0.20857 -0.32329 -0.15609 0.10686 -0.16542 -0.67966 0.094661 -0.92873 0.15863 -0.98996 0.30419 0.8794 0.26261\n",
      "\n",
      "court 0.85932 -0.59981 0.5806 0.35934 0.2492 0.95952 -0.82959 0.62772 -0.6087 -0.15539 -0.2111 0.12074 -0.46964 0.17221 -0.53435 -0.14676 0.31927 0.019107 -0.73422 0.10619 0.31664 -0.72656 1.0762 0.075347 0.12945 -0.25226 0.55357 -1.3647 -0.0236 0.3734 0.88956 -0.10047 -0.11375 -0.10827 -0.33839 0.015145 -0.32497 -0.27231 -0.017 -0.30492 -0.7494 -0.34647 0.48771 -1.1679 0.17564 -0.87289 0.18864 -0.77418 0.53442 -0.4667 0.65823 -0.92103 0.71295 1.0102 -0.57736 -1.7131 -0.20955 -0.33389 2.3036 0.7189 -0.67824 0.059951 0.23145 0.16665 0.73678 -0.74467 -0.25556 1.0724 -1.3702 0.49105 0.06957 -0.1959 0.45674 -0.5119 0.084189 0.068655 -0.31221 -0.37624 -0.64081 -0.021717 0.91631 -0.44531 0.0035942 0.071465 -1.178 -0.71733 -0.097743 0.028172 0.22805 -0.45741 0.33409 -1.1162 -0.68378 0.46071 0.68427 0.83316 0.44769 -0.14398 0.61525 0.02249\n",
      "\n",
      "say -0.091682 0.58105 0.40477 -0.41979 -0.85111 -0.28719 -0.41949 -0.10424 0.45317 -0.099073 -0.13786 0.4999 0.50049 -0.24041 -0.35101 -0.2202 -0.063055 0.23212 -0.69162 0.73671 0.55555 0.20747 0.11246 -0.25942 -0.63256 0.12418 -0.34075 -0.85638 0.10582 -0.6459 0.58201 0.64314 0.073369 0.21239 0.02933 0.19413 0.0032224 0.48618 0.52267 -0.27378 -0.87497 -0.13735 0.022005 -0.42271 -0.534 -0.59254 0.26929 -0.38206 -0.23628 -0.9333 0.39291 -0.053216 -0.25128 1.0639 -0.48971 -2.0789 -0.094213 -0.21049 1.4549 0.6457 -0.34378 0.645 -0.043274 -0.5112 0.79582 0.20839 0.71555 0.71109 -0.29547 -0.064144 0.2128 -0.46951 -0.3592 -0.50957 -0.33883 -0.21741 0.36005 -0.13833 -1.0957 -0.075884 1.0875 -0.48247 -0.58522 -0.41196 -2.0608 -0.42156 0.088446 0.16319 -0.59871 -0.089221 0.58619 -0.41774 -0.30713 -0.55838 -0.65975 -0.080926 -0.34134 -0.38741 0.34695 0.229\n",
      "\n",
      "around 0.049432 0.29907 0.070318 0.085277 -0.17125 0.93128 -0.67008 0.54096 -0.38016 -0.49383 0.30008 0.28522 0.7559 -0.065383 0.56754 -0.83206 0.1329 0.11873 -0.63604 -0.29106 1.0898 0.65408 0.58158 -0.022854 0.24991 -0.551 -0.14216 -0.065637 0.10867 -0.17114 -0.27218 -0.20419 0.9035 -0.22882 -0.12385 0.41479 -0.15903 0.23308 -0.08571 0.063337 -0.40937 -0.79099 -0.28885 -0.28497 -0.2298 -0.26469 -0.009687 -0.066363 0.017328 -0.61434 0.10515 0.23692 -0.069444 1.6105 -0.61603 -2.6838 -0.7 -0.31433 1.5742 0.81873 0.36615 2.0177 -0.30481 0.30421 0.077525 0.26503 0.33716 -0.12614 0.672 0.11275 0.30333 -0.32643 -0.27705 -0.28524 0.10183 -0.4819 0.76543 0.13401 -0.3008 0.25959 0.68685 0.14341 -0.49093 -0.029874 -0.72508 0.38488 0.49854 -0.28301 -0.12738 -0.025954 0.14759 0.43767 0.056956 -0.37114 -0.68358 0.047255 -0.43799 0.21136 1.4037 0.54495\n",
      "\n",
      "foreign -0.20331 -0.17155 0.54993 0.58449 0.13546 -0.68756 -1.0766 -0.4769 0.35732 0.051292 -0.25074 -0.074053 0.1899 0.88558 0.028868 -0.30371 -0.57873 -0.73225 -0.59986 -0.15194 0.17832 0.35982 0.33956 -0.16778 -0.59636 -0.26366 0.57739 -0.26347 0.64178 -0.087059 -0.42444 0.46175 -0.8998 -0.34521 -0.21198 0.38826 0.09415 0.671 -0.83664 -0.0536 -0.47057 -0.62799 0.27451 0.7822 1.0455 -1.1653 -0.59402 -0.22368 -0.73404 -0.4731 -0.060269 -0.058031 0.20512 0.76996 -0.075378 -1.5694 -0.75311 0.11052 2.3816 0.59037 -0.25694 -0.093778 0.070177 -0.23591 0.16259 0.054415 0.61379 1.1417 1.0726 -0.48893 0.20828 -1.2073 -0.20314 -0.62726 -0.044011 0.48148 -0.69901 0.842 -1.446 -0.12909 1.5357 0.041683 0.24144 -0.35685 -1.1933 -0.43208 -0.15319 0.20169 0.2867 -1.8047 -0.034314 0.27394 0.24232 -0.51384 -0.8006 0.45107 -0.54559 -0.34357 0.36473 -0.11026\n",
      "\n",
      "10 0.086759 0.3431 0.015262 -0.21176 0.016745 0.16417 0.66551 0.57008 -0.48016 -0.086513 1.22 -0.011157 -0.35642 0.28678 0.67092 -0.33746 0.035141 0.070877 -0.8368 0.76705 0.71027 0.093339 0.27353 0.65215 0.46239 -0.27038 0.32758 -0.12183 0.22354 0.23553 0.041341 0.40014 0.16113 -0.52684 -0.29937 0.24468 0.13245 0.17948 -0.4892 0.50635 -0.054496 -0.75931 0.52526 -0.42737 0.22847 -0.60702 -0.40055 -0.88661 -0.19123 -0.39729 -0.41452 -0.75348 -0.43084 1.1405 -0.7077 -2.3559 -0.16866 -0.52046 2.0306 0.91356 -0.39675 0.81674 -0.66153 0.15481 0.22412 0.45764 -0.21857 0.35135 0.54537 0.35387 -0.19377 -0.076632 -0.038375 0.36171 0.13228 0.13748 0.085034 -0.10479 -0.85157 -0.20092 0.92623 -0.18383 -0.53674 0.080922 -0.68858 -0.40219 0.31597 -0.087784 0.16418 0.36074 -0.030817 0.569 -0.16879 -0.3409 -1.3764 0.27924 -0.20352 0.061544 0.57853 -0.55887\n",
      "\n",
      "until -0.11003 -0.1945 -0.0746 0.13326 -0.44721 0.14976 0.58139 0.53515 0.41394 -0.61779 0.93259 0.1632 0.56684 0.37601 0.3756 -0.74354 0.32886 0.18336 -0.48717 -0.52609 -0.43348 -0.37205 0.3386 -0.22787 0.06858 0.18793 -0.27675 -0.86805 0.60734 0.033616 -0.18512 0.63641 -0.29113 -0.47396 -0.83006 0.39132 -0.33739 -0.47392 -0.45855 -0.88229 -0.52698 -0.73464 0.24544 -0.2111 0.27155 -0.60551 0.32774 -0.84155 0.4643 -0.86121 -0.20803 -0.25611 0.12957 1.5298 -0.53295 -2.6471 -0.72793 -0.56359 1.852 0.49079 -0.2382 1.0728 -0.42418 0.018965 1.3196 -0.5115 -0.082289 0.1606 0.041956 0.30951 0.01204 -0.29183 -0.12679 -0.179 0.16558 -0.11761 -0.55772 -0.44203 -0.5632 -0.15485 0.51699 -0.61597 -0.28022 0.87744 -0.43338 -0.096394 0.43562 -0.20935 -0.17785 0.0031643 -0.12619 0.004067 -0.50541 1.1415 -1.0628 0.11526 0.38815 0.65996 -0.040638 -0.27225\n",
      "\n",
      "set -0.026979 0.0054342 0.22934 0.1992 0.53153 0.46441 -0.16962 0.79351 -0.49407 0.27806 0.40256 0.028784 -0.21689 0.26313 -0.013339 0.24328 0.37118 0.83444 -0.78186 -0.49739 0.69664 -0.51034 0.89815 0.16294 0.81555 -0.64644 -0.1909 -0.24281 -0.27935 -0.30685 -0.60586 0.16049 -0.058313 0.11473 0.16565 -0.14386 -0.066771 -0.041341 -0.17164 -0.40261 -0.21321 -0.44287 0.2134 -0.088432 0.16284 -0.38352 -0.42738 -0.18242 0.16961 -0.70388 -0.13625 -0.28808 0.17642 1.4061 0.11511 -2.6623 -0.10834 0.32273 1.7832 1.0727 -0.11656 0.31391 -0.31614 0.2553 0.83315 -0.083177 -0.22755 0.76016 -0.071007 -0.64414 0.021925 -0.040112 -0.29247 -0.19531 -0.069206 -0.12067 -0.18983 -0.29643 -0.4019 -0.015466 0.87412 -0.047512 -0.72765 0.061269 -1.2187 0.24303 0.16417 0.075568 -0.15996 0.049251 -0.13424 0.27638 -0.35704 -0.22446 -0.57734 -0.048605 0.024131 -0.50613 0.29535 0.10395\n",
      "\n",
      "political -0.33926 0.068714 -0.31557 -0.24849 0.44435 0.15167 -0.31527 -1.1698 -0.10753 0.52095 -0.77615 0.16561 0.72414 -0.016989 -0.43988 0.17367 -0.10719 -0.52538 -0.07708 -0.28964 0.52395 0.29934 0.70362 -0.72564 -0.42393 -0.48204 0.033616 -0.29511 0.34794 -0.27514 0.3467 0.51157 -0.30432 -0.043146 -0.71941 -0.17902 0.28824 0.13239 -0.60676 0.26591 -1.5263 -0.49898 0.56189 -0.60347 -0.4829 -0.92018 0.24844 -0.31727 -0.58208 0.16869 0.16816 -0.42411 0.119 1.1859 0.55422 -2.7024 0.48945 -0.28438 1.5228 0.77069 -0.46766 0.21007 -0.8434 -0.40481 1.6652 -0.12326 0.32125 -0.12691 0.59459 0.29502 -0.024563 -0.42846 -0.51083 -0.45647 -0.66782 -0.1642 -0.56383 -0.24997 -0.81554 0.24945 0.52835 0.34749 0.719 -0.074859 -1.353 0.14949 -0.48989 0.44484 0.17209 -1.838 0.1503 0.29288 -0.30107 0.4089 -0.39897 -0.11257 0.23602 -0.73818 0.49146 0.88707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x93 in position 5456: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0b3cfe694faa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mxtrain_glove\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxtest_glove\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mglove_vecs_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglove_vecs_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoGlove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'comment_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sent_vec_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_vecs_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sent_vec_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_vecs_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-0b3cfe694faa>\u001b[0m in \u001b[0;36mdoGlove\u001b[1;34m(x_train, x_test)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdoGlove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadWordVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;31m# create sentence vectors using the above function for training and validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mxtrain_glove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msent2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-0b3cfe694faa>\u001b[0m in \u001b[0;36mloadWordVecs\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0membeddings_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x93 in position 5456: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "wv = '../input/glove.6B.100d.txt'\n",
    "\n",
    "def loadWordVecs():\n",
    "    embeddings_index = {}\n",
    "    f = open(wv)\n",
    "    for line in f:\n",
    "        print(line)\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def doGlove(x_train,x_test):\n",
    "    embeddings_index = loadWordVecs()\n",
    "    # create sentence vectors using the above function for training and validation set\n",
    "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n",
    "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n",
    "    xtrain_glove = np.array(xtrain_glove)\n",
    "    xtest_glove = np.array(xtest_glove)\n",
    "    return xtrain_glove,xtest_glove,embeddings_index\n",
    "\n",
    "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(train_df['comment_text'], test_df['comment_text'])\n",
    "train_df[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\n",
    "test_df[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())\n",
    "print(\"Glove sentence vector finished...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Neural Networks and Facebook's Fasttext\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "# NN\n",
    "def doAddNN(X_train,X_test,pred_train,pred_test):\n",
    "    for i in range(6):\n",
    "        X_train['nn_'+str(i)] = pred_train[:,i]\n",
    "        X_test['nn_'+str(i)] = pred_test[:,i]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN(nb_words_cnt,max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,\n",
    "                     5,\n",
    "                     padding='valid',\n",
    "                     activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def doNN(X_train,X_test,Y_train):\n",
    "    max_len = 70\n",
    "    nb_words = 10000\n",
    "    \n",
    "    print('Processing text dataset')\n",
    "    texts_1 = []\n",
    "    for text in X_train['comment_text']:\n",
    "        texts_1.append(text)\n",
    "\n",
    "    print('Found %s texts.' % len(texts_1))\n",
    "    test_texts_1 = []\n",
    "    for text in X_test['comment_text']:\n",
    "        test_texts_1.append(text)\n",
    "    print('Found %s texts.' % len(test_texts_1))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=nb_words)\n",
    "    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n",
    "    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "\n",
    "    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n",
    "    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n",
    "    del test_sequences_1\n",
    "    del sequences_1\n",
    "    nb_words_cnt = min(nb_words, len(word_index)) + 1\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    \n",
    "    # ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    ytrain_enc = Y_train\n",
    "        \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], len(labels)])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN(nb_words_cnt,max_len)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_pad)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "train_df,test_df = doNN(train_df,test_df,train_y)\n",
    "print('NN finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN Glove\n",
    "\n",
    "def doAddNN_glove(X_train,X_test,pred_train,pred_test):\n",
    "    for i in range(6):\n",
    "        X_train['nn_glove_'+str(i)] = pred_train[:,i]\n",
    "        X_test['nn_glove_'+str(i)] = pred_test[:,i]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN_glove():\n",
    "    # create a simple 3 layer sequential neural net\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, input_dim=100, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(len(labels)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def doNN_glove(X_train,X_test,Y_train,xtrain_glove,xtest_glove):\n",
    "    # scale the data before any neural net:\n",
    "    scl = preprocessing.StandardScaler()\n",
    "    #ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    ytrain_enc = Y_train\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    xtrain_glove = scl.fit_transform(xtrain_glove)\n",
    "    xtest_glove = scl.fit_transform(xtest_glove)\n",
    "    pred_train = np.zeros([xtrain_glove.shape[0], len(labels)])\n",
    "    \n",
    "    for dev_index, val_index in kf.split(xtrain_glove):\n",
    "        dev_X, val_X = xtrain_glove[dev_index], xtrain_glove[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN_glove()\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=10, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_glove)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN_glove(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "train_df,test_df = doNN_glove(train_df,test_df,train_y,glove_vecs_train,glove_vecs_test)\n",
    "print('NN Glove finished...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Text\n",
    "\n",
    "def doAddFastText(X_train,X_test,pred_train,pred_test):\n",
    "    for i in range(6):\n",
    "        X_train['ff_'+str(i)] = pred_train[:,i]\n",
    "        X_test['ff_'+str(i)] = pred_test[:,i]\n",
    "    return X_train,X_test\n",
    "\n",
    "\n",
    "def initFastText(embedding_dims,input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocessFastText(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df['comment_text']:\n",
    "        doc = preprocessFastText(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def doFastText(X_train,X_test,Y_train):\n",
    "    min_count = 2\n",
    "\n",
    "    docs = create_docs(X_train)\n",
    "    tokenizer = Tokenizer(lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "    maxlen = 300\n",
    "\n",
    "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "    input_dim = np.max(docs) + 1\n",
    "    embedding_dims = 20\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    #ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    ytrain_enc = Y_train\n",
    "\n",
    "    docs_test = create_docs(X_test)\n",
    "    docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "    xtrain_pad = docs\n",
    "    xtest_pad = docs_test\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], len(labels)])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initFastText(embedding_dims,input_dim)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=25, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(docs_test)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddFastText(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "train_df,test_df = doFastText(train_df,test_df,train_y)\n",
    "print('FastText finished...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['comment_text', 'split']\n",
    "train_X = train_df.drop(cols_to_drop, axis=1)\n",
    "test_X = test_df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNB & SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())\n",
    "\n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "cv_scores = []\n",
    "pred_full_test = np.zeros([test_df.shape[0], len(labels)])\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    dev_X, dev_y\n",
    "    for i, j in enumerate(labels):\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y[:,i], val_X, val_y[:,i], test_tfidf)\n",
    "        pred_test_y[:, 0]\n",
    "        pred_full_test[:, i] = pred_full_test[:,i] + pred_test_y[:,0] # FIXME\n",
    "        pred_train[val_index,i] = pred_val_y[:,0]\n",
    "        cv_scores.append(metrics.log_loss(val_y[:,i], pred_val_y[:,0]))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "tfidf_vec.fit(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = np.zeros([test_df.shape[0], len(labels)])\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    dev_X, dev_y\n",
    "    for i, j in enumerate(labels):\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y[:,i], val_X, val_y[:,i], test_tfidf)\n",
    "        pred_test_y[:, 0]\n",
    "        pred_full_test[:, i] = pred_full_test[:,i] + pred_test_y[:,0] # FIXME\n",
    "        pred_train[val_index,i] = pred_val_y[:,0]\n",
    "        cv_scores.append(metrics.log_loss(val_y[:,i], pred_val_y[:,0]))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "\n",
    "# add the predictions as new features #\n",
    "for i in range(6):\n",
    "    train_df[\"nb_cvec_\"+str(i)] = pred_train[:,i]\n",
    "    test_df[\"nb_cvec_\"+str(i)] = pred_full_test[:,i]\n",
    "print(\"Naive Bayesian Count Vector finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "tfidf_vec.fit(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = np.zeros([test_df.shape[0], len(labels)])\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    dev_X, dev_y\n",
    "    for i, j in enumerate(labels):\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y[:,i], val_X, val_y[:,i], test_tfidf)\n",
    "        pred_test_y[:, 0]\n",
    "        pred_full_test[:, i] = pred_full_test[:,i] + pred_test_y[:,0] # FIXME\n",
    "        pred_train[val_index,i] = pred_val_y[:,0]\n",
    "        cv_scores.append(metrics.log_loss(val_y[:,i], pred_val_y[:,0]))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "\n",
    "# add the predictions as new features #\n",
    "for i in range(6):\n",
    "    train_df[\"nb_cvec_char_\"+str(i)] = pred_train[:,i]\n",
    "    test_df[\"nb_cvec_char_\"+str(i)] = pred_full_test[:,i]\n",
    "print(\"Naive Bayersian Count Vector Char finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = np.zeros([test_df.shape[0], len(labels)])\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    dev_X, dev_y\n",
    "    for i, j in enumerate(labels):\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y[:,i], val_X, val_y[:,i], test_tfidf)\n",
    "        pred_test_y[:, 0]\n",
    "        pred_full_test[:, i] = pred_full_test[:,i] + pred_test_y[:,0] # FIXME\n",
    "        pred_train[val_index,i] = pred_val_y[:,0]\n",
    "        cv_scores.append(metrics.log_loss(val_y[:,i], pred_val_y[:,0]))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# add the predictions as new features #\n",
    "for i in range(6):\n",
    "    train_df[\"nb_tfidf_char_\"+str(i)] = pred_train[:,i]\n",
    "    test_df[\"nb_tfidf_char_\"+str(i)] = pred_full_test[:,i]\n",
    "print(\"Naive Bayersian TFIDF Vector Char finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = len(labels)\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], len(labels)])\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"cv scores : \", cv_scores)\n",
    "\n",
    "pred_full_test /= 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(pred_full_test)\n",
    "out_df.columns = labels\n",
    "out_df.insert(0, 'id', test_id)\n",
    "out_df.to_csv(\"../output/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = '../output/train_df.pkl'\n",
    "train_df.to_pickle(file_name)  \n",
    "train_df = pd.read_pickle(file_name)\n",
    "file_name = '../output/test_df.pkl'\n",
    "test_df.to_pickle(file_name)\n",
    "test_df = pd.read_pickle(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
